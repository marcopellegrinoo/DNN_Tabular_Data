{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIDDS_PyTorch(NoEmbed).ipynb","provenance":[],"mount_file_id":"1DMGAIUZzXpUHNgbrbtoLIw3j2PUKs-TQ","authorship_tag":"ABX9TyPsZ+TZRTTjTUnfIKnzV42/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"76NJn0Qtr1j4"},"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as torch_optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2u4kOnGscuy","executionInfo":{"status":"ok","timestamp":1625214706485,"user_tz":-120,"elapsed":10,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"b1eac472-1ec2-4ebb-da9f-75f523fc72f2"},"source":["if torch.cuda.is_available():\n","  print(torch.cuda.device_count())            # Numero di GPU disponibili\n","  print(torch.cuda.get_device_name(0))        # Nome della prima GPU disponibile\n","  print(torch.cuda.current_device())        # Device in uso al momento\n","  print(torch.cuda.set_device(0))             # Imposta la prima GPU come default\n","  print(torch.cuda.get_device_capability(0))  # Verifica le capacità della prima GPU"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","Tesla T4\n","0\n","None\n","(7, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_QZUQSURsgDr"},"source":["path = './drive/MyDrive/Materiale_Pellegrino_personal/CIDDS_Meta/CIDDS_Meta.csv'\n","dataset = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULiR_2a5s7Qe"},"source":["### ***PRE-ELABORAZIONE DATI***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"qryt6Agos-SM","executionInfo":{"status":"ok","timestamp":1625214706833,"user_tz":-120,"elapsed":13,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"5b6f6c36-a431-406d-ad30-ba992fd7ecce"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Duration</th>\n","      <th>Proto</th>\n","      <th>Packets</th>\n","      <th>Bytes</th>\n","      <th>Flows</th>\n","      <th>Flags</th>\n","      <th>Tos</th>\n","      <th>multilabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.245</td>\n","      <td>TCP</td>\n","      <td>2</td>\n","      <td>670</td>\n","      <td>1</td>\n","      <td>.AP...</td>\n","      <td>0</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000</td>\n","      <td>TCP</td>\n","      <td>1</td>\n","      <td>66</td>\n","      <td>1</td>\n","      <td>.A....</td>\n","      <td>0</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000</td>\n","      <td>TCP</td>\n","      <td>1</td>\n","      <td>58</td>\n","      <td>1</td>\n","      <td>....S.</td>\n","      <td>0</td>\n","      <td>portScan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000</td>\n","      <td>TCP</td>\n","      <td>1</td>\n","      <td>58</td>\n","      <td>1</td>\n","      <td>....S.</td>\n","      <td>0</td>\n","      <td>portScan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.047</td>\n","      <td>TCP</td>\n","      <td>11</td>\n","      <td>1027</td>\n","      <td>1</td>\n","      <td>.AP...</td>\n","      <td>0</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>399995</th>\n","      <td>0.034</td>\n","      <td>TCP</td>\n","      <td>2</td>\n","      <td>598</td>\n","      <td>1</td>\n","      <td>.AP...</td>\n","      <td>0</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>399996</th>\n","      <td>0.000</td>\n","      <td>TCP</td>\n","      <td>1</td>\n","      <td>95</td>\n","      <td>1</td>\n","      <td>.AP...</td>\n","      <td>32</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>399997</th>\n","      <td>0.005</td>\n","      <td>TCP</td>\n","      <td>5</td>\n","      <td>479</td>\n","      <td>1</td>\n","      <td>.AP.SF</td>\n","      <td>0</td>\n","      <td>dos</td>\n","    </tr>\n","    <tr>\n","      <th>399998</th>\n","      <td>0.000</td>\n","      <td>TCP</td>\n","      <td>1</td>\n","      <td>66</td>\n","      <td>1</td>\n","      <td>.A...F</td>\n","      <td>32</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>399999</th>\n","      <td>0.024</td>\n","      <td>TCP</td>\n","      <td>2</td>\n","      <td>108</td>\n","      <td>1</td>\n","      <td>.A...F</td>\n","      <td>0</td>\n","      <td>normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400000 rows × 8 columns</p>\n","</div>"],"text/plain":["        Duration  Proto  Packets Bytes  Flows   Flags  Tos multilabel\n","0          0.245  TCP          2   670      1  .AP...    0     normal\n","1          0.000  TCP          1    66      1  .A....    0     normal\n","2          0.000  TCP          1    58      1  ....S.    0   portScan\n","3          0.000  TCP          1    58      1  ....S.    0   portScan\n","4          0.047  TCP         11  1027      1  .AP...    0     normal\n","...          ...    ...      ...   ...    ...     ...  ...        ...\n","399995     0.034  TCP          2   598      1  .AP...    0     normal\n","399996     0.000  TCP          1    95      1  .AP...   32     normal\n","399997     0.005  TCP          5   479      1  .AP.SF    0        dos\n","399998     0.000  TCP          1    66      1  .A...F   32     normal\n","399999     0.024  TCP          2   108      1  .A...F    0     normal\n","\n","[400000 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5TnEZqkL8mu","executionInfo":{"status":"ok","timestamp":1625214706834,"user_tz":-120,"elapsed":12,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"a8eb9b94-ccf9-4074-cd99-75925bbf4fca"},"source":["print(Counter(dataset['Flows']))\n","print(Counter(dataset['multilabel']))\n","\n","dataset = dataset.drop('Flows', axis=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counter({1: 400000})\n","Counter({'normal': 243363, 'dos': 117904, 'portScan': 37723, 'pingScan': 646, 'bruteForce': 364})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"muGTLm_0tXMa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625214706835,"user_tz":-120,"elapsed":10,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"00cf3634-b249-48d7-ad09-ac983de6d35f"},"source":["dep_var = 'multilabel'\n","cat_names = [\"Proto\", \"Flags\", \"Bytes\"]\n","cont_names = [col for col in dataset.columns if col not in cat_names and col != dep_var]\n","\n","print(cont_names, 'len: ', len(cont_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Duration', 'Packets', 'Tos'] len:  3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SEyJdUhEuWGB"},"source":["# LabelEncoding della variabile target \n","target_index = dataset.columns.get_loc(dep_var)\n","dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[dep_var])\n","\n","#LabelEncoding delle variabili categoriali\n","for col in cat_names:\n","  target_index = dataset.columns.get_loc(col)\n","  dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[col])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5JIn5pDcOu2W","executionInfo":{"status":"ok","timestamp":1625214707201,"user_tz":-120,"elapsed":18,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"6e105619-d023-41f0-ca83-b7c7970a4bc1"},"source":["cont_names = [col for col in dataset.columns if col != dep_var]\n","\n","print(cont_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Duration', 'Proto', 'Packets', 'Bytes', 'Flags', 'Tos']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"z2zt6T5XNFmb","executionInfo":{"status":"ok","timestamp":1625214707204,"user_tz":-120,"elapsed":16,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"23d6003e-db6e-40d3-d11c-a3744b052865"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Duration</th>\n","      <th>Proto</th>\n","      <th>Packets</th>\n","      <th>Bytes</th>\n","      <th>Flags</th>\n","      <th>Tos</th>\n","      <th>multilabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.245</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>11030</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>10946</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>10024</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>10024</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.047</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>233</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>399995</th>\n","      <td>0.034</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>10228</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>399996</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>13159</td>\n","      <td>12</td>\n","      <td>32</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>399997</th>\n","      <td>0.005</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>8690</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>399998</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>10946</td>\n","      <td>5</td>\n","      <td>32</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>399999</th>\n","      <td>0.024</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>519</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400000 rows × 7 columns</p>\n","</div>"],"text/plain":["        Duration  Proto  Packets  Bytes  Flags  Tos  multilabel\n","0          0.245      2        2  11030     12    0           2\n","1          0.000      2        1  10946      4    0           2\n","2          0.000      2        1  10024      1    0           4\n","3          0.000      2        1  10024      1    0           4\n","4          0.047      2       11    233     12    0           2\n","...          ...    ...      ...    ...    ...  ...         ...\n","399995     0.034      2        2  10228     12    0           2\n","399996     0.000      2        1  13159     12   32           2\n","399997     0.005      2        5   8690     15    0           1\n","399998     0.000      2        1  10946      5   32           2\n","399999     0.024      2        2    519      5    0           2\n","\n","[400000 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tq0cTNOlNIkE","executionInfo":{"status":"ok","timestamp":1625214707204,"user_tz":-120,"elapsed":15,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"53415a1d-2e1c-4491-f792-031196c7d875"},"source":["print(Counter(dataset['multilabel']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counter({2: 243363, 1: 117904, 4: 37723, 3: 646, 0: 364})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Giq52mZsNNME"},"source":["target_dict = {'bruteForce' : 0,\n","               'dos' : 1,\n","               'normal' : 2,\n","               'pingScan' : 3,\n","               'portScan' : 4}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GL5xD37fnaia"},"source":["from sklearn.model_selection import train_test_split\n","\n","# train 50% e test 50%\n","train, test = train_test_split(dataset, test_size=0.50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ0TyYvfQLgW"},"source":["y_train = train[dep_var]\n","train = train.drop(dep_var, axis=1)\n","y_test = test[dep_var]\n","test = test.drop(dep_var, axis=1)\n","\n","# validation di 2500 righe da train\n","train, validation, y_train, y_val = train_test_split(train, y_train, test_size=(10000/len(train)), random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNTwI2TaaXlY"},"source":["\"\"\"Visto che nel dataset la variabile target è molto squilibrata lo amplio con una generazione\n"," randomica di dati mediante la tecnica chiamata Synthetic Minority Over-sampling Technique (SMOTE)\"\"\"\n","\n","sampling_strategy = {0: 20000, 3: 20000}\n","\n","from imblearn.over_sampling import SMOTE\n","sm = SMOTE(sampling_strategy = sampling_strategy, random_state=42)\n","x_sm, y_train = sm.fit_resample(train, y_train)\n","train = pd.DataFrame(x_sm,columns=train.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5xEsk56PSsW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625214707999,"user_tz":-120,"elapsed":26,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"5fad833f-3a18-4738-f652-1fd043b5cf15"},"source":["#y_train = y_train.values\n","y_test = y_test.values\n","y_val = y_val.values\n","\n","print(Counter(y_train))\n","print(Counter(y_test))\n","print(Counter(y_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counter({2: 115665, 1: 55910, 0: 20000, 3: 20000, 4: 17941})\n","Counter({2: 121584, 1: 59074, 4: 18846, 3: 298, 0: 198})\n","Counter({2: 6114, 1: 2920, 4: 936, 3: 23, 0: 7})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"79A_sfwnQVqX","executionInfo":{"status":"ok","timestamp":1625214708002,"user_tz":-120,"elapsed":26,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"9ec6c75b-4453-4235-e2fa-4cb910a00662"},"source":["train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Duration</th>\n","      <th>Proto</th>\n","      <th>Packets</th>\n","      <th>Bytes</th>\n","      <th>Flags</th>\n","      <th>Tos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>10946.0</td>\n","      <td>5.0</td>\n","      <td>32.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>11756.0</td>\n","      <td>6.0</td>\n","      <td>32.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>9509.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.006000</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>9574.0</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.005000</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>8690.0</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>229511</th>\n","      <td>0.123532</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>12502.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>229512</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>7848.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>229513</th>\n","      <td>0.123000</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>12502.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>229514</th>\n","      <td>0.001543</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>7848.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>229515</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>7848.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>229516 rows × 6 columns</p>\n","</div>"],"text/plain":["        Duration  Proto  Packets    Bytes  Flags   Tos\n","0       0.000000    2.0      1.0  10946.0    5.0  32.0\n","1       0.000000    2.0      1.0  11756.0    6.0  32.0\n","2       0.000000    2.0      1.0   9509.0    8.0   0.0\n","3       0.006000    2.0      6.0   9574.0   15.0   0.0\n","4       0.005000    2.0      5.0   8690.0   15.0   0.0\n","...          ...    ...      ...      ...    ...   ...\n","229511  0.123532    0.0      2.0  12502.0    0.0   0.0\n","229512  0.000000    0.0      1.0   7848.0    0.0   0.0\n","229513  0.123000    0.0      2.0  12502.0    0.0   0.0\n","229514  0.001543    0.0      1.0   7848.0    0.0   0.0\n","229515  0.000000    0.0      1.0   7848.0    0.0   0.0\n","\n","[229516 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"IKp_gSloQTZO","executionInfo":{"status":"ok","timestamp":1625214708003,"user_tz":-120,"elapsed":26,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"9a149f3c-03c2-4f45-f694-90d3df56f605"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Duration</th>\n","      <th>Proto</th>\n","      <th>Packets</th>\n","      <th>Bytes</th>\n","      <th>Flags</th>\n","      <th>Tos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>175966</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>10946</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>242975</th>\n","      <td>0.003</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>9574</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>303194</th>\n","      <td>0.003</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3724</td>\n","      <td>7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>367177</th>\n","      <td>0.067</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>9333</td>\n","      <td>14</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>248827</th>\n","      <td>0.005</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>5177</td>\n","      <td>7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>118065</th>\n","      <td>0.005</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>8690</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>332486</th>\n","      <td>0.265</td>\n","      <td>2</td>\n","      <td>13</td>\n","      <td>1932</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>196684</th>\n","      <td>0.007</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>8690</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>101236</th>\n","      <td>0.006</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>8690</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>142983</th>\n","      <td>0.357</td>\n","      <td>2</td>\n","      <td>78</td>\n","      <td>9560</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200000 rows × 6 columns</p>\n","</div>"],"text/plain":["        Duration  Proto  Packets  Bytes  Flags  Tos\n","175966     0.000      2        1  10946      4    0\n","242975     0.003      2        6   9574     15    0\n","303194     0.003      2        3   3724      7    0\n","367177     0.067      2        3   9333     14    0\n","248827     0.005      2        4   5177      7    0\n","...          ...    ...      ...    ...    ...  ...\n","118065     0.005      2        5   8690     15    0\n","332486     0.265      2       13   1932     15    0\n","196684     0.007      2        5   8690     15    0\n","101236     0.006      2        5   8690     15    0\n","142983     0.357      2       78   9560     15    0\n","\n","[200000 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"va0f2YIwqfM4","executionInfo":{"status":"ok","timestamp":1625214708004,"user_tz":-120,"elapsed":26,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"aa46f8bf-5680-49e0-8795-80372e3b75ba"},"source":["validation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Duration</th>\n","      <th>Proto</th>\n","      <th>Packets</th>\n","      <th>Bytes</th>\n","      <th>Flags</th>\n","      <th>Tos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>269906</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>694</td>\n","      <td>12</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>197818</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>9642</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>195222</th>\n","      <td>0.056</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>13053</td>\n","      <td>14</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>291473</th>\n","      <td>0.410</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3056</td>\n","      <td>12</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>351884</th>\n","      <td>0.004</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>8690</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>129206</th>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>11364</td>\n","      <td>0</td>\n","      <td>192</td>\n","    </tr>\n","    <tr>\n","      <th>20135</th>\n","      <td>0.030</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4287</td>\n","      <td>13</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>101442</th>\n","      <td>0.000</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>10946</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>48347</th>\n","      <td>0.006</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>9574</td>\n","      <td>15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>253847</th>\n","      <td>1.103</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>1915</td>\n","      <td>14</td>\n","      <td>32</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 6 columns</p>\n","</div>"],"text/plain":["        Duration  Proto  Packets  Bytes  Flags  Tos\n","269906     0.000      2        1    694     12   32\n","197818     0.000      2        1   9642      4    0\n","195222     0.056      2        5  13053     14    0\n","291473     0.410      2        3   3056     12    0\n","351884     0.004      2        5   8690     15    0\n","...          ...    ...      ...    ...    ...  ...\n","129206     0.000      0        1  11364      0  192\n","20135      0.030      2        3   4287     13    0\n","101442     0.000      2        1  10946      4    0\n","48347      0.006      2        6   9574     15    0\n","253847     1.103      2        8   1915     14   32\n","\n","[10000 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"pVx4I4U9OJno"},"source":["### ***MODEL***"]},{"cell_type":"code","metadata":{"id":"1Oa_SzfA6bq7"},"source":["\"\"\" Pytorch Dataset e DataLoader\n","Estendiamo la Datasetclasse (astratta) fornita da Pytorch per un accesso più facile al nostro set di dati durante l'addestramento \n","e per utilizzare efficacemente  il DataLoader modulo per gestire i batch. Ciò comporta la sovrascrittura dei metodi __len__e __getitem__\n","secondo il nostro particolare set di dati.\n","Poiché abbiamo solo bisogno di incorporare colonne categoriali, dividiamo il nostro input in due parti: numerico e categoriale. \"\"\" \n","\n","class CIDDS_Dataset(Dataset):\n","    def __init__(self, X, Y):\n","        X = X.copy()\n","        self.X = X.copy().values.astype(np.float32) #numerical columns\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","        \n","#creating train and valid datasets\n","train_ds = CIDDS_Dataset(train, y_train)\n","valid_ds = CIDDS_Dataset(validation, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oGGkh8o6icS","executionInfo":{"status":"ok","timestamp":1625214708008,"user_tz":-120,"elapsed":28,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"e8f44529-bcd8-4e1c-c613-3b4db6158c23"},"source":["\"\"\" Making device (GPU/CPU) compatible\n","\n","(borrowed from https://jovian.ml/aakashns/04-feedforward-nn)\n","\n","In order to make use of a GPU if available, we'll have to move our data and model to it. \"\"\" \n","\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"wZr4ZQflaCAw"},"source":["class CIDDSModel(nn.Module):\n","    def __init__(self, n_cont):\n","        super().__init__()\n","        self.n_cont =  n_cont\n","        self.lin1 = nn.Linear(self.n_cont, 200)\n","        self.lin2 = nn.Linear(200, 100)\n","        self.lin3 = nn.Linear(100, 5)\n","        self.bn1 = nn.BatchNorm1d(self.n_cont, momentum=1.0)\n","        self.bn2 = nn.BatchNorm1d(200, momentum=1.0)\n","        self.bn3 = nn.BatchNorm1d(100, momentum=1.0)\n","        #self.drops = nn.Dropout(0.00001)\n","        \n","\n","    def forward(self, x_cont):\n","        x = self.bn1(x_cont)\n","        x = F.relu(self.lin1(x))\n","        #x = self.drops(x)\n","        x = self.bn2(x)\n","        x = F.relu(self.lin2(x))\n","        #x = self.drops(x)\n","        x = self.bn3(x)\n","        x = self.lin3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQc6Y5qBapPL"},"source":["\"\"\" Fase di preparazione per l'addestramento \"\"\"\n","\n","# Optimizer\n","def get_optimizer(model, lr = 0.001, wd = 0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim\n","\n","# Training function\n","def train_model(model, optim, train_dl):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x, y in train_dl:\n","        batch = y.shape[0]\n","        output = model(x)\n","        loss = F.cross_entropy(output, y)   \n","        optim.zero_grad()\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","# Evaluation function\n","def val_loss(model, valid_dl):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x, y in valid_dl:\n","        current_batch_size = y.shape[0]\n","        out = model(x)\n","        loss = F.cross_entropy(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.max(out, 1)[1]\n","        correct += (pred == y).float().sum().item()\n","    print('valid loss ', sum_loss/total, ' and accuracy ', correct/total)\n","    return sum_loss/total, correct/total\n","\n","# Funzione per l'addestramento \n","def train_loop(model, epochs, lr=0.01, wd=0.0):\n","    optim = get_optimizer(model, lr = lr, wd = wd)\n","    for i in range(epochs): \n","        loss = train_model(model, optim, train_dl)\n","        print('ep ', i, \" training loss: \", loss)\n","        val_loss(model, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceUGliiDrULo"},"source":["### ***TRAINING***"]},{"cell_type":"code","metadata":{"id":"qNNiIdczasqp"},"source":["\"\"\" Ora addestriamo il modello sul set di addestramento. Ho usato l'ottimizzatore Adam per ottimizzare la perdita di entropia incrociata. \n","L'addestramento è piuttosto semplice: iterare attraverso ogni batch, eseguire un passaggio in avanti, calcolare i gradienti, \n","eseguire una discesa del gradiente e ripetere questo processo per tutte le epoche necessarie. \"\"\" \n","\n","batch_size = 512\n","train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIE4j_1iOOP4","executionInfo":{"status":"ok","timestamp":1625214711294,"user_tz":-120,"elapsed":3057,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"512840c9-e334-4f00-862d-0bb4c0b2be53"},"source":["model = CIDDSModel(len(cont_names))\n","to_device(model, device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CIDDSModel(\n","  (lin1): Linear(in_features=6, out_features=200, bias=True)\n","  (lin2): Linear(in_features=200, out_features=100, bias=True)\n","  (lin3): Linear(in_features=100, out_features=5, bias=True)\n","  (bn1): BatchNorm1d(6, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","  (bn2): BatchNorm1d(200, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","  (bn3): BatchNorm1d(100, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",")"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b976OcWbQLBe","executionInfo":{"status":"ok","timestamp":1625215651071,"user_tz":-120,"elapsed":672396,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8597ae72-f59c-4239-d1d5-282177f985ff"},"source":["train_loop(model, epochs=500, lr=0.00008)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ep  0  training loss:  0.49836702261797694\n","valid loss  0.1919153579235077  and accuracy  0.9589\n","ep  1  training loss:  0.15396028815942706\n","valid loss  0.234397740650177  and accuracy  0.9697\n","ep  2  training loss:  0.1004868601487545\n","valid loss  4.401644498252868  and accuracy  0.9699\n","ep  3  training loss:  0.078913691233656\n","valid loss  0.06917644340991974  and accuracy  0.9786\n","ep  4  training loss:  0.06497160072854698\n","valid loss  0.06011707645058632  and accuracy  0.9819\n","ep  5  training loss:  0.05917807847365827\n","valid loss  0.053764922672510145  and accuracy  0.9897\n","ep  6  training loss:  0.054877192758329016\n","valid loss  0.04321585251688957  and accuracy  0.9853\n","ep  7  training loss:  0.05082448431930661\n","valid loss  0.08417292122840882  and accuracy  0.969\n","ep  8  training loss:  0.04791555380794323\n","valid loss  0.06596859434843064  and accuracy  0.9808\n","ep  9  training loss:  0.04595687293833805\n","valid loss  0.12238265590667724  and accuracy  0.9839\n","ep  10  training loss:  0.04389143015536653\n","valid loss  0.05286431546211243  and accuracy  0.9884\n","ep  11  training loss:  0.0435134512611642\n","valid loss  0.035478585493564606  and accuracy  0.9915\n","ep  12  training loss:  0.04203434645708639\n","valid loss  0.030338579684495926  and accuracy  0.9925\n","ep  13  training loss:  0.040473109303365715\n","valid loss  0.03513040733858943  and accuracy  0.9905\n","ep  14  training loss:  0.04058385708020882\n","valid loss  11.032517901659013  and accuracy  0.9766\n","ep  15  training loss:  0.03916416049035284\n","valid loss  0.09130439791679382  and accuracy  0.9874\n","ep  16  training loss:  0.03840234057613603\n","valid loss  0.047052140206098554  and accuracy  0.9882\n","ep  17  training loss:  0.037415345106710504\n","valid loss  0.09868277901411057  and accuracy  0.9834\n","ep  18  training loss:  0.03762999563564202\n","valid loss  0.041815915417671205  and accuracy  0.9904\n","ep  19  training loss:  0.03639507139012581\n","valid loss  0.05726534527540207  and accuracy  0.9868\n","ep  20  training loss:  0.03650330624168566\n","valid loss  0.04639416570663452  and accuracy  0.9891\n","ep  21  training loss:  0.03566548375667942\n","valid loss  0.03533513187170029  and accuracy  0.9923\n","ep  22  training loss:  0.036137408979744656\n","valid loss  0.04555208656191826  and accuracy  0.9905\n","ep  23  training loss:  0.035141233788176476\n","valid loss  0.03773218709304929  and accuracy  0.9918\n","ep  24  training loss:  0.033848422938871886\n","valid loss  0.07340985382795334  and accuracy  0.9832\n","ep  25  training loss:  0.035078509805567266\n","valid loss  0.05223963725566864  and accuracy  0.9859\n","ep  26  training loss:  0.03442167125989125\n","valid loss  0.0586553523182869  and accuracy  0.9879\n","ep  27  training loss:  0.03334024518360265\n","valid loss  0.026929296827316283  and accuracy  0.9944\n","ep  28  training loss:  0.033006038155001784\n","valid loss  0.035496186184883115  and accuracy  0.9916\n","ep  29  training loss:  0.03298084673535261\n","valid loss  0.0418176084369421  and accuracy  0.99\n","ep  30  training loss:  0.03268164585241637\n","valid loss  0.07006203987002373  and accuracy  0.9784\n","ep  31  training loss:  0.03233484293132645\n","valid loss  0.032254909324645996  and accuracy  0.9936\n","ep  32  training loss:  0.03234243430249827\n","valid loss  0.03431486548185349  and accuracy  0.9935\n","ep  33  training loss:  0.03193434158166067\n","valid loss  175.68556825561524  and accuracy  0.9812\n","ep  34  training loss:  0.03150401898129026\n","valid loss  11.336324666976928  and accuracy  0.9526\n","ep  35  training loss:  0.03190878586224784\n","valid loss  0.4967215949177742  and accuracy  0.993\n","ep  36  training loss:  0.031547041435659\n","valid loss  0.04957317550182343  and accuracy  0.9805\n","ep  37  training loss:  0.032302280721398145\n","valid loss  0.039619015413522723  and accuracy  0.9895\n","ep  38  training loss:  0.031433675429693046\n","valid loss  0.0674203258395195  and accuracy  0.9864\n","ep  39  training loss:  0.03123376954532384\n","valid loss  3.1733201110124587  and accuracy  0.9891\n","ep  40  training loss:  0.031229200683589073\n","valid loss  0.05617069164514542  and accuracy  0.9832\n","ep  41  training loss:  0.029877816912484537\n","valid loss  0.23672377160787583  and accuracy  0.9928\n","ep  42  training loss:  0.030404994824866197\n","valid loss  0.04322660436034203  and accuracy  0.9917\n","ep  43  training loss:  0.030203916051489525\n","valid loss  0.5296399733185768  and accuracy  0.9877\n","ep  44  training loss:  0.030187847159132766\n","valid loss  3.2860772216022016  and accuracy  0.9925\n","ep  45  training loss:  0.03030857864983868\n","valid loss  0.5297893020808697  and accuracy  0.9921\n","ep  46  training loss:  0.030126757297196972\n","valid loss  0.3868088555157185  and accuracy  0.9935\n","ep  47  training loss:  0.029590435204301446\n","valid loss  0.03912705462872982  and accuracy  0.9918\n","ep  48  training loss:  0.02976827479930223\n","valid loss  0.04087817910015583  and accuracy  0.9938\n","ep  49  training loss:  0.029511402711929532\n","valid loss  0.03458823543787003  and accuracy  0.9925\n","ep  50  training loss:  0.029285056635307102\n","valid loss  0.5768624374628067  and accuracy  0.9847\n","ep  51  training loss:  0.029419274212153564\n","valid loss  4.629541879844665  and accuracy  0.9855\n","ep  52  training loss:  0.0293731608088718\n","valid loss  2.99163445956707  and accuracy  0.9862\n","ep  53  training loss:  0.028726854050882347\n","valid loss  0.09961689901351929  and accuracy  0.9833\n","ep  54  training loss:  0.02893122845644568\n","valid loss  0.6298993291616439  and accuracy  0.9898\n","ep  55  training loss:  0.02951435722016199\n","valid loss  0.35267319496273997  and accuracy  0.992\n","ep  56  training loss:  0.02859759538448778\n","valid loss  0.03057952196598053  and accuracy  0.9933\n","ep  57  training loss:  0.029103665514521176\n","valid loss  0.5593300631284713  and accuracy  0.9875\n","ep  58  training loss:  0.028687665705765825\n","valid loss  14.267629533755779  and accuracy  0.9902\n","ep  59  training loss:  0.027752358959162084\n","valid loss  0.06461198134422302  and accuracy  0.9849\n","ep  60  training loss:  0.02828364653260637\n","valid loss  0.02789311619400978  and accuracy  0.9932\n","ep  61  training loss:  0.027906798320493753\n","valid loss  0.049943347823619845  and accuracy  0.9882\n","ep  62  training loss:  0.027451668807887717\n","valid loss  0.7180471220970154  and accuracy  0.9858\n","ep  63  training loss:  0.02753533133546949\n","valid loss  0.03079219855070114  and accuracy  0.9928\n","ep  64  training loss:  0.027534906927199947\n","valid loss  4.1306056979775425  and accuracy  0.9883\n","ep  65  training loss:  0.028303184439064332\n","valid loss  0.02931901717185974  and accuracy  0.9927\n","ep  66  training loss:  0.02829598368810561\n","valid loss  0.04021337620019913  and accuracy  0.9914\n","ep  67  training loss:  0.027698216688710302\n","valid loss  0.5486231704235077  and accuracy  0.9928\n","ep  68  training loss:  0.027311270389857884\n","valid loss  0.25939301785826685  and accuracy  0.9927\n","ep  69  training loss:  0.02803515446342611\n","valid loss  0.029512077555060388  and accuracy  0.9924\n","ep  70  training loss:  0.027227276926726957\n","valid loss  0.05787544282674789  and accuracy  0.99\n","ep  71  training loss:  0.027912881484005853\n","valid loss  1.504104930716753  and accuracy  0.9918\n","ep  72  training loss:  0.026937038271086514\n","valid loss  19.051424644851686  and accuracy  0.9845\n","ep  73  training loss:  0.02694858208943511\n","valid loss  2.0979922105953097  and accuracy  0.993\n","ep  74  training loss:  0.026593686435923167\n","valid loss  0.03222962821125984  and accuracy  0.9924\n","ep  75  training loss:  0.02635456868488996\n","valid loss  5.842744679069519  and accuracy  0.9806\n","ep  76  training loss:  0.026577280939851625\n","valid loss  0.4722573718547821  and accuracy  0.9937\n","ep  77  training loss:  0.026928367635830418\n","valid loss  0.03940997121334076  and accuracy  0.9913\n","ep  78  training loss:  0.026925599538302075\n","valid loss  4.077230616569519  and accuracy  0.9881\n","ep  79  training loss:  0.026606838625926787\n","valid loss  0.03851497180834412  and accuracy  0.9908\n","ep  80  training loss:  0.027051053856263964\n","valid loss  0.03216008444428444  and accuracy  0.9926\n","ep  81  training loss:  0.026079752558386564\n","valid loss  0.28006396126151084  and accuracy  0.99\n","ep  82  training loss:  0.02618787035354601\n","valid loss  1.2096625203609466  and accuracy  0.9898\n","ep  83  training loss:  0.026417448041171215\n","valid loss  0.04228602315187454  and accuracy  0.9924\n","ep  84  training loss:  0.025332223914462243\n","valid loss  0.030450094473361968  and accuracy  0.9932\n","ep  85  training loss:  0.02631278706376223\n","valid loss  0.4246980278968811  and accuracy  0.994\n","ep  86  training loss:  0.02624737315562768\n","valid loss  1.900793153321743  and accuracy  0.9913\n","ep  87  training loss:  0.02632833675547813\n","valid loss  0.03484818972051144  and accuracy  0.9911\n","ep  88  training loss:  0.025787534928673052\n","valid loss  0.02907404693365097  and accuracy  0.9929\n","ep  89  training loss:  0.02614530498325294\n","valid loss  0.03080711969137192  and accuracy  0.9927\n","ep  90  training loss:  0.026389126765565218\n","valid loss  2.3926266627371313  and accuracy  0.9936\n","ep  91  training loss:  0.025928387566968535\n","valid loss  0.6301853846311569  and accuracy  0.979\n","ep  92  training loss:  0.026418590367917587\n","valid loss  0.030981127870082854  and accuracy  0.9922\n","ep  93  training loss:  0.025863815709873488\n","valid loss  1.4507672101080418  and accuracy  0.99\n","ep  94  training loss:  0.027101356680996048\n","valid loss  0.043095329028368  and accuracy  0.9913\n","ep  95  training loss:  0.025836714176531318\n","valid loss  83.53913945960998  and accuracy  0.9876\n","ep  96  training loss:  0.0255927988376814\n","valid loss  5.404233676588535  and accuracy  0.9907\n","ep  97  training loss:  0.025823716415015834\n","valid loss  0.20268563179969787  and accuracy  0.9938\n","ep  98  training loss:  0.026324049105084464\n","valid loss  0.7351205867793411  and accuracy  0.9935\n","ep  99  training loss:  0.025969764578810724\n","valid loss  0.04813636905550957  and accuracy  0.9893\n","ep  100  training loss:  0.026634481614297652\n","valid loss  0.03621669872105122  and accuracy  0.9918\n","ep  101  training loss:  0.025659406728852044\n","valid loss  0.035993364995718  and accuracy  0.9889\n","ep  102  training loss:  0.026326494612572707\n","valid loss  0.7545224033385515  and accuracy  0.9927\n","ep  103  training loss:  0.026131206581508273\n","valid loss  0.024360195922851564  and accuracy  0.9941\n","ep  104  training loss:  0.025584809271093713\n","valid loss  5.5341235018253325  and accuracy  0.99\n","ep  105  training loss:  0.025633902545635245\n","valid loss  0.026148791790008546  and accuracy  0.9941\n","ep  106  training loss:  0.0260666620977235\n","valid loss  0.03240431226789951  and accuracy  0.9892\n","ep  107  training loss:  0.02516361334750503\n","valid loss  0.040279350543022154  and accuracy  0.9864\n","ep  108  training loss:  0.02539737238168808\n","valid loss  0.03103427004814148  and accuracy  0.9933\n","ep  109  training loss:  0.025717743633589564\n","valid loss  0.971056136417389  and accuracy  0.9931\n","ep  110  training loss:  0.02593698386894277\n","valid loss  0.2068032824039459  and accuracy  0.9273\n","ep  111  training loss:  0.025569949315393057\n","valid loss  0.02860133025050163  and accuracy  0.993\n","ep  112  training loss:  0.025851394699910105\n","valid loss  3.6769615156680344  and accuracy  0.9936\n","ep  113  training loss:  0.025380190204708934\n","valid loss  4.919780211937428  and accuracy  0.989\n","ep  114  training loss:  0.02573600881135778\n","valid loss  0.0726913781940937  and accuracy  0.9736\n","ep  115  training loss:  0.02542821475508347\n","valid loss  1.7933922204136847  and accuracy  0.9919\n","ep  116  training loss:  0.02548379946016734\n","valid loss  0.9683066239356994  and accuracy  0.9927\n","ep  117  training loss:  0.025213335995620822\n","valid loss  0.034615450102090835  and accuracy  0.9938\n","ep  118  training loss:  0.025016617232891083\n","valid loss  0.8567149026215076  and accuracy  0.9929\n","ep  119  training loss:  0.024925904127425447\n","valid loss  2.173850491306186  and accuracy  0.9934\n","ep  120  training loss:  0.02516623513770438\n","valid loss  0.039885611805319784  and accuracy  0.9911\n","ep  121  training loss:  0.02497886325381994\n","valid loss  2.080448157507181  and accuracy  0.992\n","ep  122  training loss:  0.025124367424858317\n","valid loss  0.04151056017279625  and accuracy  0.9894\n","ep  123  training loss:  0.024883557658985787\n","valid loss  0.023323427900671958  and accuracy  0.9942\n","ep  124  training loss:  0.024893184021322023\n","valid loss  0.0439245180606842  and accuracy  0.9886\n","ep  125  training loss:  0.02531960027632217\n","valid loss  0.07845440266132354  and accuracy  0.9849\n","ep  126  training loss:  0.025536707716886587\n","valid loss  0.03436414242386818  and accuracy  0.9935\n","ep  127  training loss:  0.02486154066483823\n","valid loss  0.03651027607917785  and accuracy  0.9918\n","ep  128  training loss:  0.025189974564337574\n","valid loss  0.06459873073101044  and accuracy  0.993\n","ep  129  training loss:  0.02505563382671507\n","valid loss  0.09809372494220733  and accuracy  0.9936\n","ep  130  training loss:  0.02477602193688087\n","valid loss  0.09027637634277344  and accuracy  0.9869\n","ep  131  training loss:  0.02472098803281385\n","valid loss  0.08599642627239228  and accuracy  0.9828\n","ep  132  training loss:  0.02493746036445798\n","valid loss  2.8330421572335065  and accuracy  0.9922\n","ep  133  training loss:  0.024749058715044868\n","valid loss  0.2044216917037964  and accuracy  0.9918\n","ep  134  training loss:  0.024610034708262764\n","valid loss  0.07358933712840081  and accuracy  0.9856\n","ep  135  training loss:  0.024228269954991638\n","valid loss  0.08222089046239853  and accuracy  0.9898\n","ep  136  training loss:  0.02470755648140535\n","valid loss  0.10026805703639984  and accuracy  0.9866\n","ep  137  training loss:  0.024793777129003533\n","valid loss  0.05670795767456293  and accuracy  0.9918\n","ep  138  training loss:  0.025474939776966618\n","valid loss  0.22679573292732239  and accuracy  0.9858\n","ep  139  training loss:  0.024838011478067874\n","valid loss  0.025389351127296685  and accuracy  0.9933\n","ep  140  training loss:  0.024694325041497423\n","valid loss  0.062054068374633786  and accuracy  0.989\n","ep  141  training loss:  0.023878422779440266\n","valid loss  0.04027198637723923  and accuracy  0.9924\n","ep  142  training loss:  0.024401800357592596\n","valid loss  0.05213673529624939  and accuracy  0.9872\n","ep  143  training loss:  0.024472543083826133\n","valid loss  0.06841263920068741  and accuracy  0.9832\n","ep  144  training loss:  0.024330853720749283\n","valid loss  0.13430426664352416  and accuracy  0.9911\n","ep  145  training loss:  0.02401634447237508\n","valid loss  0.15935011497735976  and accuracy  0.9841\n","ep  146  training loss:  0.026449619718177105\n","valid loss  0.0345031136572361  and accuracy  0.9906\n","ep  147  training loss:  0.024952441359598735\n","valid loss  0.1126138548206538  and accuracy  0.9908\n","ep  148  training loss:  0.02444918865026039\n","valid loss  0.38148746524453164  and accuracy  0.9899\n","ep  149  training loss:  0.02526066334738853\n","valid loss  0.028117578664422035  and accuracy  0.9936\n","ep  150  training loss:  0.0242269581523053\n","valid loss  0.024376069527864457  and accuracy  0.9943\n","ep  151  training loss:  0.02438621987599108\n","valid loss  0.038635149890184406  and accuracy  0.9915\n","ep  152  training loss:  0.024197337066891006\n","valid loss  6.066288809931279  and accuracy  0.9861\n","ep  153  training loss:  0.024838596228315694\n","valid loss  0.045118053436279296  and accuracy  0.991\n","ep  154  training loss:  0.02408131189675111\n","valid loss  0.13941807436943054  and accuracy  0.9917\n","ep  155  training loss:  0.024656499354954193\n","valid loss  0.3571519451141357  and accuracy  0.9939\n","ep  156  training loss:  0.023805343052370226\n","valid loss  3.609613151359558  and accuracy  0.9913\n","ep  157  training loss:  0.02426580492170824\n","valid loss  0.034659604358673095  and accuracy  0.9931\n","ep  158  training loss:  0.023555959599792908\n","valid loss  2.1578621573984624  and accuracy  0.9857\n","ep  159  training loss:  0.024111185714909042\n","valid loss  0.03303979845643044  and accuracy  0.9926\n","ep  160  training loss:  0.02468640807992754\n","valid loss  0.07821370928287506  and accuracy  0.988\n","ep  161  training loss:  0.02387179330918914\n","valid loss  0.027417595145106315  and accuracy  0.9935\n","ep  162  training loss:  0.024030092546921525\n","valid loss  8.252083549296856  and accuracy  0.9892\n","ep  163  training loss:  0.024068363308366387\n","valid loss  4.645978890180587  and accuracy  0.9828\n","ep  164  training loss:  0.023875732540006026\n","valid loss  0.03698447558581829  and accuracy  0.9892\n","ep  165  training loss:  0.024400255738838104\n","valid loss  2.951805888545513  and accuracy  0.9904\n","ep  166  training loss:  0.023786470278741513\n","valid loss  0.03139967322200537  and accuracy  0.992\n","ep  167  training loss:  0.023993622100849366\n","valid loss  0.08783136506080627  and accuracy  0.9864\n","ep  168  training loss:  0.024267947069781054\n","valid loss  0.2656176008701324  and accuracy  0.993\n","ep  169  training loss:  0.024163852786365892\n","valid loss  0.8193967398822307  and accuracy  0.9936\n","ep  170  training loss:  0.023577032632556137\n","valid loss  0.02598041270971298  and accuracy  0.994\n","ep  171  training loss:  0.024637432052598874\n","valid loss  0.08612765755653382  and accuracy  0.9891\n","ep  172  training loss:  0.02419609613220581\n","valid loss  0.023962682557106017  and accuracy  0.9946\n","ep  173  training loss:  0.023911740031159517\n","valid loss  0.06633976491689682  and accuracy  0.9912\n","ep  174  training loss:  0.024043287562035668\n","valid loss  0.023148580756783485  and accuracy  0.9945\n","ep  175  training loss:  0.023742161998004732\n","valid loss  0.03749084538817406  and accuracy  0.9919\n","ep  176  training loss:  0.023011063401560835\n","valid loss  0.03530444667339325  and accuracy  0.993\n","ep  177  training loss:  0.0235925538301739\n","valid loss  0.027721855330467223  and accuracy  0.9933\n","ep  178  training loss:  0.02383132861634116\n","valid loss  0.3884474279999733  and accuracy  0.9847\n","ep  179  training loss:  0.024582557092584868\n","valid loss  1.4620206534862519  and accuracy  0.9898\n","ep  180  training loss:  0.024854998761908543\n","valid loss  0.1323215082883835  and accuracy  0.974\n","ep  181  training loss:  0.025047168160557697\n","valid loss  0.09693472135066986  and accuracy  0.992\n","ep  182  training loss:  0.024134765933097265\n","valid loss  0.15013833824545145  and accuracy  0.9892\n","ep  183  training loss:  0.024680914536268354\n","valid loss  0.03722428053617478  and accuracy  0.9914\n","ep  184  training loss:  0.023590539996956074\n","valid loss  0.12064194107055665  and accuracy  0.9826\n","ep  185  training loss:  0.023994829465623712\n","valid loss  0.02774710159301758  and accuracy  0.9932\n","ep  186  training loss:  0.023542753140151196\n","valid loss  0.024719052952528  and accuracy  0.9935\n","ep  187  training loss:  0.023494836808716446\n","valid loss  0.028273775911331178  and accuracy  0.9925\n","ep  188  training loss:  0.02367200606365229\n","valid loss  0.2592420926898718  and accuracy  0.9867\n","ep  189  training loss:  0.023601786161629796\n","valid loss  0.022336686730384827  and accuracy  0.9945\n","ep  190  training loss:  0.024452554800879635\n","valid loss  0.03767133342027664  and accuracy  0.992\n","ep  191  training loss:  0.02360285194732385\n","valid loss  2.4133387256145475  and accuracy  0.9872\n","ep  192  training loss:  0.024030596818004936\n","valid loss  2.7457290889322756  and accuracy  0.9921\n","ep  193  training loss:  0.023781753021956353\n","valid loss  0.0479254954457283  and accuracy  0.9921\n","ep  194  training loss:  0.02313776560354061\n","valid loss  0.26002451801300047  and accuracy  0.9874\n","ep  195  training loss:  0.024050960714850363\n","valid loss  0.04513144156932831  and accuracy  0.9836\n","ep  196  training loss:  0.023696306193703758\n","valid loss  0.02620847441777587  and accuracy  0.9938\n","ep  197  training loss:  0.02321236401575079\n","valid loss  0.038042311619967226  and accuracy  0.9918\n","ep  198  training loss:  0.02326018599237141\n","valid loss  0.5734557018756866  and accuracy  0.9774\n","ep  199  training loss:  0.023605582417543056\n","valid loss  0.04252207494974136  and accuracy  0.9929\n","ep  200  training loss:  0.022860202821478895\n","valid loss  0.030651149368286133  and accuracy  0.9911\n","ep  201  training loss:  0.023539361865636998\n","valid loss  0.028700347208976747  and accuracy  0.9933\n","ep  202  training loss:  0.02297053498710583\n","valid loss  0.022527461436390877  and accuracy  0.9946\n","ep  203  training loss:  0.0235075306131885\n","valid loss  0.03187444443106651  and accuracy  0.9873\n","ep  204  training loss:  0.023682618125878158\n","valid loss  0.0374616672039032  and accuracy  0.9857\n","ep  205  training loss:  0.023380144362909434\n","valid loss  0.036354441973567006  and accuracy  0.9926\n","ep  206  training loss:  0.023342059401162223\n","valid loss  0.03403870117664337  and accuracy  0.9917\n","ep  207  training loss:  0.024307180944818558\n","valid loss  0.03974441616535187  and accuracy  0.9898\n","ep  208  training loss:  0.022947718724211735\n","valid loss  0.026920012321323156  and accuracy  0.9947\n","ep  209  training loss:  0.022980791569466634\n","valid loss  6.384925607728958  and accuracy  0.9317\n","ep  210  training loss:  0.02272986899592291\n","valid loss  0.02407669185400009  and accuracy  0.993\n","ep  211  training loss:  0.02326683173470861\n","valid loss  0.029376246505975725  and accuracy  0.9943\n","ep  212  training loss:  0.0235157397002322\n","valid loss  0.028046172672510146  and accuracy  0.9934\n","ep  213  training loss:  0.022882056576832274\n","valid loss  0.033545089164376256  and accuracy  0.9941\n","ep  214  training loss:  0.02305831938586491\n","valid loss  0.02413169839978218  and accuracy  0.9943\n","ep  215  training loss:  0.023769974864894797\n","valid loss  0.029359874269366263  and accuracy  0.9929\n","ep  216  training loss:  0.023075246908909795\n","valid loss  0.05156480793952942  and accuracy  0.9891\n","ep  217  training loss:  0.02384713228327289\n","valid loss  0.02655566958785057  and accuracy  0.9938\n","ep  218  training loss:  0.02266697052427787\n","valid loss  1.7570361680448054  and accuracy  0.9906\n","ep  219  training loss:  0.02281811031959579\n","valid loss  0.04750640172958374  and accuracy  0.9891\n","ep  220  training loss:  0.023404786020800938\n","valid loss  0.02559939197450876  and accuracy  0.995\n","ep  221  training loss:  0.022385311878025744\n","valid loss  0.026222437116503714  and accuracy  0.9935\n","ep  222  training loss:  0.022687468054745773\n","valid loss  0.022709267222881316  and accuracy  0.9949\n","ep  223  training loss:  0.023721153074089403\n","valid loss  0.072578574898839  and accuracy  0.9906\n","ep  224  training loss:  0.022836410655647385\n","valid loss  1.1947611201763153  and accuracy  0.99\n","ep  225  training loss:  0.02346130372391976\n","valid loss  0.16401948297023775  and accuracy  0.9859\n","ep  226  training loss:  0.023046707256712165\n","valid loss  0.07564839801192283  and accuracy  0.9903\n","ep  227  training loss:  0.023195864302098815\n","valid loss  0.024091991844773293  and accuracy  0.9945\n","ep  228  training loss:  0.02277515653537086\n","valid loss  0.09922349391579628  and accuracy  0.9925\n","ep  229  training loss:  0.02251767676203088\n","valid loss  0.030922887569665908  and accuracy  0.9937\n","ep  230  training loss:  0.0232124810317196\n","valid loss  0.04006085140705109  and accuracy  0.9879\n","ep  231  training loss:  0.022952037252155028\n","valid loss  0.027332881927490234  and accuracy  0.9923\n","ep  232  training loss:  0.022470792923825124\n","valid loss  0.023408029106259345  and accuracy  0.9938\n","ep  233  training loss:  0.022917549149239172\n","valid loss  0.0517176482796669  and accuracy  0.9913\n","ep  234  training loss:  0.022604957609367857\n","valid loss  0.027493318095058204  and accuracy  0.9937\n","ep  235  training loss:  0.023169340076174206\n","valid loss  0.03275010416507721  and accuracy  0.9924\n","ep  236  training loss:  0.0226311513454485\n","valid loss  0.06691843473911285  and accuracy  0.9881\n","ep  237  training loss:  0.023286599150706036\n","valid loss  0.03850770970582962  and accuracy  0.9927\n","ep  238  training loss:  0.022331765730908296\n","valid loss  0.046829307091236116  and accuracy  0.9936\n","ep  239  training loss:  0.022684325928684225\n","valid loss  0.09812238724827767  and accuracy  0.9858\n","ep  240  training loss:  0.022900949164791466\n","valid loss  0.02667561559677124  and accuracy  0.9929\n","ep  241  training loss:  0.02315299608762941\n","valid loss  0.02750939812362194  and accuracy  0.9936\n","ep  242  training loss:  0.022529313589800197\n","valid loss  0.02196107664257288  and accuracy  0.9943\n","ep  243  training loss:  0.022642770068530306\n","valid loss  0.13105293361842632  and accuracy  0.9893\n","ep  244  training loss:  0.022540709185155862\n","valid loss  0.05179019759595394  and accuracy  0.9907\n","ep  245  training loss:  0.02270333916644856\n","valid loss  0.04937568238377571  and accuracy  0.9909\n","ep  246  training loss:  0.022571143864404744\n","valid loss  0.03206004938930273  and accuracy  0.9935\n","ep  247  training loss:  0.02251379224577818\n","valid loss  0.05527905280739069  and accuracy  0.9939\n","ep  248  training loss:  0.022154844775994605\n","valid loss  0.06283888362050057  and accuracy  0.9865\n","ep  249  training loss:  0.022927301891556985\n","valid loss  0.04566019992232323  and accuracy  0.9879\n","ep  250  training loss:  0.022917516759716663\n","valid loss  0.053350702834129335  and accuracy  0.9846\n","ep  251  training loss:  0.023060003815227215\n","valid loss  0.918004154086113  and accuracy  0.9824\n","ep  252  training loss:  0.02263012936403674\n","valid loss  0.03541870741248131  and accuracy  0.9927\n","ep  253  training loss:  0.022742485157171824\n","valid loss  0.0365811478972435  and accuracy  0.9908\n","ep  254  training loss:  0.022337845447866263\n","valid loss  0.06357607064247131  and accuracy  0.9916\n","ep  255  training loss:  0.022508778332725524\n","valid loss  0.041425520730018615  and accuracy  0.9943\n","ep  256  training loss:  0.022814577380560946\n","valid loss  0.027161523389816283  and accuracy  0.9928\n","ep  257  training loss:  0.023023769644323955\n","valid loss  0.08656777749061584  and accuracy  0.9872\n","ep  258  training loss:  0.022614262851569995\n","valid loss  0.021811673963069916  and accuracy  0.9949\n","ep  259  training loss:  0.02246961280294262\n","valid loss  0.022695692813768983  and accuracy  0.9935\n","ep  260  training loss:  0.023155020836761315\n","valid loss  0.8729324357628823  and accuracy  0.9826\n","ep  261  training loss:  0.023408756192858177\n","valid loss  0.05979340593516826  and accuracy  0.9906\n","ep  262  training loss:  0.022410568466979783\n","valid loss  0.02585370018184185  and accuracy  0.9948\n","ep  263  training loss:  0.02301650367663318\n","valid loss  0.035953091144561765  and accuracy  0.9869\n","ep  264  training loss:  0.022347868074478094\n","valid loss  0.027250034791231156  and accuracy  0.9933\n","ep  265  training loss:  0.0224960107308201\n","valid loss  0.024098409305885433  and accuracy  0.993\n","ep  266  training loss:  0.02255497687301813\n","valid loss  0.252031928730011  and accuracy  0.9554\n","ep  267  training loss:  0.02210718548339018\n","valid loss  0.03917834602594376  and accuracy  0.9939\n","ep  268  training loss:  0.022413285373315665\n","valid loss  0.023916626864671706  and accuracy  0.9946\n","ep  269  training loss:  0.022815234554842023\n","valid loss  0.048745220577716825  and accuracy  0.9912\n","ep  270  training loss:  0.021724077108497247\n","valid loss  0.13701899151802063  and accuracy  0.9866\n","ep  271  training loss:  0.022056975412017458\n","valid loss  26.318798919701575  and accuracy  0.9913\n","ep  272  training loss:  0.023026595873766865\n","valid loss  0.027549953150749207  and accuracy  0.9927\n","ep  273  training loss:  0.021915144911864817\n","valid loss  0.04747964062690735  and accuracy  0.992\n","ep  274  training loss:  0.022328761190710467\n","valid loss  0.04333872785568237  and accuracy  0.9922\n","ep  275  training loss:  0.023270661133882813\n","valid loss  0.06845789133906365  and accuracy  0.9803\n","ep  276  training loss:  0.022144936835267845\n","valid loss  0.07855903203487397  and accuracy  0.9879\n","ep  277  training loss:  0.022255981720275788\n","valid loss  0.030697745233774186  and accuracy  0.9914\n","ep  278  training loss:  0.0221466965883522\n","valid loss  0.06948116356134415  and accuracy  0.9917\n","ep  279  training loss:  0.022744501371244178\n","valid loss  1.048160989522934  and accuracy  0.9901\n","ep  280  training loss:  0.022341399020612527\n","valid loss  0.06170609571933746  and accuracy  0.9866\n","ep  281  training loss:  0.022330141598689377\n","valid loss  0.02794899037182331  and accuracy  0.9938\n","ep  282  training loss:  0.022530069004451286\n","valid loss  0.02688040474951267  and accuracy  0.993\n","ep  283  training loss:  0.022131231098299266\n","valid loss  1.6719143897056579  and accuracy  0.9823\n","ep  284  training loss:  0.022630777329652626\n","valid loss  0.020470624661445617  and accuracy  0.9952\n","ep  285  training loss:  0.022411002200356217\n","valid loss  0.03231280295625329  and accuracy  0.9932\n","ep  286  training loss:  0.022019544163332423\n","valid loss  0.026692663809657097  and accuracy  0.9935\n","ep  287  training loss:  0.02242009073592472\n","valid loss  0.03356246554553509  and accuracy  0.9934\n","ep  288  training loss:  0.023027999410306584\n","valid loss  0.03221238560527563  and accuracy  0.9922\n","ep  289  training loss:  0.021939748170120114\n","valid loss  0.05023327038884163  and accuracy  0.9909\n","ep  290  training loss:  0.02200626883574696\n","valid loss  0.4677046157777309  and accuracy  0.9938\n","ep  291  training loss:  0.021983015687013063\n","valid loss  0.035418396851420404  and accuracy  0.9941\n","ep  292  training loss:  0.021707837729719015\n","valid loss  0.10989571359455585  and accuracy  0.9842\n","ep  293  training loss:  0.022738868587531758\n","valid loss  0.14681052265167235  and accuracy  0.9827\n","ep  294  training loss:  0.023021739025452764\n","valid loss  0.024921984094381334  and accuracy  0.9939\n","ep  295  training loss:  0.022202543982748277\n","valid loss  0.04184042443633079  and accuracy  0.9894\n","ep  296  training loss:  0.02181966936914798\n","valid loss  0.03822944217920304  and accuracy  0.993\n","ep  297  training loss:  0.022301165702598883\n","valid loss  0.03273155761957169  and accuracy  0.9917\n","ep  298  training loss:  0.02261818446938718\n","valid loss  0.0362980521529913  and accuracy  0.9907\n","ep  299  training loss:  0.022206761432594868\n","valid loss  0.08125397412776947  and accuracy  0.9859\n","ep  300  training loss:  0.022336357315232345\n","valid loss  0.08445813598036767  and accuracy  0.9901\n","ep  301  training loss:  0.022089924442238108\n","valid loss  0.05762567103505135  and accuracy  0.988\n","ep  302  training loss:  0.021740207322421382\n","valid loss  0.08038240488171577  and accuracy  0.9899\n","ep  303  training loss:  0.02183921307082027\n","valid loss  0.02841272155046463  and accuracy  0.9929\n","ep  304  training loss:  0.022382898286924316\n","valid loss  0.03738730337917805  and accuracy  0.9904\n","ep  305  training loss:  0.022295472269480164\n","valid loss  0.08290415964275598  and accuracy  0.9901\n","ep  306  training loss:  0.021964936257011188\n","valid loss  0.052209569269418714  and accuracy  0.9909\n","ep  307  training loss:  0.022273011595678113\n","valid loss  0.021756565335020423  and accuracy  0.9948\n","ep  308  training loss:  0.021855353688487063\n","valid loss  0.027865668767690657  and accuracy  0.9923\n","ep  309  training loss:  0.021712709330907253\n","valid loss  0.08018458970487118  and accuracy  0.9894\n","ep  310  training loss:  0.02215528386773423\n","valid loss  0.04177537070512772  and accuracy  0.9934\n","ep  311  training loss:  0.02186697637923774\n","valid loss  0.06517413544654846  and accuracy  0.9833\n","ep  312  training loss:  0.021657089220511228\n","valid loss  0.05785743452906609  and accuracy  0.9918\n","ep  313  training loss:  0.02211903447948853\n","valid loss  0.2319162208557129  and accuracy  0.9875\n","ep  314  training loss:  0.021802471584185114\n","valid loss  0.02818630332350731  and accuracy  0.9939\n","ep  315  training loss:  0.021588191705309845\n","valid loss  0.032256154876947406  and accuracy  0.9913\n","ep  316  training loss:  0.021934460682428637\n","valid loss  0.026288573694229125  and accuracy  0.994\n","ep  317  training loss:  0.021865024429799096\n","valid loss  0.07476447722017765  and accuracy  0.9917\n","ep  318  training loss:  0.021898930744093693\n","valid loss  0.0360784973949194  and accuracy  0.9874\n","ep  319  training loss:  0.02160553040640306\n","valid loss  0.022780221965909005  and accuracy  0.9951\n","ep  320  training loss:  0.02160991925584736\n","valid loss  0.1515385735988617  and accuracy  0.9892\n","ep  321  training loss:  0.022010627909114514\n","valid loss  1.5369461228370667  and accuracy  0.9854\n","ep  322  training loss:  0.021939694363992693\n","valid loss  0.02633270260095596  and accuracy  0.9931\n","ep  323  training loss:  0.022095885144078817\n","valid loss  0.06548211483955384  and accuracy  0.9918\n","ep  324  training loss:  0.022183407032026894\n","valid loss  0.02273048702478409  and accuracy  0.9949\n","ep  325  training loss:  0.02204685525407368\n","valid loss  0.024275191079080105  and accuracy  0.9944\n","ep  326  training loss:  0.02236502691325593\n","valid loss  0.045657323181629184  and accuracy  0.9895\n","ep  327  training loss:  0.022128620121007563\n","valid loss  0.021928927854821086  and accuracy  0.9954\n","ep  328  training loss:  0.02164870321438485\n","valid loss  0.0616187001824379  and accuracy  0.993\n","ep  329  training loss:  0.022022869549797503\n","valid loss  0.06126204627752304  and accuracy  0.9934\n","ep  330  training loss:  0.021892003716466633\n","valid loss  0.08984352712631226  and accuracy  0.9311\n","ep  331  training loss:  0.022040212985834737\n","valid loss  0.028165025699138643  and accuracy  0.9946\n","ep  332  training loss:  0.022343118055123627\n","valid loss  0.05341196796894074  and accuracy  0.9926\n","ep  333  training loss:  0.021658230169629287\n","valid loss  0.03798156362771988  and accuracy  0.9933\n","ep  334  training loss:  0.02213115017128717\n","valid loss  0.13149995563030242  and accuracy  0.9861\n","ep  335  training loss:  0.021734713160788545\n","valid loss  0.031043541210889817  and accuracy  0.9942\n","ep  336  training loss:  0.02182802358660172\n","valid loss  0.052868303081393245  and accuracy  0.9908\n","ep  337  training loss:  0.02115255874930944\n","valid loss  0.02166021459400654  and accuracy  0.9944\n","ep  338  training loss:  0.021703838832920135\n","valid loss  0.03513500284254551  and accuracy  0.9919\n","ep  339  training loss:  0.02220292587567727\n","valid loss  0.10343589806556702  and accuracy  0.9923\n","ep  340  training loss:  0.021675697960400083\n","valid loss  0.0645495132446289  and accuracy  0.9904\n","ep  341  training loss:  0.021304641333251943\n","valid loss  0.035824720799922945  and accuracy  0.9929\n","ep  342  training loss:  0.021730153732462485\n","valid loss  0.028669267761707307  and accuracy  0.9933\n","ep  343  training loss:  0.02192221183765008\n","valid loss  0.021599771693348885  and accuracy  0.9948\n","ep  344  training loss:  0.02129203890791135\n","valid loss  0.024230375009775162  and accuracy  0.9946\n","ep  345  training loss:  0.021794771852765803\n","valid loss  0.0499158665060997  and accuracy  0.9827\n","ep  346  training loss:  0.02258202851467195\n","valid loss  0.020050219097733496  and accuracy  0.9952\n","ep  347  training loss:  0.021973203448587643\n","valid loss  0.05602350449562073  and accuracy  0.9862\n","ep  348  training loss:  0.02173114727014112\n","valid loss  0.02613152923285961  and accuracy  0.9933\n","ep  349  training loss:  0.021706898172603115\n","valid loss  0.05451625769138336  and accuracy  0.9865\n","ep  350  training loss:  0.0215002802618744\n","valid loss  0.022023287235200407  and accuracy  0.995\n","ep  351  training loss:  0.021430082668922216\n","valid loss  0.02978550935983658  and accuracy  0.992\n","ep  352  training loss:  0.021899317318842412\n","valid loss  0.13594155057668686  and accuracy  0.9861\n","ep  353  training loss:  0.021656544751989585\n","valid loss  0.044412687188386916  and accuracy  0.991\n","ep  354  training loss:  0.02203570753499562\n","valid loss  0.027348933558166026  and accuracy  0.9937\n","ep  355  training loss:  0.021412078086341643\n","valid loss  0.36560899930000307  and accuracy  0.9754\n","ep  356  training loss:  0.021822799860981325\n","valid loss  0.03373098436295986  and accuracy  0.9902\n","ep  357  training loss:  0.021189578872291344\n","valid loss  0.04985794283151627  and accuracy  0.9929\n","ep  358  training loss:  0.02158307861098827\n","valid loss  0.030223069136589766  and accuracy  0.9927\n","ep  359  training loss:  0.021743056556095104\n","valid loss  0.04517726258635521  and accuracy  0.9861\n","ep  360  training loss:  0.021622386080505845\n","valid loss  0.021629944774508476  and accuracy  0.9946\n","ep  361  training loss:  0.021204892814335467\n","valid loss  0.02377340428829193  and accuracy  0.9952\n","ep  362  training loss:  0.02184217739563642\n","valid loss  0.02154737381488085  and accuracy  0.9951\n","ep  363  training loss:  0.0215276794324656\n","valid loss  0.023986344090104104  and accuracy  0.9941\n","ep  364  training loss:  0.021235837503527817\n","valid loss  0.08357879154682159  and accuracy  0.9889\n","ep  365  training loss:  0.02140108368838696\n","valid loss  0.024109778720140456  and accuracy  0.9945\n","ep  366  training loss:  0.02156293982138596\n","valid loss  0.036283118844032285  and accuracy  0.9871\n","ep  367  training loss:  0.021374000223829175\n","valid loss  0.040909520506858824  and accuracy  0.9917\n","ep  368  training loss:  0.021367344518406637\n","valid loss  0.04744500397443771  and accuracy  0.9903\n","ep  369  training loss:  0.021560124506006785\n","valid loss  0.025506968247890473  and accuracy  0.9924\n","ep  370  training loss:  0.021490840043602313\n","valid loss  0.04613164787814021  and accuracy  0.9931\n","ep  371  training loss:  0.021825907805821224\n","valid loss  0.06913810803294182  and accuracy  0.9927\n","ep  372  training loss:  0.02146255630005748\n","valid loss  0.1473989153981209  and accuracy  0.9895\n","ep  373  training loss:  0.02107792344933043\n","valid loss  0.025401674276590347  and accuracy  0.9938\n","ep  374  training loss:  0.02141096719982741\n","valid loss  0.04697682825922966  and accuracy  0.9939\n","ep  375  training loss:  0.021632121350774566\n","valid loss  0.021922825586795806  and accuracy  0.9947\n","ep  376  training loss:  0.0216083637768189\n","valid loss  0.06895928223133087  and accuracy  0.9922\n","ep  377  training loss:  0.021528769907759863\n","valid loss  0.0654633512377739  and accuracy  0.984\n","ep  378  training loss:  0.021532614187384416\n","valid loss  0.038170017850399016  and accuracy  0.9932\n","ep  379  training loss:  0.021917081550648598\n","valid loss  0.06957123584747314  and accuracy  0.9848\n","ep  380  training loss:  0.021948871300360263\n","valid loss  0.020386236703395844  and accuracy  0.9954\n","ep  381  training loss:  0.02105622069340815\n","valid loss  0.03338768258988857  and accuracy  0.9903\n","ep  382  training loss:  0.021350100367225015\n","valid loss  0.05253453377783299  and accuracy  0.9934\n","ep  383  training loss:  0.021313325754225457\n","valid loss  2.3137610159635544  and accuracy  0.9884\n","ep  384  training loss:  0.021453620234065485\n","valid loss  0.033040990620851515  and accuracy  0.9929\n","ep  385  training loss:  0.021008464148711114\n","valid loss  0.09568599416613578  and accuracy  0.9863\n","ep  386  training loss:  0.021630504429127412\n","valid loss  0.15286901330947875  and accuracy  0.9085\n","ep  387  training loss:  0.022072274770055498\n","valid loss  0.021784185373783113  and accuracy  0.994\n","ep  388  training loss:  0.021372815838939036\n","valid loss  0.033617321062088014  and accuracy  0.9937\n","ep  389  training loss:  0.021086970235664894\n","valid loss  0.04587080947160721  and accuracy  0.9929\n","ep  390  training loss:  0.021202820732968322\n","valid loss  0.019462040662765502  and accuracy  0.9954\n","ep  391  training loss:  0.021323333053391385\n","valid loss  0.16477091574072839  and accuracy  0.9935\n","ep  392  training loss:  0.02124586745781858\n","valid loss  0.021704411508142948  and accuracy  0.9951\n","ep  393  training loss:  0.020614335706490964\n","valid loss  0.024503126043081285  and accuracy  0.9883\n","ep  394  training loss:  0.02148352835566503\n","valid loss  0.02123128013908863  and accuracy  0.9949\n","ep  395  training loss:  0.021527269997488125\n","valid loss  0.07550092366933822  and accuracy  0.9282\n","ep  396  training loss:  0.02167018063530152\n","valid loss  0.022307241463661195  and accuracy  0.9951\n","ep  397  training loss:  0.021443212029747114\n","valid loss  0.02725761596262455  and accuracy  0.995\n","ep  398  training loss:  0.021568203921501213\n","valid loss  0.03135934891104698  and accuracy  0.9937\n","ep  399  training loss:  0.021231742677647082\n","valid loss  0.23058814879655837  and accuracy  0.9948\n","ep  400  training loss:  0.021238537031354265\n","valid loss  0.08454836373329162  and accuracy  0.9845\n","ep  401  training loss:  0.021130416892395823\n","valid loss  0.05046961694955826  and accuracy  0.9865\n","ep  402  training loss:  0.021081912387491287\n","valid loss  0.050804115468263626  and accuracy  0.9852\n","ep  403  training loss:  0.021105978127434113\n","valid loss  0.05822727411389351  and accuracy  0.994\n","ep  404  training loss:  0.021227831492269086\n","valid loss  0.02603771818727255  and accuracy  0.9933\n","ep  405  training loss:  0.021500275180088645\n","valid loss  0.02447181124687195  and accuracy  0.9948\n","ep  406  training loss:  0.021453606002241162\n","valid loss  0.026694070744514464  and accuracy  0.9937\n","ep  407  training loss:  0.020989436166666996\n","valid loss  0.021199724626541138  and accuracy  0.9953\n","ep  408  training loss:  0.021301038778345636\n","valid loss  0.04754433082938194  and accuracy  0.99\n","ep  409  training loss:  0.021041333459316514\n","valid loss  0.031193801537156104  and accuracy  0.9932\n","ep  410  training loss:  0.021724276373677357\n","valid loss  0.049844602155685425  and accuracy  0.9874\n","ep  411  training loss:  0.02108948472007229\n","valid loss  0.0723171441435814  and accuracy  0.9923\n","ep  412  training loss:  0.021567650243616444\n","valid loss  0.02487389059662819  and accuracy  0.9936\n","ep  413  training loss:  0.021518532466505324\n","valid loss  0.04018895621299744  and accuracy  0.9871\n","ep  414  training loss:  0.021294414815312843\n","valid loss  0.022998681062459947  and accuracy  0.9952\n","ep  415  training loss:  0.020962295826921766\n","valid loss  0.03333562517166138  and accuracy  0.9939\n","ep  416  training loss:  0.021497230277650412\n","valid loss  0.05063591609597206  and accuracy  0.9842\n","ep  417  training loss:  0.020906945500901296\n","valid loss  0.033220320105552675  and accuracy  0.9932\n","ep  418  training loss:  0.02095656642169381\n","valid loss  0.2161939509153366  and accuracy  0.9839\n","ep  419  training loss:  0.021689974106451553\n","valid loss  0.04617662167549133  and accuracy  0.9929\n","ep  420  training loss:  0.021472375986612603\n","valid loss  0.06197060885429383  and accuracy  0.9924\n","ep  421  training loss:  0.021383862654647864\n","valid loss  0.05746950313746929  and accuracy  0.9923\n","ep  422  training loss:  0.02135050704923445\n","valid loss  0.02424929687678814  and accuracy  0.995\n","ep  423  training loss:  0.02087611048490393\n","valid loss  0.08772101916670799  and accuracy  0.9857\n","ep  424  training loss:  0.021339367310226957\n","valid loss  0.05146464674472809  and accuracy  0.9865\n","ep  425  training loss:  0.0212896786017658\n","valid loss  0.028759153936058282  and accuracy  0.9946\n","ep  426  training loss:  0.02116068208370998\n","valid loss  0.04051792035102844  and accuracy  0.9879\n","ep  427  training loss:  0.021155807422315775\n","valid loss  0.021503270292282106  and accuracy  0.9944\n","ep  428  training loss:  0.021088151394272756\n","valid loss  0.023920699295401575  and accuracy  0.995\n","ep  429  training loss:  0.021093957112506497\n","valid loss  0.03475059888511896  and accuracy  0.9906\n","ep  430  training loss:  0.02091136934321814\n","valid loss  0.027861509943008424  and accuracy  0.9936\n","ep  431  training loss:  0.021211377696704135\n","valid loss  0.02815107940733433  and accuracy  0.9925\n","ep  432  training loss:  0.020893812428874666\n","valid loss  0.04066101975440979  and accuracy  0.9914\n","ep  433  training loss:  0.021005443433359163\n","valid loss  0.03866892762184143  and accuracy  0.9926\n","ep  434  training loss:  0.021081689592291078\n","valid loss  0.025444670361280442  and accuracy  0.995\n","ep  435  training loss:  0.02100996122055823\n","valid loss  0.06902300100326537  and accuracy  0.9915\n","ep  436  training loss:  0.02095112030877623\n","valid loss  0.021276706485450266  and accuracy  0.9946\n","ep  437  training loss:  0.020941914861572236\n","valid loss  0.018847581891715526  and accuracy  0.9954\n","ep  438  training loss:  0.020510152067828944\n","valid loss  0.035882658088207245  and accuracy  0.9888\n","ep  439  training loss:  0.021164736024287843\n","valid loss  0.08499882048964501  and accuracy  0.9916\n","ep  440  training loss:  0.020864218822272608\n","valid loss  0.022908838118612766  and accuracy  0.9946\n","ep  441  training loss:  0.020945319139803065\n","valid loss  0.02206475426405668  and accuracy  0.9946\n","ep  442  training loss:  0.02052428220090999\n","valid loss  0.07911861222982407  and accuracy  0.9872\n","ep  443  training loss:  0.021290796583887123\n","valid loss  0.0326617307741195  and accuracy  0.9927\n","ep  444  training loss:  0.020889112396071035\n","valid loss  0.025235279500484466  and accuracy  0.9948\n","ep  445  training loss:  0.021074978906260883\n","valid loss  0.03394947266578675  and accuracy  0.993\n","ep  446  training loss:  0.02135419850940554\n","valid loss  0.034142859230935574  and accuracy  0.9867\n","ep  447  training loss:  0.02065341917259993\n","valid loss  0.024348309180140495  and accuracy  0.9943\n","ep  448  training loss:  0.021396353206452796\n","valid loss  0.023945655345916746  and accuracy  0.9952\n","ep  449  training loss:  0.020759431118446088\n","valid loss  0.01918621119260788  and accuracy  0.9953\n","ep  450  training loss:  0.020579834705230587\n","valid loss  0.02537954934835434  and accuracy  0.9945\n","ep  451  training loss:  0.021006780080652435\n","valid loss  0.03253318604826927  and accuracy  0.9936\n","ep  452  training loss:  0.020904386405981887\n","valid loss  0.035203977847099305  and accuracy  0.9926\n","ep  453  training loss:  0.020994651497748328\n","valid loss  0.02679523312449455  and accuracy  0.9943\n","ep  454  training loss:  0.020695394824874577\n","valid loss  0.023321445181965827  and accuracy  0.9943\n","ep  455  training loss:  0.021095738007760836\n","valid loss  0.04453429334163666  and accuracy  0.9875\n","ep  456  training loss:  0.021190830994176724\n","valid loss  0.042056462813168764  and accuracy  0.9925\n","ep  457  training loss:  0.020832424770008576\n","valid loss  0.02756144552230835  and accuracy  0.9922\n","ep  458  training loss:  0.02046012246048983\n","valid loss  0.029567315888404846  and accuracy  0.9918\n","ep  459  training loss:  0.02090785046491591\n","valid loss  0.02153827467560768  and accuracy  0.9949\n","ep  460  training loss:  0.0210693713785481\n","valid loss  0.06797506353855133  and accuracy  0.9853\n","ep  461  training loss:  0.021078457797487668\n","valid loss  0.022905552496016026  and accuracy  0.9943\n","ep  462  training loss:  0.02037014630123326\n","valid loss  0.05794239275455475  and accuracy  0.9814\n","ep  463  training loss:  0.02070631528667299\n","valid loss  0.022269899295270443  and accuracy  0.9948\n","ep  464  training loss:  0.0209890004079127\n","valid loss  0.06202583605647087  and accuracy  0.9847\n","ep  465  training loss:  0.02109383852775606\n","valid loss  0.9713015964925289  and accuracy  0.993\n","ep  466  training loss:  0.020671484255634268\n","valid loss  0.03184908566176892  and accuracy  0.9927\n","ep  467  training loss:  0.02066839570177796\n","valid loss  0.03022035646736622  and accuracy  0.9932\n","ep  468  training loss:  0.0205192451769397\n","valid loss  0.03971045436859131  and accuracy  0.9909\n","ep  469  training loss:  0.020733623906667997\n","valid loss  0.03146518308520317  and accuracy  0.9919\n","ep  470  training loss:  0.020558944317964914\n","valid loss  0.0639942598849535  and accuracy  0.9916\n","ep  471  training loss:  0.020480226934785618\n","valid loss  0.026354997277259828  and accuracy  0.9942\n","ep  472  training loss:  0.021340151134964163\n","valid loss  0.019939413495361803  and accuracy  0.9949\n","ep  473  training loss:  0.02126256604947472\n","valid loss  0.03523006825447082  and accuracy  0.9903\n","ep  474  training loss:  0.020635741945757298\n","valid loss  0.03212971182465553  and accuracy  0.9936\n","ep  475  training loss:  0.020626657913758905\n","valid loss  0.04597290604710579  and accuracy  0.9899\n","ep  476  training loss:  0.020745365868272674\n","valid loss  0.02596057171225548  and accuracy  0.9939\n","ep  477  training loss:  0.021077697041300948\n","valid loss  0.037685166627168656  and accuracy  0.9929\n","ep  478  training loss:  0.020739587443532922\n","valid loss  0.02147115152478218  and accuracy  0.995\n","ep  479  training loss:  0.020334148430543444\n","valid loss  0.028830414488166572  and accuracy  0.9939\n","ep  480  training loss:  0.020048360710300698\n","valid loss  0.03942142105102539  and accuracy  0.9909\n","ep  481  training loss:  0.020675360546954475\n","valid loss  0.026688615031540394  and accuracy  0.9945\n","ep  482  training loss:  0.02054797855623573\n","valid loss  0.05347754912376404  and accuracy  0.9906\n","ep  483  training loss:  0.02080805134481043\n","valid loss  0.04257364273965359  and accuracy  0.988\n","ep  484  training loss:  0.02108888294587465\n","valid loss  0.04447923833727837  and accuracy  0.9873\n","ep  485  training loss:  0.021365567210186343\n","valid loss  0.021417040023207663  and accuracy  0.9953\n","ep  486  training loss:  0.020509016401421058\n","valid loss  0.029715639609098433  and accuracy  0.9927\n","ep  487  training loss:  0.020361959751975932\n","valid loss  0.05576057014614343  and accuracy  0.9939\n","ep  488  training loss:  0.02055201149621101\n","valid loss  0.06692440956830978  and accuracy  0.9836\n","ep  489  training loss:  0.02099073199055458\n","valid loss  0.042420492300391195  and accuracy  0.9915\n","ep  490  training loss:  0.02036600161788739\n","valid loss  0.03950365136861801  and accuracy  0.9932\n","ep  491  training loss:  0.02059900427449488\n","valid loss  0.03443790912926197  and accuracy  0.9928\n","ep  492  training loss:  0.020630030022852696\n","valid loss  0.3062758261680603  and accuracy  0.9879\n","ep  493  training loss:  0.02087494566613063\n","valid loss  0.029165711897611617  and accuracy  0.9942\n","ep  494  training loss:  0.02067556786842626\n","valid loss  0.02362297176718712  and accuracy  0.993\n","ep  495  training loss:  0.020653510005288065\n","valid loss  0.1262509039402008  and accuracy  0.9827\n","ep  496  training loss:  0.02100985257224171\n","valid loss  0.021509160220623016  and accuracy  0.9952\n","ep  497  training loss:  0.020493111208504357\n","valid loss  0.04907059650719166  and accuracy  0.992\n","ep  498  training loss:  0.020508561366258296\n","valid loss  0.026608007389307024  and accuracy  0.9947\n","ep  499  training loss:  0.020473370461640345\n","valid loss  0.02108826690725982  and accuracy  0.9949\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lyj8CC2dr8P7"},"source":["### ***PREDICTION***"]},{"cell_type":"code","metadata":{"id":"1WL6v0uDgDJA"},"source":["\"\"\" Effettuiamo le predizioni sul dataset di test \"\"\"\n","\n","test_ds = CIDDS_Dataset(test, np.zeros(len(test)))\n","test_dl = DataLoader(test_ds, batch_size=batch_size)\n","test_dl = DeviceDataLoader(test_dl, device)\n","\n","# Utilizziamo la funzione softmax poiché siamo interessati alla probabilità per ogni classe\n","preds = []\n","model.eval()\n","with torch.no_grad():\n","    for x, y in test_dl:\n","        out = model(x)\n","        prob = F.softmax(out, dim=1)\n","        preds.append(prob)\n","        \n","y_pred = []\n","for i in range(0, len(preds)):\n","  pred = preds[i].cpu()\n","  temp = np.argmax(pred, 1)\n","  temp = np.array(temp)\n","  y_pred = np.append(y_pred, temp)\n","\n","y_pred = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3hmjVkNmFkX","executionInfo":{"status":"ok","timestamp":1625215651930,"user_tz":-120,"elapsed":7,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"13984619-1842-4824-ed48-d64a8d67dc01"},"source":["y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 1, 1, ..., 1, 1, 2])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"IsdRVjr9sFbO"},"source":["### ***EVALUATION***"]},{"cell_type":"code","metadata":{"id":"WOciCjPisC_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625215652272,"user_tz":-120,"elapsed":343,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"6d400f5b-9dca-43f6-c062-f33e9df44a14"},"source":["print('Test:', Counter(y_test))\n","print('Pred:', Counter(y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: Counter({2: 121584, 1: 59074, 4: 18846, 3: 298, 0: 198})\n","Pred: Counter({2: 121394, 1: 59075, 4: 18508, 3: 669, 0: 354})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oqp05uc1-d4t"},"source":["# Matrice di confusione, accuracy, classification_report\n","from sklearn.metrics import *\n","\n","# y_test è la variabile che contiene i valori effettivi\n","# y_pred contiene i valori predetti dal modello\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","acc = accuracy_score(y_test, y_pred)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","# non presente nella libreria, calcolo mediante formula\n","f2 = (1+2**2)*((precision*recall)/((2**2*precision)+recall))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"8rOxIo2L-d4z","executionInfo":{"status":"ok","timestamp":1625215653372,"user_tz":-120,"elapsed":581,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"989d7494-8109-45d2-d39e-98ca3c6c60ad"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","disp = ConfusionMatrixDisplay(cm, target_dict)\n","disp.plot()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fc20c622750>"]},"metadata":{"tags":[]},"execution_count":31},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELCQniZh-d40","executionInfo":{"status":"ok","timestamp":1625215653373,"user_tz":-120,"elapsed":33,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"885b3e9f-0da3-41b7-94ca-50c0ec90e2ca"},"source":["mcm = multilabel_confusion_matrix(y_test, y_pred)\n","print(mcm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[199642    160]\n","  [     4    194]]\n","\n"," [[140913     13]\n","  [    12  59062]]\n","\n"," [[ 78144    272]\n","  [   462 121122]]\n","\n"," [[199326    376]\n","  [     5    293]]\n","\n"," [[180867    287]\n","  [   625  18221]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7P4q-xyB7_n","executionInfo":{"status":"ok","timestamp":1625215653375,"user_tz":-120,"elapsed":24,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"9261f945-452f-4521-c985-943b860fef7a"},"source":["FP = cm.sum (axis = 0) - np.diag (cm) \n","FN = cm.sum (axis = 1) - np.diag (cm) \n","TP = np.diag (cm) \n","TN = cm.sum () - (FP + FN + TP)\n","\n","print('True positive: ', TP)\n","print('True negative: ', TN)\n","print('False positive: ', FP)\n","print('False negative: ', FN)\n","\n","FP = FP.astype(float)\n","FN = FN.astype(float)\n","TP = TP.astype(float)\n","TN = TN.astype(float)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","# False negative rate\n","FNR = FN/(TP+FN)\n","\n","print('True positive rate: ', TPR)\n","print('True negative rate: ', TNR)\n","print('False positive rate: ', FPR)\n","print('False negative rate: ', FNR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True positive:  [   194  59062 121122    293  18221]\n","True negative:  [199642 140913  78144 199326 180867]\n","False positive:  [160  13 272 376 287]\n","False negative:  [  4  12 462   5 625]\n","True positive rate:  [0.97979798 0.99979686 0.99620016 0.98322148 0.96683646]\n","True negative rate:  [0.99919921 0.99990775 0.99653132 0.99811719 0.99841571]\n","False positive rate:  [8.00792785e-04 9.22469949e-05 3.46867986e-03 1.88280538e-03\n"," 1.58428740e-03]\n","False negative rate:  [0.02020202 0.00020314 0.00379984 0.01677852 0.03316354]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89Nap2dd-d40","executionInfo":{"status":"ok","timestamp":1625215653376,"user_tz":-120,"elapsed":21,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"b1c7f36c-0cbf-4b25-d107-f59f667f9b1b"},"source":["print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   194      0      4      0      0]\n"," [     0  59062     12      0      0]\n"," [   160     13 121122      3    286]\n"," [     0      0      4    293      1]\n"," [     0      0    252    373  18221]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wE1uRjas-d41","executionInfo":{"status":"ok","timestamp":1625215653376,"user_tz":-120,"elapsed":14,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"f636861f-051d-415c-8a97-b64c2d7e8dfa"},"source":["print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.55      0.98      0.70       198\n","           1       1.00      1.00      1.00     59074\n","           2       1.00      1.00      1.00    121584\n","           3       0.44      0.98      0.61       298\n","           4       0.98      0.97      0.98     18846\n","\n","    accuracy                           0.99    200000\n","   macro avg       0.79      0.99      0.86    200000\n","weighted avg       1.00      0.99      0.99    200000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJICMuXI-d41","executionInfo":{"status":"ok","timestamp":1625215653378,"user_tz":-120,"elapsed":13,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"5c7da1d7-957f-4408-9759-8089a0a46a35"},"source":["print('Accuracy: ', acc)\n","print('Precision_weighted: ', precision)\n","print('Recall_weighted: ', recall)\n","print('mcc: ', mcc)\n","print('f2: ', f2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy:  0.99446\n","Precision_weighted:  0.9958267793633506\n","Recall_weighted:  0.99446\n","mcc:  0.9896537535823418\n","f2:  0.9947330556439477\n"],"name":"stdout"}]}]}