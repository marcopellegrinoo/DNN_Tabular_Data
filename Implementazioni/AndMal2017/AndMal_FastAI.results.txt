1) AndMal_Shuffled_FastAI

model: (layers[200,100], emb_drop=0.01 )

- learn.fit_one_cycle(100, lr=0.002)

epoch	train_loss	valid_loss	accuracy	time

0	1.225376	1.196054	0.820872	00:07
1	0.672260	0.649161	0.875250	00:07
2	0.550068	0.532537	0.875250	00:07
3	0.529395	0.519010	0.876050	00:07
4	0.533556	0.563990	0.874050	00:07
5	0.523950	0.519093	0.875650	00:07
6	0.525382	0.527491	0.873651	00:07
7	0.551394	0.520082	0.877649	00:07
8	0.532329	0.524394	0.877649	00:07
9	0.546555	0.528235	0.875650	00:07
10	0.542420	0.511639	0.876449	00:07
11	0.542195	0.542216	0.874050	00:07
12	0.523066	0.514462	0.875650	00:07
13	0.532181	0.508700	0.876449	00:07
14	0.522337	0.516409	0.874450	00:07
15	0.536912	0.516406	0.876449	00:07
16	0.534804	0.553841	0.872051	00:07
17	0.504766	0.510772	0.877249	00:07
18	0.529594	0.517342	0.877249	00:07
19	0.537700	0.506835	0.876849	00:07
20	0.517257	0.514826	0.877649	00:07
21	0.513020	0.509969	0.878049	00:08
22	0.518886	0.507133	0.876449	00:07
23	0.504755	0.502632	0.878848	00:07
24	0.502366	0.502165	0.875250	00:07
25	0.503500	0.527759	0.872051	00:07
26	0.519462	0.529269	0.874050	00:07
27	0.501345	0.523550	0.874850	00:07
28	0.511939	0.519585	0.873251	00:07
29	0.522963	1.330797	0.874450	00:07
30	0.517065	0.577720	0.872851	00:07
31	0.520953	1.012627	0.870452	00:07
32	0.493011	0.618475	0.871651	00:07
33	0.508221	0.903168	0.872051	00:07
34	0.510835	0.998432	0.870052	00:07
35	0.506385	0.793070	0.870452	00:07
36	0.511575	0.521677	0.874850	00:07
37	0.517033	0.551349	0.875250	00:07
38	0.505143	0.547019	0.875650	00:07
39	0.514237	0.585954	0.872451	00:07
40	0.507843	0.720608	0.870052	00:07
41	0.516427	0.601019	0.874450	00:07
42	0.499252	1.089847	0.873251	00:07
43	0.492344	2.082442	0.873251	00:07
44	0.516200	0.625229	0.870452	00:07
45	0.494939	1.026168	0.875250	00:07
46	0.474069	2.072354	0.874450	00:07
47	0.520854	0.714965	0.876849	00:07
48	0.517945	3.641783	0.869252	00:07
49	0.483805	1.813244	0.870452	00:07
50	0.498852	2.349520	0.870852	00:07
51	0.493878	2.357224	0.871651	00:07
52	0.505821	0.825208	0.872451	00:07
53	0.494995	4.931105	0.870052	00:07
54	0.480953	0.568609	0.874450	00:07
55	0.494867	1.513396	0.875250	00:07
56	0.506355	1.092828	0.878449	00:07
57	0.496175	0.565999	0.877249	00:07
58	0.470535	2.758782	0.871252	00:07
59	0.504743	1.106887	0.878049	00:07
60	0.473533	1.067039	0.874850	00:07
61	0.484041	0.860233	0.878848	00:07
62	0.481350	0.789178	0.876849	00:07
63	0.506288	1.636620	0.872451	00:07
64	0.492609	1.328731	0.873651	00:07
65	0.495687	1.655619	0.873251	00:07
66	0.485524	3.342121	0.874050	00:07
67	0.451061	1.434967	0.879248	00:07
68	0.472604	1.996980	0.877649	00:07
69	0.472600	2.415808	0.876050	00:07
70	0.477831	6.890435	0.875650	00:07
71	0.500719	1.944664	0.872451	00:07
72	0.489911	2.593884	0.875650	00:07
73	0.495082	2.297073	0.876849	00:07
74	0.484519	2.823060	0.875650	00:07
75	0.477102	1.158216	0.876849	00:07
76	0.472490	0.782776	0.878049	00:07
77	0.470001	3.019372	0.875250	00:07
78	0.481192	6.100766	0.875250	00:07
79	0.481365	3.307916	0.875650	00:07
80	0.452462	1.352606	0.872851	00:07
81	0.462262	0.890426	0.877249	00:07
82	0.475048	1.274793	0.874850	00:07
83	0.448590	1.624749	0.875650	00:07
84	0.486451	2.302823	0.875650	00:07
85	0.432102	0.983115	0.875650	00:07
86	0.470872	2.191799	0.874050	00:07
87	0.458802	1.318734	0.877649	00:07
88	0.465412	5.037269	0.874050	00:07
89	0.475175	0.880713	0.880048	00:07
90	0.464790	1.438879	0.877249	00:07
91	0.465311	1.128819	0.876449	00:07
92	0.483062	2.802317	0.876050	00:07
93	0.452687	4.782852	0.875650	00:07
94	0.457738	1.444052	0.877249	00:07
95	0.466079	4.243413	0.874450	00:07
96	0.451369	0.723568	0.876849	00:07
97	0.474678	2.381696	0.876449	00:07
98	0.473312	1.813307	0.875650	00:07
99	0.468238	12.138687	0.873251	00:07

[[  150     6    61    19  1557]
 [   11   105    35    17  2033]
 [   23     9   137     5  1303]
 [    4    14    16    25  1079]
 [   44    54   269    79 43207]]

              precision    recall  f1-score   support

           0       0.65      0.08      0.15      1793
           1       0.56      0.05      0.09      2201
           2       0.26      0.09      0.14      1477
           3       0.17      0.02      0.04      1138
           4       0.88      0.99      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.50      0.25      0.27     50262
weighted avg       0.82      0.87      0.82     50262

accuracy : 0.8679320361306753
mcc: 0.17232096269672054

- learn.fit_one_cycle(5, 0.0003)

epoch	train_loss	valid_loss	accuracy	time

0	1.222405	1.135262	0.826469	00:09
1	0.545535	0.556528	0.867253	00:09
2	0.550979	0.547287	0.867253	00:09
3	0.522101	0.537252	0.866453	00:09
4	0.531814	0.535551	0.866453	00:09

True positive:  [    2    13    17     1 43570]
True negative:  [48418 48051 48755 49108    57]
False positive:  [  51   10   30   16 6552]
False negative:  [1791 2188 1460 1137   83]

[[    2     2     1     1  1787]
 [    5    13     1     2  2180]
 [    4     0    17     4  1452]
 [    1     1     2     1  1133]
 [   41     7    26     9 43570]]

              precision    recall  f1-score   support

           0       0.04      0.00      0.00      1793
           1       0.57      0.01      0.01      2201
           2       0.36      0.01      0.02      1477
           3       0.06      0.00      0.00      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.38      0.20      0.19     50262
weighted avg       0.79      0.87      0.81     50262

Accuracy:  0.8675142254585969
Precision_weighted:  0.7930347675515852
Recall_weighted:  0.8675142254585969
mcc:  0.036364350198874726
f2:  0.8515197862060714

- learn.fit_one_cycle(5, 0.01)

epoch	train_loss	valid_loss	accuracy	time

0	0.564152	0.570196	0.867253	00:09
1	0.552121	0.581083	0.866453	00:09
2	0.536674	0.597855	0.865654	00:09
3	0.528590	1.248209	0.865654	00:09
4	0.531713	0.544226	0.866054	00:09

True positive:  [    0     0    15     0 43572]
True negative:  [48459 48061 48719 49102    32]
False positive:  [  10    0   66   22 6577]
False negative:  [1793 2201 1462 1138   81]

[[    0     0     2     1  1790]
 [    0     0     7     4  2190]
 [    0     0    15     1  1461]
 [    1     0     1     0  1136]
 [    9     0    56    16 43572]]

              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1793
           1       0.00      0.00      0.00      2201
           2       0.19      0.01      0.02      1477
           3       0.00      0.00      0.00      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.21      0.20      0.19     50262
weighted avg       0.76      0.87      0.81     50262

Accuracy:  0.8671958935179659
Precision_weighted:  0.7600466269206971
Recall_weighted:  0.8671958935179659
mcc:  0.017481523875213054
f2:  0.8434154199308251

- learn.fit_one_cycle(5, 0.001)

epoch	train_loss	valid_loss	accuracy	time

0	0.590784	6.611426	0.863655	00:09
1	0.551917	0.580253	0.864854	00:09
2	0.567888	1.988249	0.865254	00:09
3	0.507457	0.592177	0.864454	00:09
4	0.507192	0.564957	0.863255	00:09


True positive:  [    1    29    26    14 43392]
True negative:  [48460 48047 48698 48948    95]
False positive:  [   9   14   87  176 6514]
False negative:  [1792 2172 1451 1124  261]

[[    1     0     2     1  1789]
 [    1    29     2     9  2160]
 [    0     3    26     2  1446]
 [    1     3     1    14  1119]
 [    7     8    82   164 43392]]

              precision    recall  f1-score   support

           0       0.10      0.00      0.00      1793
           1       0.67      0.01      0.03      2201
           2       0.23      0.02      0.03      1477
           3       0.07      0.01      0.02      1138
           4       0.87      0.99      0.93     43653

    accuracy                           0.86     50262
   macro avg       0.39      0.21      0.20     50262
weighted avg       0.80      0.86      0.81     50262

Accuracy:  0.8647089252317854
Precision_weighted:  0.7966766928862711
Recall_weighted:  0.8647089252317854
mcc:  0.03696878026014671
f2:  0.8501885494136708

- learn.fit_one_cycle(5, 0.02)

epoch	train_loss	valid_loss	accuracy	time

0	0.585077	2.055383	0.850460	00:09
1	0.555177	0.615121	0.854058	00:09
2	0.543041	0.615614	0.854058	00:09
3	0.547355	1.066120	0.849660	00:08
4	0.530956	0.594053	0.851659	00:09

True positive:  [    1     0    16     2 43595]
True negative:  [48469 48061 48755 49095    20]
False positive:  [   0    0   30   29 6589]
False negative:  [1792 2201 1461 1136   58]

[[    1     0     0     0  1792]
 [    0     0     0     0  2201]
 [    0     0    16     1  1460]
 [    0     0     0     2  1136]
 [    0     0    30    28 43595]]

              precision    recall  f1-score   support

           0       1.00      0.00      0.00      1793
           1       0.00      0.00      0.00      2201
           2       0.35      0.01      0.02      1477
           3       0.06      0.00      0.00      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.46      0.20      0.19     50262
weighted avg       0.80      0.87      0.81     50262

Accuracy:  0.8677330786677808
Precision_weighted:  0.8018315639412166
Recall_weighted:  0.8677330786677808
mcc:  0.019396290184175137
f2:  0.8537001727467892

- learn.fit_one_cycle(10, 0.0001, wd=0.01)

epoch	train_loss	valid_loss	accuracy	time

0	1.597121	3.511965	0.167133	00:08
1	1.356392	1.466410	0.722911	00:08
2	0.751072	0.827147	0.850060	00:09
3	0.541440	0.973046	0.850060	00:08
4	0.533267	0.604544	0.850460	00:08
5	0.513335	1.384006	0.853259	00:08
6	0.507787	0.596984	0.851659	00:08
7	0.542075	0.655486	0.850060	00:08
8	0.516187	0.643264	0.849660	00:08
9	0.508303	1.280910	0.846062	00:08

True positive:  [    3     7    71     4 43192]
True negative:  [48443 48039 48350 49031   200]
False positive:  [  26   22  435   93 6409]
False negative:  [1790 2194 1406 1134  461]

[[    3     2    49     1  1738]
 [    3     7    37     8  2146]
 [    4     0    71     2  1400]
 [    1     0     8     4  1125]
 [   18    20   341    82 43192]]

              precision    recall  f1-score   support

           0       0.10      0.00      0.00      1793
           1       0.24      0.00      0.01      2201
           2       0.14      0.05      0.07      1477
           3       0.04      0.00      0.01      1138
           4       0.87      0.99      0.93     43653

    accuracy                           0.86     50262
   macro avg       0.28      0.21      0.20     50262
weighted avg       0.78      0.86      0.81     50262

Accuracy:  0.8610282121682384
Precision_weighted:  0.7756054502718417
Recall_weighted:  0.8610282121682384
mcc:  0.0448864687314592
f2:  0.8424707918127051


- learn.fit_one_cycle(10, 0.0001, wd=0.001)

epoch	train_loss	valid_loss	accuracy	time

0	1.525746	1.569790	0.504198	00:09
1	1.295406	1.317690	0.780888	00:08
2	0.730264	0.822108	0.847661	00:08
3	0.539454	0.609226	0.852059	00:08
4	0.514840	0.593972	0.852459	00:08
5	0.531038	0.583544	0.851259	00:08
6	0.526311	0.580759	0.852059	00:08
7	0.529598	0.580031	0.850460	00:09
8	0.503762	0.632214	0.849260	00:08
9	0.523962	0.641127	0.849260	00:08

True positive:  [    2    23    28    12 43390]
True negative:  [48466 47993 48678 49003   101]
False positive:  [   3   68  107  121 6508]
False negative:  [1791 2178 1449 1126  263]

[[    2     0     1     2  1788]
 [    1    23     3    17  2157]
 [    2     1    28     4  1442]
 [    0     4     1    12  1121]
 [    0    63   102    98 43390]]

              precision    recall  f1-score   support

           0       0.40      0.00      0.00      1793
           1       0.25      0.01      0.02      2201
           2       0.21      0.02      0.03      1477
           3       0.09      0.01      0.02      1138
           4       0.87      0.99      0.93     43653

    accuracy                           0.86     50262
   macro avg       0.36      0.21      0.20     50262
weighted avg       0.79      0.86      0.81     50262

Accuracy:  0.8645696550077593
Precision_weighted:  0.7887076705623118
Recall_weighted:  0.8645696550077593
mcc:  0.036108517972835284
f2:  0.848251805523534

- modifica layers : [80, 200, 100]

learn.fit_one_cycle(10, 0.0001, wd=0.0002)

epoch	train_loss	valid_loss	accuracy	time

0	1.570050	4.297357	0.357457	00:10
1	1.330573	3.606475	0.744502	00:09
2	0.760794	0.763807	0.849260	00:10
3	0.550607	0.616585	0.852459	00:10
4	0.514207	0.692460	0.853659	00:09
5	0.508888	0.579963	0.852459	00:10
6	0.536284	0.602510	0.852059	00:10
7	0.510923	1.251854	0.852859	00:10
8	0.505416	0.667876	0.852459	00:10
9	0.523627	0.575622	0.853259	00:10

True positive:  [    2    14    11     7 43611]
True negative:  [48467 48060 48753 49103    48]
False positive:  [   2    1   32   21 6561]
False negative:  [1791 2187 1466 1131   42]

[[    2     1     2     1  1787]
 [    2    14     3     5  2177]
 [    0     0    11     0  1466]
 [    0     0     0     7  1131]
 [    0     0    27    15 43611]]

              precision    recall  f1-score   support

           0       0.50      0.00      0.00      1793
           1       0.93      0.01      0.01      2201
           2       0.26      0.01      0.01      1477
           3       0.25      0.01      0.01      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.56      0.20      0.19     50262
weighted avg       0.83      0.87      0.81     50262

Accuracy:  0.8683498468027536
Precision_weighted:  0.8268193563380427
Recall_weighted:  0.8683498468027536
mcc:  0.045707555427704924
f2:  0.8597133012774476

- layers -> [500, 200, 100]

learn.fit_one_cycle(5, 0.01, wd=0.02)

epoch	train_loss	valid_loss	accuracy	time

0	0.559098	0.620857	0.853259	00:14
1	0.558267	0.617849	0.854058	00:13
2	0.535005	0.759779	0.848860	00:13
3	0.568127	6.484042	0.847261	00:13
4	0.526740	6.947249	0.849260	00:13

True positive:  [    1     0    57     1 43296]
True negative:  [48424 48058 48411 49120   128]
False positive:  [  45    3  374    4 6481]
False negative:  [1792 2201 1420 1137  357]

[[    1     0    37     0  1755]
 [    0     0    23     0  2178]
 [    1     0    57     0  1419]
 [    1     0     7     1  1129]
 [   43     3   307     4 43296]]

              precision    recall  f1-score   support

           0       0.02      0.00      0.00      1793
           1       0.00      0.00      0.00      2201
           2       0.13      0.04      0.06      1477
           3       0.20      0.00      0.00      1138
           4       0.87      0.99      0.93     43653

    accuracy                           0.86     50262
   macro avg       0.24      0.21      0.20     50262
weighted avg       0.76      0.86      0.81     50262

Accuracy:  0.8625800803788151
Precision_weighted:  0.7646186268829612
Recall_weighted:  0.8625800803788151
mcc:  0.03183455447778662
f2:  0.841029857974106

- layers -> [1000, 500, 200, 100]

learn.fit_one_cycle(10, 0.001, wd=0.00000002)

epoch	train_loss	valid_loss	accuracy	time

0	1.004465	4.091664	0.839264	00:26
1	0.560160	0.616035	0.852059	00:26
2	0.551195	3.888098	0.853259	00:26
3	0.532115	0.609575	0.850860	00:26
4	0.524721	3.291222	0.852059	00:26
5	0.519567	0.589940	0.854458	00:26
6	0.505076	0.619470	0.850860	00:26
7	0.514494	1.314181	0.851659	00:26
8	0.497584	4.804447	0.853259	00:26
9	0.494428	0.571035	0.853659	00:26


True positive:  [   89    45    87     9 43464]
True negative:  [48432 47971 48664 49082   331]
False positive:  [  37   90  121   42 6278]
False negative:  [1704 2156 1390 1129  189]

[[   89     6    44     0  1654]
 [    5    45    15     1  2135]
 [    6     6    87     0  1378]
 [    2    11     5     9  1111]
 [   24    67    57    41 43464]]

              precision    recall  f1-score   support

           0       0.71      0.05      0.09      1793
           1       0.33      0.02      0.04      2201
           2       0.42      0.06      0.10      1477
           3       0.18      0.01      0.02      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.50      0.23      0.24     50262
weighted avg       0.81      0.87      0.82     50262

Accuracy:  0.8693247383709363
Precision_weighted:  0.8149746981509837
Recall_weighted:  0.8693247383709363
mcc:  0.13423042753446893
f2:  0.8578824335065384

- layers -> [200, 100]

learn.fit(100, lr=0.0003, wd=0.000001)

epoch	train_loss	valid_loss	accuracy	time

0	0.535084	1.334825	0.860056	00:08
1	0.537699	0.581633	0.858057	00:08
2	0.536932	1.485741	0.857657	00:08
3	0.512050	1.109864	0.854458	00:08
4	0.549831	1.502028	0.850060	00:08
5	0.521098	0.559138	0.859656	00:08
6	0.513209	1.072232	0.857657	00:08
7	0.521573	0.736864	0.859656	00:08
8	0.513980	0.711583	0.858856	00:08
9	0.515225	0.836597	0.859256	00:08
10	0.523560	0.548059	0.860056	00:08
11	0.531708	0.572736	0.857657	00:08
12	0.520020	0.565971	0.860456	00:08
13	0.523995	0.545210	0.861655	00:08
14	0.497761	0.676159	0.858856	00:08
15	0.495158	0.602506	0.856457	00:08
16	0.502283	0.770097	0.858057	00:08
17	0.511875	0.550022	0.858057	00:08
18	0.490721	0.588581	0.858457	00:08
19	0.503074	0.594319	0.858057	00:09
20	0.495075	0.835093	0.859656	00:08
21	0.503484	0.572313	0.855658	00:08
22	0.489750	0.854008	0.860456	00:08
23	0.506961	0.554621	0.860456	00:08
24	0.514575	0.599251	0.853659	00:08
25	0.501598	0.539678	0.859656	00:08
26	0.509029	1.181564	0.854858	00:08
27	0.492323	0.627737	0.852059	00:08
28	0.498837	0.544379	0.858057	00:08
29	0.474227	0.574179	0.857257	00:08
30	0.500969	0.540115	0.860056	00:08
31	0.494079	0.613035	0.860056	00:08
32	0.503719	0.771500	0.858457	00:08
33	0.490018	0.697844	0.857257	00:08
34	0.506616	0.804901	0.857657	00:08
35	0.499110	0.669240	0.858856	00:08
36	0.501701	0.546467	0.860056	00:08
37	0.526718	0.613783	0.857657	00:08
38	0.502030	0.931201	0.853659	00:08
39	0.488111	0.710893	0.857657	00:08
40	0.491645	0.539981	0.862055	00:08
41	0.515961	0.543889	0.858856	00:08
42	0.495997	0.551510	0.860856	00:08
43	0.458685	0.540862	0.860056	00:08
44	0.499558	0.536540	0.861655	00:08
45	0.511030	0.658420	0.860056	00:08
46	0.494383	0.689784	0.860456	00:08
47	0.505209	0.573965	0.860456	00:09
48	0.488902	0.561030	0.861256	00:08
49	0.500728	0.550593	0.860056	00:08
50	0.491393	0.671057	0.855658	00:08
51	0.504348	0.582924	0.861655	00:09
52	0.493888	0.579477	0.862455	00:08
53	0.487772	0.654340	0.859256	00:08
54	0.501270	0.604032	0.861655	00:08
55	0.479699	0.534153	0.864454	00:08
56	0.513397	0.532075	0.860056	00:08
57	0.483132	0.554245	0.861655	00:09
58	0.480607	0.557721	0.860856	00:08
59	0.497043	0.548309	0.861655	00:08
60	0.488482	0.534437	0.862055	00:08
61	0.470060	0.540731	0.861256	00:08
62	0.486361	0.537025	0.862055	00:08
63	0.493030	0.568598	0.859656	00:08
64	0.500841	0.548546	0.860456	00:08
65	0.485398	0.609203	0.860856	00:08
66	0.506085	0.534480	0.863255	00:08
67	0.512026	0.548449	0.861256	00:08
68	0.475458	0.531071	0.862855	00:08
69	0.493035	0.606701	0.857657	00:08
70	0.498222	0.750194	0.861256	00:09
71	0.496191	0.553939	0.861256	00:08
72	0.474900	0.622196	0.863655	00:08
73	0.464421	0.559274	0.861655	00:08
74	0.495563	0.576129	0.862455	00:08
75	0.476539	0.570104	0.860456	00:08
76	0.474079	0.548153	0.861655	00:08
77	0.484516	0.548825	0.861256	00:08
78	0.483988	0.541218	0.861256	00:08
79	0.477639	0.540696	0.864454	00:08
80	0.495452	0.546608	0.861256	00:08
81	0.462980	0.564223	0.859256	00:08
82	0.492139	0.566696	0.859656	00:09
83	0.476150	0.534020	0.863655	00:08
84	0.487722	0.539267	0.863255	00:08
85	0.487709	0.540352	0.863255	00:08
86	0.495441	0.542781	0.861256	00:08
87	0.477814	0.558215	0.861256	00:08
88	0.478567	0.540011	0.860856	00:09
89	0.476428	0.553307	0.861256	00:08
90	0.476363	0.538714	0.863255	00:09
91	0.474135	0.541290	0.860856	00:08
92	0.479606	0.577414	0.859656	00:08
93	0.468874	0.568361	0.861655	00:08
94	0.496763	0.555321	0.860456	00:08
95	0.469975	0.575376	0.856857	00:09
96	0.472656	0.545153	0.860056	00:08
97	0.476116	0.593834	0.860056	00:08
98	0.488175	0.572396	0.860856	00:08
99	0.487906	0.625486	0.858856	00:08

True positive:  [  168    70   112    33 43297]
True negative:  [48347 47927 48707 48971   514]
False positive:  [ 122  134   78  153 6095]
False negative:  [1625 2131 1365 1105  356]

[[  168     4    37     5  1579]
 [   13    70    10     9  2099]
 [   18    17   112     4  1326]
 [    5     5     4    33  1091]
 [   86   108    27   135 43297]]

              precision    recall  f1-score   support

           0       0.58      0.09      0.16      1793
           1       0.34      0.03      0.06      2201
           2       0.59      0.08      0.13      1477
           3       0.18      0.03      0.05      1138
           4       0.88      0.99      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.51      0.24      0.27     50262
weighted avg       0.82      0.87      0.82     50262

Accuracy:  0.8690461979228841
Precision_weighted:  0.8183657666656224
Recall_weighted:  0.8690461979228841
mcc:  0.164992463599744
f2:  0.858414082632341

- learn.fit(500, lr=0.0003, wd=0.000001)

epoch	train_loss	valid_loss	accuracy	time

0	0.536112	0.575555	0.858457	00:08
1	0.525967	0.581729	0.856457	00:08
2	0.542507	0.574634	0.858457	00:08
3	0.512283	0.565629	0.857257	00:08
4	0.531698	0.562844	0.858057	00:08
5	0.518743	0.567711	0.856857	00:09
6	0.550050	0.568057	0.858856	00:08
7	0.529127	0.563298	0.857657	00:08
8	0.501053	0.568023	0.860056	00:08
9	0.528894	0.588957	0.857257	00:08
10	0.522678	0.555993	0.856857	00:08
11	0.499707	1.060904	0.856857	00:08
12	0.502021	0.611802	0.860856	00:08
13	0.509595	0.566805	0.858457	00:08
14	0.508277	0.546210	0.860056	00:08
15	0.510876	0.555454	0.858856	00:09
16	0.507285	0.593792	0.858856	00:08
17	0.510619	0.560581	0.858057	00:08
18	0.521349	0.565026	0.856457	00:08
19	0.499675	0.600125	0.856857	00:08
20	0.505855	0.552406	0.861655	00:08
21	0.497990	0.964902	0.858856	00:08
22	0.508607	0.549952	0.860856	00:08
23	0.480809	0.597116	0.859256	00:08
24	0.506900	0.558779	0.860056	00:08
25	0.480786	0.553850	0.860456	00:09
26	0.511795	0.549538	0.858856	00:08
27	0.498884	0.564735	0.857257	00:08
28	0.502344	0.545536	0.859256	00:08
29	0.502121	0.544455	0.860056	00:09
30	0.489910	0.543338	0.859656	00:08
31	0.517081	0.538585	0.861256	00:08
32	0.469513	0.572173	0.857657	00:09
33	0.489032	0.531434	0.863655	00:08
34	0.499385	0.587055	0.855258	00:08
35	0.499226	0.534392	0.863255	00:08
36	0.475393	0.545318	0.861655	00:08
37	0.491472	0.535747	0.861655	00:09
38	0.491024	0.555701	0.856457	00:09
39	0.476886	0.542078	0.860856	00:08
40	0.493825	0.531463	0.861655	00:08
41	0.492664	0.548619	0.860056	00:08
42	0.505000	0.529975	0.861256	00:08
43	0.491407	0.544965	0.862855	00:08
44	0.502594	0.539088	0.863655	00:08
45	0.484210	0.571517	0.857257	00:08
46	0.500557	0.656294	0.861256	00:08
47	0.483355	0.544143	0.860056	00:08
48	0.474450	0.535093	0.864054	00:08
49	0.512990	0.718960	0.865254	00:08
50	0.497254	0.533561	0.864054	00:08
51	0.505714	0.541310	0.860456	00:08
52	0.496079	0.532942	0.863255	00:08
53	0.472785	0.544755	0.860456	00:08
54	0.488835	0.707712	0.860856	00:08
55	0.477888	0.557410	0.860456	00:08
56	0.474055	0.546735	0.861655	00:08
57	0.485940	0.897026	0.862055	00:08
58	0.495387	0.550405	0.861655	00:08
59	0.503677	0.536708	0.861256	00:08
60	0.481684	0.892798	0.862055	00:08
61	0.483613	0.541769	0.863255	00:08
62	0.474410	0.965225	0.859656	00:08
63	0.487024	0.659277	0.862055	00:08
64	0.475036	1.214705	0.862455	00:08
65	0.512726	0.665141	0.860856	00:08
66	0.492141	0.561789	0.858057	00:08
67	0.501678	0.553539	0.863255	00:08
68	0.479758	0.788356	0.860856	00:08
69	0.492165	1.584984	0.863255	00:08
70	0.489909	0.754808	0.862055	00:08
71	0.485388	0.592027	0.861655	00:08
72	0.471732	1.466640	0.861655	00:08
73	0.473201	0.635237	0.862855	00:08
74	0.493330	0.552142	0.862855	00:08
75	0.469736	0.547690	0.861256	00:08
76	0.481610	1.568342	0.863655	00:08
77	0.481694	0.544649	0.863655	00:08
78	0.479081	1.196725	0.860056	00:08
79	0.467176	0.651939	0.861256	00:08
80	0.471994	0.548844	0.863655	00:08
81	0.476923	0.624933	0.860456	00:08
82	0.472007	0.560846	0.860456	00:08
83	0.483742	0.550480	0.862455	00:08
84	0.484585	0.569052	0.862055	00:08
85	0.471995	0.558806	0.859656	00:09
86	0.487104	0.570259	0.860856	00:08
87	0.495329	0.754273	0.864054	00:08
88	0.466815	0.554572	0.862855	00:08
89	0.489863	0.721622	0.862855	00:08
90	0.472336	1.390310	0.861655	00:08
91	0.481098	0.790706	0.864054	00:08
92	0.450275	1.537608	0.864454	00:08
93	0.470909	0.576775	0.863655	00:08
94	0.483110	0.547003	0.864054	00:08
95	0.472639	0.654082	0.865254	00:08
96	0.452186	1.720290	0.860056	00:08
97	0.457065	0.590298	0.863255	00:08
98	0.472952	0.554917	0.863255	00:08
99	0.486092	0.538936	0.862855	00:08
100	0.501240	0.654153	0.863655	00:08
101	0.469859	0.572517	0.864454	00:08
102	0.466060	1.447726	0.862055	00:08
103	0.461377	0.757800	0.865654	00:08
104	0.455481	0.580188	0.862455	00:08
105	0.492021	0.533213	0.866853	00:08
106	0.493217	0.561798	0.860456	00:08
107	0.468072	0.604217	0.860056	00:08
108	0.493450	0.632725	0.863255	00:08
109	0.476171	1.632412	0.863655	00:08
110	0.486156	0.551421	0.862855	00:08
111	0.469612	1.273757	0.860456	00:08
112	0.467855	0.633936	0.859256	00:08
113	0.471143	0.541923	0.864054	00:08
114	0.473714	0.541929	0.860856	00:08
115	0.452688	0.597026	0.864454	00:08
116	0.469116	1.292053	0.861256	00:08
117	0.472900	0.535755	0.863655	00:08
118	0.456231	1.611638	0.860056	00:08
119	0.454063	0.599725	0.862855	00:08
120	0.490317	0.585813	0.859656	00:08
121	0.486779	0.552867	0.861256	00:08
122	0.484851	0.538521	0.862055	00:09
123	0.461153	0.597961	0.863655	00:09
124	0.475533	1.298431	0.860856	00:08
125	0.483356	0.554116	0.863655	00:08
126	0.471343	0.545197	0.865254	00:09
127	0.476936	0.530655	0.864854	00:08
128	0.489039	0.576346	0.865654	00:08
129	0.460425	0.539605	0.865254	00:09
130	0.482995	0.600895	0.862855	00:08
131	0.479511	1.016160	0.863255	00:08
132	0.458849	0.557587	0.864054	00:08
133	0.462342	0.554113	0.862455	00:08
134	0.457215	0.550016	0.860856	00:08
135	0.448006	0.536733	0.864054	00:08
136	0.475370	0.552758	0.863655	00:08
137	0.471927	0.535488	0.865254	00:08
138	0.476019	0.615957	0.861655	00:08
139	0.467043	0.598830	0.864054	00:09
140	0.470502	0.551147	0.864454	00:09
141	0.496997	1.140634	0.863655	00:09
142	0.462886	1.000747	0.862855	00:08
143	0.473086	1.013910	0.865654	00:08
144	0.461519	0.545864	0.862855	00:08
145	0.466144	0.549280	0.863255	00:08
146	0.484725	0.549882	0.862455	00:08
147	0.461679	0.556322	0.862455	00:08
148	0.465627	0.609904	0.862855	00:08
149	0.448853	0.609787	0.862455	00:09
150	0.453136	0.560040	0.864854	00:08
151	0.469419	0.628423	0.861256	00:08
152	0.452704	0.537565	0.863655	00:08
153	0.485939	0.540959	0.864454	00:08
154	0.475599	0.563548	0.863255	00:08
155	0.467700	0.566265	0.864854	00:08
156	0.467392	0.542243	0.865254	00:08
157	0.474276	0.748167	0.862455	00:08
158	0.471330	0.677358	0.860456	00:08
159	0.477354	0.655419	0.861655	00:08
160	0.479618	0.551821	0.860856	00:08
161	0.474932	0.568639	0.862855	00:08
162	0.476771	0.554317	0.860856	00:08
163	0.456787	0.909795	0.863655	00:08
164	0.497176	0.581932	0.861655	00:08
165	0.478019	0.987664	0.864054	00:08
166	0.470508	0.745533	0.862855	00:08
167	0.461165	0.621391	0.862055	00:08
168	0.482260	0.865671	0.862855	00:08
169	0.451796	0.563895	0.865254	00:09
170	0.458551	0.714527	0.862455	00:08
171	0.456793	0.808852	0.864454	00:08
172	0.456990	0.552432	0.861655	00:09
173	0.476108	1.106385	0.863655	00:08
174	0.456708	0.578046	0.861256	00:08
175	0.436336	0.607465	0.861655	00:08
176	0.490127	0.610614	0.864454	00:08
177	0.462870	2.043514	0.861256	00:08
178	0.466904	0.586534	0.865254	00:09
179	0.452203	0.616885	0.859256	00:09
180	0.474514	1.339870	0.860056	00:08
181	0.464860	1.143802	0.863255	00:08
182	0.461221	4.789226	0.860056	00:08
183	0.469881	0.558282	0.864454	00:08
184	0.484255	0.553974	0.862455	00:08
185	0.447613	0.934615	0.862055	00:08
186	0.480327	4.167901	0.862055	00:08
187	0.473749	0.560701	0.862055	00:08
188	0.484937	0.742110	0.862455	00:08
189	0.451098	0.621884	0.862055	00:08
190	0.460877	0.544127	0.864854	00:08
191	0.465383	0.565821	0.863655	00:09
192	0.468232	1.293059	0.862055	00:09
193	0.489758	1.847510	0.862055	00:09
194	0.469042	0.548955	0.864054	00:08
195	0.466918	0.773213	0.861256	00:08
196	0.465046	0.650809	0.862855	00:08
197	0.469462	1.934958	0.862855	00:08
198	0.453527	0.571438	0.865254	00:08
199	0.479436	0.901429	0.864854	00:08
200	0.456890	0.549279	0.863255	00:08
201	0.447613	0.584106	0.861655	00:09
202	0.452048	1.688609	0.864854	00:08
203	0.482094	0.648753	0.862855	00:08
204	0.470212	3.793161	0.865254	00:08
205	0.462562	1.360417	0.863655	00:08
206	0.450895	1.121912	0.862055	00:09
207	0.480602	6.478297	0.862055	00:09
208	0.458774	16.277382	0.864454	00:09
209	0.458307	1.539679	0.864054	00:09
210	0.448705	0.615958	0.866054	00:09
211	0.451918	1.047272	0.862855	00:09
212	0.448254	1.503158	0.863255	00:08
213	0.456210	1.991494	0.860456	00:08
214	0.472919	0.540469	0.864854	00:09
215	0.471238	2.898331	0.862855	00:08
216	0.443771	0.690897	0.862855	00:09
217	0.442857	0.786427	0.863255	00:09
218	0.457229	1.009917	0.859656	00:09
219	0.471705	1.202869	0.861655	00:08
220	0.447915	2.688964	0.860456	00:09
221	0.466555	1.249135	0.862855	00:08
222	0.470222	1.122481	0.865654	00:08
223	0.461153	2.328655	0.860456	00:08
224	0.468833	0.756054	0.863655	00:08
225	0.472966	0.542080	0.862455	00:08
226	0.468458	0.678794	0.861256	00:08
227	0.466231	1.683862	0.861256	00:08
228	0.459993	1.231847	0.863655	00:08
229	0.461241	0.574712	0.864454	00:08
230	0.464369	1.164166	0.864054	00:08
231	0.463333	0.615744	0.866853	00:09
232	0.437372	0.572838	0.865254	00:09
233	0.454676	6.609427	0.864054	00:09
234	0.441778	0.536561	0.866853	00:09
235	0.464959	0.554754	0.864054	00:09
236	0.457037	9.452672	0.864054	00:09
237	0.454472	2.413933	0.863655	00:09
238	0.450639	0.654083	0.864454	00:09
239	0.461600	4.991222	0.864454	00:09
240	0.432506	4.815463	0.865654	00:09
241	0.451361	2.179194	0.861256	00:09
242	0.461355	2.897655	0.863255	00:09
243	0.460127	0.533719	0.865254	00:09
244	0.461760	0.675961	0.866453	00:09
245	0.440793	0.545926	0.863655	00:08
246	0.452889	0.872841	0.864854	00:09
247	0.446975	0.730988	0.862855	00:09
248	0.467196	0.826187	0.864854	00:09
249	0.487541	1.122648	0.866453	00:08
250	0.464279	3.150329	0.864454	00:09
251	0.469026	0.627958	0.864054	00:08
252	0.431076	0.542883	0.864054	00:09
253	0.480020	0.559466	0.863255	00:08
254	0.443959	0.553641	0.862855	00:08
255	0.462699	0.883101	0.865254	00:08
256	0.455325	0.828822	0.861655	00:09
257	0.473156	0.539858	0.866453	00:09
258	0.459188	0.563672	0.863255	00:09
259	0.449688	51.283989	0.862455	00:08
260	0.463864	16.662546	0.862055	00:09
261	0.456286	0.624699	0.862855	00:09
262	0.457570	0.533845	0.864854	00:09
263	0.439818	1.706143	0.866054	00:09
264	0.456634	1.383857	0.864054	00:08
265	0.454740	1.364349	0.863255	00:09
266	0.474513	4.361269	0.866853	00:09
267	0.464612	19.070539	0.863655	00:09
268	0.450864	1.876328	0.861256	00:09
269	0.463012	1.409738	0.863655	00:09
270	0.458174	1.010978	0.865654	00:09
271	0.470564	5.614339	0.867253	00:09
272	0.437727	1.103974	0.863655	00:09
273	0.473502	3.290352	0.864054	00:09
274	0.464382	0.556834	0.865254	00:09
275	0.469908	31.792353	0.866453	00:09
276	0.475836	0.629668	0.862455	00:09
277	0.454783	0.887046	0.864054	00:09
278	0.443776	36.085743	0.863255	00:09
279	0.460125	16.538927	0.862855	00:09
280	0.460466	18.162247	0.863255	00:09
281	0.456390	2.313880	0.866054	00:09
282	0.456616	0.794290	0.864854	00:09
283	0.464918	0.668185	0.866054	00:09
284	0.456222	0.934819	0.865254	00:09
285	0.446446	2.965272	0.864854	00:09
286	0.466426	1.406412	0.865254	00:09
287	0.450689	4.554437	0.864454	00:09
288	0.474653	0.540670	0.866453	00:09
289	0.475722	3.853650	0.864854	00:09
290	0.453293	2.394547	0.863655	00:09
291	0.456360	10.686367	0.863655	00:09
292	0.476972	1.175362	0.865254	00:09
293	0.487055	0.551113	0.864854	00:09
294	0.485916	1.674627	0.864054	00:09
295	0.455323	0.698007	0.864454	00:09
296	0.473114	0.592308	0.863655	00:09
297	0.463924	0.550452	0.865254	00:09
298	0.455686	0.554872	0.862855	00:09
299	0.461839	1.388385	0.863655	00:09
300	0.450193	4.506576	0.862455	00:09
301	0.463251	0.542030	0.864454	00:09
302	0.472570	0.563880	0.865654	00:09
303	0.435436	0.655942	0.862455	00:09
304	0.481204	0.570017	0.862055	00:09
305	0.462839	0.645676	0.862855	00:09
306	0.468818	1.377173	0.861256	00:09
307	0.461336	0.549689	0.864454	00:09
308	0.447410	0.661802	0.863255	00:09
309	0.473518	1.144506	0.864854	00:09
310	0.457873	0.850619	0.858856	00:09
311	0.456540	0.554949	0.863255	00:10
312	0.460279	0.673268	0.861256	00:09
313	0.456689	0.833178	0.862855	00:09
314	0.445627	0.559617	0.864454	00:09
315	0.449404	0.560534	0.864454	00:09
316	0.451240	0.595493	0.867653	00:09
317	0.464303	0.535536	0.866853	00:09
318	0.456201	0.576945	0.863655	00:09
319	0.455318	1.133203	0.861256	00:09
320	0.455050	0.685399	0.863655	00:09
321	0.440934	1.155618	0.862455	00:09
322	0.470921	1.360441	0.863655	00:09
323	0.464361	0.801575	0.866054	00:09
324	0.457643	0.534964	0.864454	00:09
325	0.418211	0.575967	0.865654	00:09
326	0.445738	1.008654	0.862055	00:10
327	0.446979	0.796377	0.866453	00:09
328	0.449662	0.796370	0.865254	00:09
329	0.448892	0.649956	0.862055	00:09
330	0.467421	0.624495	0.864454	00:09
331	0.449445	0.714844	0.864854	00:09
332	0.443394	0.577872	0.860456	00:09
333	0.459818	0.541514	0.866054	00:09
334	0.444306	0.553340	0.864854	00:09
335	0.472649	0.546838	0.863655	00:09
336	0.442325	0.971786	0.863255	00:09
337	0.455575	0.921223	0.863655	00:09
338	0.447237	0.614092	0.862855	00:09
339	0.468389	0.828087	0.862055	00:09
340	0.464937	0.731283	0.860456	00:09
341	0.479570	0.707675	0.863655	00:09
342	0.442289	0.619453	0.863655	00:09
343	0.453146	0.913072	0.861256	00:09
344	0.448926	0.675392	0.866853	00:09
345	0.444951	0.932799	0.864854	00:09
346	0.471691	20.082840	0.863655	00:09
347	0.434934	36.175037	0.863655	00:09
348	0.461467	0.913520	0.863655	00:09
349	0.438311	2.103531	0.864454	00:09
350	0.460542	0.639049	0.864454	00:09
351	0.468209	0.596516	0.864054	00:09
352	0.465188	0.553575	0.864454	00:09
353	0.459621	1.508246	0.864054	00:09
354	0.439075	0.706645	0.860456	00:09
355	0.442331	0.852835	0.862455	00:09
356	0.457291	0.567696	0.863655	00:09
357	0.458241	0.638228	0.863655	00:09
358	0.446091	0.597929	0.861655	00:09
359	0.448070	0.894374	0.864454	00:09
360	0.440364	1.566274	0.864054	00:09
361	0.452034	0.719593	0.862855	00:09
362	0.453260	0.565998	0.865254	00:09
363	0.430351	0.559649	0.863255	00:09
364	0.469867	1.253034	0.864454	00:09
365	0.452497	0.682005	0.863255	00:09
366	0.458608	0.812939	0.864054	00:09
367	0.452882	1.120147	0.862855	00:09
368	0.457780	0.682717	0.863655	00:09
369	0.441692	0.696146	0.865254	00:09
370	0.458118	0.850644	0.865254	00:09
371	0.459588	0.591669	0.864454	00:09
372	0.437726	5.200237	0.862455	00:09
373	0.451760	0.647829	0.864054	00:09
374	0.450596	0.851620	0.862855	00:09
375	0.445560	0.588085	0.865254	00:09
376	0.455074	0.568742	0.864054	00:09
377	0.441479	0.553838	0.864454	00:09
378	0.431937	0.702505	0.863255	00:09
379	0.448828	0.938089	0.864054	00:09
380	0.461958	0.961242	0.864054	00:09
381	0.463236	0.544441	0.864854	00:09
382	0.448279	0.707485	0.865254	00:09
383	0.448590	0.703609	0.862855	00:09
384	0.444751	1.202120	0.866054	00:09
385	0.443975	0.670533	0.864454	00:09
386	0.443250	2.385067	0.865254	00:09
387	0.460967	1.205371	0.861655	00:09
388	0.450544	0.833724	0.864054	00:09
389	0.458497	0.871282	0.862055	00:09
390	0.459432	0.988545	0.865254	00:09
391	0.418657	0.556350	0.866853	00:09
392	0.463543	0.818294	0.868053	00:09
393	0.467207	1.290639	0.861655	00:09
394	0.467961	0.655831	0.864454	00:09
395	0.451492	0.584955	0.866453	00:09
396	0.479150	0.550885	0.865654	00:09
397	0.461850	0.780789	0.861256	00:09
398	0.447984	0.616885	0.864454	00:09
399	0.436967	0.980476	0.865254	00:09
400	0.442256	0.544429	0.865654	00:09
401	0.452627	0.571953	0.861256	00:09
402	0.428667	0.651146	0.863655	00:09
403	0.459831	0.968439	0.859256	00:09
404	0.439629	2.424906	0.862455	00:09
405	0.440939	0.596113	0.862855	00:09
406	0.450228	0.721951	0.863655	00:09
407	0.441453	0.942712	0.862855	00:09
408	0.446551	0.556442	0.862855	00:09
409	0.440184	0.543547	0.863255	00:09
410	0.443334	0.547546	0.864054	00:09
411	0.467859	0.681182	0.865654	00:09
412	0.428448	4.120251	0.861655	00:09
413	0.445700	1.379917	0.866453	00:09
414	0.447370	0.753679	0.862455	00:09
415	0.456634	0.555090	0.866054	00:09
416	0.436798	0.762603	0.864054	00:09
417	0.450813	0.917854	0.866054	00:09
418	0.436245	1.131644	0.864054	00:09
419	0.444674	0.566033	0.864854	00:09
420	0.441551	0.977477	0.861655	00:09
421	0.446987	0.567528	0.864454	00:09
422	0.466311	0.613842	0.864454	00:09
423	0.442253	1.269001	0.866054	00:09
424	0.452559	3.002318	0.860056	00:09
425	0.449176	1.016002	0.861256	00:09
426	0.439706	0.647439	0.863655	00:09
427	0.451418	0.760357	0.865654	00:09
428	0.449595	1.354920	0.864854	00:09
429	0.441561	1.017543	0.864454	00:09
430	0.441496	0.549280	0.866054	00:09
431	0.475457	0.610233	0.865654	00:09
432	0.444013	0.711370	0.864054	00:09
433	0.456127	1.601859	0.865654	00:09
434	0.458673	1.301208	0.863255	00:09
435	0.465479	0.595306	0.867253	00:09
436	0.461894	1.033014	0.862455	00:09
437	0.450053	0.652592	0.864454	00:09
438	0.439420	0.751846	0.864854	00:09
439	0.446500	0.972179	0.863255	00:09
440	0.455593	0.856455	0.861256	00:09
441	0.454542	0.922629	0.863655	00:09
442	0.454757	1.122632	0.862055	00:09
443	0.450125	1.484774	0.862055	00:09
444	0.446661	1.115613	0.868053	00:09
445	0.450111	0.840148	0.864454	00:09
446	0.439961	0.668176	0.862855	00:10
447	0.450385	0.659631	0.865254	00:09
448	0.456894	0.620852	0.864054	00:09
449	0.446620	0.910214	0.861655	00:09
450	0.434815	1.397579	0.864854	00:09
451	0.451489	0.837033	0.863255	00:09
452	0.440526	0.558555	0.862055	00:09
453	0.452371	10.266188	0.862055	00:09
454	0.439113	0.843348	0.863655	00:09
455	0.431656	0.974507	0.867253	00:09
456	0.436185	1.115579	0.864054	00:09
457	0.449572	2.020694	0.862855	00:09
458	0.434251	3.897383	0.865254	00:09
459	0.462400	8.578383	0.864054	00:09
460	0.470676	0.663236	0.861655	00:09
461	0.468032	1.199617	0.865254	00:09
462	0.453439	5.264529	0.862055	00:09
463	0.450217	0.578273	0.865654	00:09
464	0.469175	1.095744	0.858856	00:09
465	0.460859	1.801104	0.863255	00:09
466	0.442264	0.622227	0.866054	00:09
467	0.460569	0.840861	0.864454	00:09
468	0.430229	1.152602	0.863655	00:09
469	0.434422	0.739431	0.861655	00:09
470	0.446269	1.953928	0.860856	00:09
471	0.449138	1.547201	0.861655	00:09
472	0.460502	0.564481	0.865254	00:10
473	0.446018	0.947188	0.866054	00:09
474	0.468942	0.662267	0.862455	00:09
475	0.463121	0.925879	0.867653	00:09
476	0.464442	0.565463	0.865654	00:09
477	0.450507	1.314708	0.866853	00:09
478	0.463095	1.131033	0.865654	00:09
479	0.440595	0.602443	0.864454	00:09
480	0.438689	1.146884	0.863655	00:09
481	0.438853	0.603356	0.866453	00:09
482	0.441689	1.178955	0.868053	00:09
483	0.436567	1.104582	0.864054	00:09
484	0.450260	0.666652	0.868053	00:09
485	0.454909	0.685825	0.864054	00:09
486	0.435910	0.699765	0.864854	00:09
487	0.434441	0.543610	0.866853	00:09
488	0.461130	0.654332	0.864854	00:09
489	0.462865	1.242453	0.863255	00:09
490	0.442781	0.566688	0.867253	00:09
491	0.454518	0.846261	0.862855	00:09
492	0.448200	0.911909	0.865654	00:09
493	0.442718	0.568142	0.864054	00:09
494	0.459015	0.619626	0.868053	00:09
495	0.451455	1.352842	0.866453	00:09
496	0.451295	1.653014	0.862455	00:09
497	0.451368	1.800567	0.866054	00:09
498	0.449293	1.731730	0.865254	00:10
499	0.419673	0.576760	0.867253	00:11

True positive:  [  162   137   153    38 43510]
True negative:  [48392 47988 48682 49073   651]
False positive:  [  77   73  103   51 5958]
False negative:  [1631 2064 1324 1100  143]

[[  162    11    43    11  1566]
 [    6   137    22    13  2023]
 [   19    11   153     0  1294]
 [    8    13     4    38  1075]
 [   44    38    34    27 43510]]

              precision    recall  f1-score   support

           0       0.68      0.09      0.16      1793
           1       0.65      0.06      0.11      2201
           2       0.60      0.10      0.18      1477
           3       0.43      0.03      0.06      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.88     50262
   macro avg       0.65      0.26      0.29     50262
weighted avg       0.84      0.88      0.83     50262

Accuracy:  0.875412836735506
Precision_weighted:  0.8438825080216189
Recall_weighted:  0.875412836735506
mcc:  0.2312440216282092
f2:  0.8689196761933893
