{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AndMal_PyTorch.ipynb","provenance":[],"mount_file_id":"1xXbrIt0BFYZj3BZ5lv6P0Wr01Yq51Sr-","authorship_tag":"ABX9TyN4uxIc/M8p1+pdKmmUtKRz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sHDsrAJqmeaY","executionInfo":{"status":"ok","timestamp":1621869269534,"user_tz":-120,"elapsed":4959,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as torch_optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"04ZXjd9MhmlY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621869276420,"user_tz":-120,"elapsed":3065,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"65b843fd-0bd0-434e-a287-81225d79f6f2"},"source":["path = './drive/MyDrive/Materiale_Pellegrino_personal/AndMal2017/AndMal_Shuffled.csv'\n","dataset = pd.read_csv(path)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"AyNqMGc1Zf-g","executionInfo":{"status":"ok","timestamp":1621869279932,"user_tz":-120,"elapsed":1402,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["train = dataset[0:50259].copy()\n","test = dataset[50259:].copy()\n","\n","# elimino la riga e resetto l'indice\n","test = test.drop(index=36889+50259)\n","test = test.reset_index(drop=True)\n","test = test.drop(index=37822)\n","test = test.reset_index(drop=True)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VpLZPPa258q","executionInfo":{"status":"ok","timestamp":1621869287170,"user_tz":-120,"elapsed":809,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["# Tolgo gli spazi dalle etichette delle colonne\n","train.columns = [x.replace(\" \", \"\") for x in train.columns]\n","test.columns = [x.replace(\" \", \"\") for x in test.columns]\n","\n","\"\"\" Il dataset presentava all'interno della stessa colonna, per ogni features, valori di tipo diverso.\n","  Ho distinto quelli interi da quelli continui e ho convertito per uniformarli \"\"\"\n","\n","# convert columns to int64 dtype\n","train = train.astype({\"Protocol\": int, \"FlowDuration\": int, 'TotalFwdPackets':int, 'TotalBackwardPackets':int,'TotalLengthofFwdPackets':int, 'TotalLengthofBwdPackets':int,\n","                'FwdPacketLengthMax':int, 'FwdPacketLengthMin':int, 'BwdPacketLengthMax':int, 'BwdPacketLengthMin':int, 'FwdPSHFlags':int, 'BwdPSHFlags':int,\n","                'FwdURGFlags':int, 'BwdURGFlags':int, 'MinPacketLength':int, 'MaxPacketLength':int, 'FINFlagCount':int, 'SYNFlagCount':int, \n","                'RSTFlagCount':int, 'PSHFlagCount':int, 'ACKFlagCount':int, 'URGFlagCount':int, 'CWEFlagCount':int, 'ECEFlagCount':int, 'Down/UpRatio':int,\n","                'FwdAvgBytes/Bulk':int, 'FwdAvgPackets/Bulk':int, 'FwdAvgBulkRate':int, 'BwdAvgBytes/Bulk':int, 'BwdAvgPackets/Bulk':int, 'BwdAvgBulkRate':int,\n","                'SubflowFwdPackets':int, 'SubflowFwdBytes':int, 'SubflowBwdPackets':int, 'SubflowBwdBytes':int, 'Init_Win_bytes_forward':int, \n","                'Init_Win_bytes_backward':int, 'act_data_pkt_fwd':int, 'min_seg_size_forward':int})\n","\n","# convert columns to float64 dtype\n","train = train.astype({'FwdPacketLengthMean':float, 'FwdPacketLengthStd':float, 'BwdPacketLengthMean':float, 'BwdPacketLengthStd':float, 'FlowBytes/s':float, \n","                'FlowPackets/s':float, 'FlowIATMean':float, 'FlowIATStd':float, 'FlowIATMax':float, 'FlowIATMin':float, 'FwdIATTotal':float, 'FwdIATMean':float,\n","                'FwdIATStd':float, 'FwdIATMax':float, 'FwdIATMin':float, 'BwdIATTotal':float, 'BwdIATMean':float, 'BwdIATStd':float, 'BwdIATMax':float,\n","                'BwdIATMin':float, 'FwdHeaderLength1':float, 'BwdHeaderLength':float, 'FwdPackets/s':float, 'BwdPackets/s':float, 'PacketLengthMean':float,\n","                'PacketLengthStd':float, 'PacketLengthVariance':float, 'AveragePacketSize':float, 'AvgFwdSegmentSize':float, 'AvgBwdSegmentSize' :float, \n","                'FwdHeaderLength2':float, 'ActiveMean':float, 'ActiveStd':float, 'ActiveMax':float, 'ActiveMin':float, 'IdleMean':float, 'IdleStd':float,\n","                'IdleMax':float, 'IdleMin':float})\n","\n","test = test.astype({\"Protocol\": int, \"FlowDuration\": int, 'TotalFwdPackets':int, 'TotalBackwardPackets':int,'TotalLengthofFwdPackets':int, 'TotalLengthofBwdPackets':int,\n","                'FwdPacketLengthMax':int, 'FwdPacketLengthMin':int, 'BwdPacketLengthMax':int, 'BwdPacketLengthMin':int, 'FwdPSHFlags':int, 'BwdPSHFlags':int,\n","                'FwdURGFlags':int, 'BwdURGFlags':int, 'MinPacketLength':int, 'MaxPacketLength':int, 'FINFlagCount':int, 'SYNFlagCount':int, \n","                'RSTFlagCount':int, 'PSHFlagCount':int, 'ACKFlagCount':int, 'URGFlagCount':int, 'CWEFlagCount':int, 'ECEFlagCount':int, 'Down/UpRatio':int,\n","                'FwdAvgBytes/Bulk':int, 'FwdAvgPackets/Bulk':int, 'FwdAvgBulkRate':int, 'BwdAvgBytes/Bulk':int, 'BwdAvgPackets/Bulk':int, 'BwdAvgBulkRate':int,\n","                'SubflowFwdPackets':int, 'SubflowFwdBytes':int, 'SubflowBwdPackets':int, 'SubflowBwdBytes':int, 'Init_Win_bytes_forward':int, \n","                'Init_Win_bytes_backward':int, 'act_data_pkt_fwd':int, 'min_seg_size_forward':int})\n","\n","# convert columns to float64 dtype\n","test = test.astype({'FwdPacketLengthMean':float, 'FwdPacketLengthStd':float, 'BwdPacketLengthMean':float, 'BwdPacketLengthStd':float, 'FlowBytes/s':float, \n","                'FlowPackets/s':float, 'FlowIATMean':float, 'FlowIATStd':float, 'FlowIATMax':float, 'FlowIATMin':float, 'FwdIATTotal':float, 'FwdIATMean':float,\n","                'FwdIATStd':float, 'FwdIATMax':float, 'FwdIATMin':float, 'BwdIATTotal':float, 'BwdIATMean':float, 'BwdIATStd':float, 'BwdIATMax':float,\n","                'BwdIATMin':float, 'FwdHeaderLength1':float, 'BwdHeaderLength':float, 'FwdPackets/s':float, 'BwdPackets/s':float, 'PacketLengthMean':float,\n","                'PacketLengthStd':float, 'PacketLengthVariance':float, 'AveragePacketSize':float, 'AvgFwdSegmentSize':float, 'AvgBwdSegmentSize' :float, \n","                'FwdHeaderLength2':float, 'ActiveMean':float, 'ActiveStd':float, 'ActiveMax':float, 'ActiveMin':float, 'IdleMean':float, 'IdleStd':float,\n","                'IdleMax':float, 'IdleMin':float})"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeNMm_2znfrM","executionInfo":{"status":"ok","timestamp":1621869290328,"user_tz":-120,"elapsed":4,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["# features inutili per l'addestramento\n","unnecessary_features = ['BwdPSHFlags', 'FwdURGFlags', 'BwdURGFlags', 'RSTFlagCount', 'CWEFlagCount', 'ECEFlagCount', \n","                        'FwdAvgBytes/Bulk', 'FwdAvgPackets/Bulk', 'FwdAvgBulkRate', 'BwdAvgBytes/Bulk', 'BwdAvgPackets/Bulk', 'BwdAvgBulkRate']\n","\n","# elimino le colonne che corrispondono alle caratteristiche inutili in test e train\n","train = train.drop(labels=unnecessary_features, axis=1)\n","test = test.drop(labels=unnecessary_features, axis=1)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghwOUeUlMegO","executionInfo":{"status":"ok","timestamp":1621869293983,"user_tz":-120,"elapsed":297,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["# LabelEncoding della variabile target \n","dep_var = 'multilabel'\n","y_train = LabelEncoder().fit_transform(train[dep_var])\n","y_test = LabelEncoder().fit_transform(test[dep_var])\n","\n","# elimino la colonna target da test e train\n","train = train.drop(dep_var, axis=1)\n","test = test.drop(dep_var, axis=1)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fIqmedo6syM","executionInfo":{"status":"ok","timestamp":1621869297122,"user_tz":-120,"elapsed":5,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"09ff5864-6548-4ae8-b23a-d8b9c54509c6"},"source":["len(train)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50259"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"id":"5ANU4qJeQ3o0","executionInfo":{"status":"ok","timestamp":1621869299436,"user_tz":-120,"elapsed":921,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"5e966f2e-88e5-4899-a912-68f936210fd9"},"source":["train"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Protocol</th>\n","      <th>FlowDuration</th>\n","      <th>TotalFwdPackets</th>\n","      <th>TotalBackwardPackets</th>\n","      <th>TotalLengthofFwdPackets</th>\n","      <th>TotalLengthofBwdPackets</th>\n","      <th>FwdPacketLengthMax</th>\n","      <th>FwdPacketLengthMin</th>\n","      <th>FwdPacketLengthMean</th>\n","      <th>FwdPacketLengthStd</th>\n","      <th>BwdPacketLengthMax</th>\n","      <th>BwdPacketLengthMin</th>\n","      <th>BwdPacketLengthMean</th>\n","      <th>BwdPacketLengthStd</th>\n","      <th>FlowBytes/s</th>\n","      <th>FlowPackets/s</th>\n","      <th>FlowIATMean</th>\n","      <th>FlowIATStd</th>\n","      <th>FlowIATMax</th>\n","      <th>FlowIATMin</th>\n","      <th>FwdIATTotal</th>\n","      <th>FwdIATMean</th>\n","      <th>FwdIATStd</th>\n","      <th>FwdIATMax</th>\n","      <th>FwdIATMin</th>\n","      <th>BwdIATTotal</th>\n","      <th>BwdIATMean</th>\n","      <th>BwdIATStd</th>\n","      <th>BwdIATMax</th>\n","      <th>BwdIATMin</th>\n","      <th>FwdPSHFlags</th>\n","      <th>FwdHeaderLength1</th>\n","      <th>BwdHeaderLength</th>\n","      <th>FwdPackets/s</th>\n","      <th>BwdPackets/s</th>\n","      <th>MinPacketLength</th>\n","      <th>MaxPacketLength</th>\n","      <th>PacketLengthMean</th>\n","      <th>PacketLengthStd</th>\n","      <th>PacketLengthVariance</th>\n","      <th>FINFlagCount</th>\n","      <th>SYNFlagCount</th>\n","      <th>PSHFlagCount</th>\n","      <th>ACKFlagCount</th>\n","      <th>URGFlagCount</th>\n","      <th>Down/UpRatio</th>\n","      <th>AveragePacketSize</th>\n","      <th>AvgFwdSegmentSize</th>\n","      <th>AvgBwdSegmentSize</th>\n","      <th>FwdHeaderLength2</th>\n","      <th>SubflowFwdPackets</th>\n","      <th>SubflowFwdBytes</th>\n","      <th>SubflowBwdPackets</th>\n","      <th>SubflowBwdBytes</th>\n","      <th>Init_Win_bytes_forward</th>\n","      <th>Init_Win_bytes_backward</th>\n","      <th>act_data_pkt_fwd</th>\n","      <th>min_seg_size_forward</th>\n","      <th>ActiveMean</th>\n","      <th>ActiveStd</th>\n","      <th>ActiveMax</th>\n","      <th>ActiveMin</th>\n","      <th>IdleMean</th>\n","      <th>IdleStd</th>\n","      <th>IdleMax</th>\n","      <th>IdleMin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>2520</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>793.650794</td>\n","      <td>2.520000e+03</td>\n","      <td>0.000000</td>\n","      <td>2520.0</td>\n","      <td>2520.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>396.825397</td>\n","      <td>396.825397</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>362</td>\n","      <td>1594</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6</td>\n","      <td>2518</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>794.281176</td>\n","      <td>2.518000e+03</td>\n","      <td>0.000000</td>\n","      <td>2518.0</td>\n","      <td>2518.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>397.140588</td>\n","      <td>397.140588</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>383</td>\n","      <td>1593</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17</td>\n","      <td>154712</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>86</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31.000000</td>\n","      <td>0.000000</td>\n","      <td>86</td>\n","      <td>86</td>\n","      <td>86.00</td>\n","      <td>0.000000</td>\n","      <td>7.562439e+02</td>\n","      <td>12.927245</td>\n","      <td>1.547120e+05</td>\n","      <td>0.000000</td>\n","      <td>154712.0</td>\n","      <td>154712.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>6.463623</td>\n","      <td>6.463623</td>\n","      <td>31</td>\n","      <td>86</td>\n","      <td>49.333333</td>\n","      <td>31.754265</td>\n","      <td>1008.333333</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>74.00</td>\n","      <td>31.000000</td>\n","      <td>86.00</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>1</td>\n","      <td>86</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>29</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>55</td>\n","      <td>0</td>\n","      <td>55</td>\n","      <td>55</td>\n","      <td>55.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>1.896552e+06</td>\n","      <td>68965.517240</td>\n","      <td>2.900000e+01</td>\n","      <td>0.000000</td>\n","      <td>29.0</td>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>34482.758620</td>\n","      <td>34482.758620</td>\n","      <td>0</td>\n","      <td>55</td>\n","      <td>36.666667</td>\n","      <td>31.754265</td>\n","      <td>1008.333333</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>55.00</td>\n","      <td>55.000000</td>\n","      <td>0.00</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>55</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>362</td>\n","      <td>362</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17</td>\n","      <td>299103</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>85</td>\n","      <td>35</td>\n","      <td>35</td>\n","      <td>35.000000</td>\n","      <td>0.000000</td>\n","      <td>85</td>\n","      <td>85</td>\n","      <td>85.00</td>\n","      <td>0.000000</td>\n","      <td>4.011996e+02</td>\n","      <td>6.686660</td>\n","      <td>2.991030e+05</td>\n","      <td>0.000000</td>\n","      <td>299103.0</td>\n","      <td>299103.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>20.0</td>\n","      <td>20.0</td>\n","      <td>3.343330</td>\n","      <td>3.343330</td>\n","      <td>35</td>\n","      <td>85</td>\n","      <td>51.666667</td>\n","      <td>28.867513</td>\n","      <td>833.333333</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>77.50</td>\n","      <td>35.000000</td>\n","      <td>85.00</td>\n","      <td>20.0</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>50254</th>\n","      <td>6</td>\n","      <td>1143</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>3499.562555</td>\n","      <td>3.810000e+02</td>\n","      <td>403.301376</td>\n","      <td>839.0</td>\n","      <td>79.0</td>\n","      <td>1143.0</td>\n","      <td>3.810000e+02</td>\n","      <td>403.301376</td>\n","      <td>839.0</td>\n","      <td>79.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>128.0</td>\n","      <td>0.0</td>\n","      <td>3499.562555</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>128.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3105</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>50255</th>\n","      <td>6</td>\n","      <td>3572</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>559.910414</td>\n","      <td>3.572000e+03</td>\n","      <td>0.000000</td>\n","      <td>3572.0</td>\n","      <td>3572.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>279.955207</td>\n","      <td>279.955207</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>1369</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>50256</th>\n","      <td>6</td>\n","      <td>200537</td>\n","      <td>12</td>\n","      <td>8</td>\n","      <td>7871</td>\n","      <td>1046</td>\n","      <td>1368</td>\n","      <td>0</td>\n","      <td>655.916667</td>\n","      <td>661.719037</td>\n","      <td>586</td>\n","      <td>0</td>\n","      <td>130.75</td>\n","      <td>216.323006</td>\n","      <td>4.446561e+04</td>\n","      <td>99.732219</td>\n","      <td>1.055458e+04</td>\n","      <td>18184.560870</td>\n","      <td>62431.0</td>\n","      <td>2.0</td>\n","      <td>200537.0</td>\n","      <td>1.823064e+04</td>\n","      <td>24717.518580</td>\n","      <td>69093.0</td>\n","      <td>2.0</td>\n","      <td>156750.0</td>\n","      <td>22392.85714</td>\n","      <td>28038.35904</td>\n","      <td>63241.0</td>\n","      <td>6.0</td>\n","      <td>0</td>\n","      <td>392.0</td>\n","      <td>264.0</td>\n","      <td>59.839331</td>\n","      <td>39.892888</td>\n","      <td>0</td>\n","      <td>1368</td>\n","      <td>424.619048</td>\n","      <td>576.945359</td>\n","      <td>332865.947600</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>445.85</td>\n","      <td>655.916667</td>\n","      <td>130.75</td>\n","      <td>392.0</td>\n","      <td>12</td>\n","      <td>7871</td>\n","      <td>8</td>\n","      <td>1046</td>\n","      <td>65535</td>\n","      <td>468</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>50257</th>\n","      <td>6</td>\n","      <td>249392</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>12.029255</td>\n","      <td>1.246960e+05</td>\n","      <td>142439.590000</td>\n","      <td>225416.0</td>\n","      <td>23976.0</td>\n","      <td>225416.0</td>\n","      <td>2.254160e+05</td>\n","      <td>0.000000</td>\n","      <td>225416.0</td>\n","      <td>225416.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>64.0</td>\n","      <td>32.0</td>\n","      <td>8.019503</td>\n","      <td>4.009752</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>64.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>349</td>\n","      <td>349</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>50258</th>\n","      <td>6</td>\n","      <td>10056405</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.198878</td>\n","      <td>1.010000e+07</td>\n","      <td>0.000000</td>\n","      <td>10100000.0</td>\n","      <td>10100000.0</td>\n","      <td>10100000.0</td>\n","      <td>1.010000e+07</td>\n","      <td>0.000000</td>\n","      <td>10100000.0</td>\n","      <td>10100000.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>64.0</td>\n","      <td>0.0</td>\n","      <td>0.198878</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>64.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1405</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50259 rows Ã— 66 columns</p>\n","</div>"],"text/plain":["       Protocol  FlowDuration  TotalFwdPackets  ...  IdleStd  IdleMax  IdleMin\n","0             6          2520                1  ...      0.0      0.0      0.0\n","1             6          2518                1  ...      0.0      0.0      0.0\n","2            17        154712                1  ...      0.0      0.0      0.0\n","3             6            29                1  ...      0.0      0.0      0.0\n","4            17        299103                1  ...      0.0      0.0      0.0\n","...         ...           ...              ...  ...      ...      ...      ...\n","50254         6          1143                4  ...      0.0      0.0      0.0\n","50255         6          3572                1  ...      0.0      0.0      0.0\n","50256         6        200537               12  ...      0.0      0.0      0.0\n","50257         6        249392                2  ...      0.0      0.0      0.0\n","50258         6      10056405                2  ...      0.0      0.0      0.0\n","\n","[50259 rows x 66 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9owTDxO6RvDl","executionInfo":{"status":"ok","timestamp":1621869303787,"user_tz":-120,"elapsed":755,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"6c897384-c848-4ba1-a452-3efdfe62309d"},"source":["len(y_train)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50259"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"O6nwjEB65Usp","executionInfo":{"status":"ok","timestamp":1621869306505,"user_tz":-120,"elapsed":419,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["# validation di 2500 righe da train\n","train, validation, y_train, y_val = train_test_split(train, y_train, test_size=0.04974233471, random_state=0)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"id":"DM7g06120PeI","executionInfo":{"status":"ok","timestamp":1621869309452,"user_tz":-120,"elapsed":891,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"bb3392c2-9e4d-49ad-f699-207b639cb22d"},"source":["train"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Protocol</th>\n","      <th>FlowDuration</th>\n","      <th>TotalFwdPackets</th>\n","      <th>TotalBackwardPackets</th>\n","      <th>TotalLengthofFwdPackets</th>\n","      <th>TotalLengthofBwdPackets</th>\n","      <th>FwdPacketLengthMax</th>\n","      <th>FwdPacketLengthMin</th>\n","      <th>FwdPacketLengthMean</th>\n","      <th>FwdPacketLengthStd</th>\n","      <th>BwdPacketLengthMax</th>\n","      <th>BwdPacketLengthMin</th>\n","      <th>BwdPacketLengthMean</th>\n","      <th>BwdPacketLengthStd</th>\n","      <th>FlowBytes/s</th>\n","      <th>FlowPackets/s</th>\n","      <th>FlowIATMean</th>\n","      <th>FlowIATStd</th>\n","      <th>FlowIATMax</th>\n","      <th>FlowIATMin</th>\n","      <th>FwdIATTotal</th>\n","      <th>FwdIATMean</th>\n","      <th>FwdIATStd</th>\n","      <th>FwdIATMax</th>\n","      <th>FwdIATMin</th>\n","      <th>BwdIATTotal</th>\n","      <th>BwdIATMean</th>\n","      <th>BwdIATStd</th>\n","      <th>BwdIATMax</th>\n","      <th>BwdIATMin</th>\n","      <th>FwdPSHFlags</th>\n","      <th>FwdHeaderLength1</th>\n","      <th>BwdHeaderLength</th>\n","      <th>FwdPackets/s</th>\n","      <th>BwdPackets/s</th>\n","      <th>MinPacketLength</th>\n","      <th>MaxPacketLength</th>\n","      <th>PacketLengthMean</th>\n","      <th>PacketLengthStd</th>\n","      <th>PacketLengthVariance</th>\n","      <th>FINFlagCount</th>\n","      <th>SYNFlagCount</th>\n","      <th>PSHFlagCount</th>\n","      <th>ACKFlagCount</th>\n","      <th>URGFlagCount</th>\n","      <th>Down/UpRatio</th>\n","      <th>AveragePacketSize</th>\n","      <th>AvgFwdSegmentSize</th>\n","      <th>AvgBwdSegmentSize</th>\n","      <th>FwdHeaderLength2</th>\n","      <th>SubflowFwdPackets</th>\n","      <th>SubflowFwdBytes</th>\n","      <th>SubflowBwdPackets</th>\n","      <th>SubflowBwdBytes</th>\n","      <th>Init_Win_bytes_forward</th>\n","      <th>Init_Win_bytes_backward</th>\n","      <th>act_data_pkt_fwd</th>\n","      <th>min_seg_size_forward</th>\n","      <th>ActiveMean</th>\n","      <th>ActiveStd</th>\n","      <th>ActiveMax</th>\n","      <th>ActiveMin</th>\n","      <th>IdleMean</th>\n","      <th>IdleStd</th>\n","      <th>IdleMax</th>\n","      <th>IdleMin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>16561</th>\n","      <td>6</td>\n","      <td>15409502</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.129790</td>\n","      <td>15400000.0</td>\n","      <td>0.000000e+00</td>\n","      <td>15400000.0</td>\n","      <td>15400000.0</td>\n","      <td>15400000.0</td>\n","      <td>1.540000e+07</td>\n","      <td>0.000</td>\n","      <td>15400000.0</td>\n","      <td>15400000.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>64.0</td>\n","      <td>0.0</td>\n","      <td>0.129790</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1641</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>25773</th>\n","      <td>6</td>\n","      <td>2520743</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.793417</td>\n","      <td>2520743.0</td>\n","      <td>0.000000e+00</td>\n","      <td>2520743.0</td>\n","      <td>2520743.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>20.0</td>\n","      <td>0.396708</td>\n","      <td>0.396708</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2373</td>\n","      <td>519</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>19870</th>\n","      <td>6</td>\n","      <td>143</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31.00</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>216783.216800</td>\n","      <td>13986.013990</td>\n","      <td>143.0</td>\n","      <td>0.000000e+00</td>\n","      <td>143.0</td>\n","      <td>143.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>6993.006993</td>\n","      <td>6993.006993</td>\n","      <td>0</td>\n","      <td>31</td>\n","      <td>20.666667</td>\n","      <td>17.897858</td>\n","      <td>320.333333</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>31.000000</td>\n","      <td>31.00</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>148</td>\n","      <td>148</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>27535</th>\n","      <td>6</td>\n","      <td>35</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>57142.857140</td>\n","      <td>35.0</td>\n","      <td>0.000000e+00</td>\n","      <td>35.0</td>\n","      <td>35.0</td>\n","      <td>35.0</td>\n","      <td>3.500000e+01</td>\n","      <td>0.000</td>\n","      <td>35.0</td>\n","      <td>35.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>40.0</td>\n","      <td>0.0</td>\n","      <td>57142.857140</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>16383</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10851</th>\n","      <td>6</td>\n","      <td>12342825</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.324075</td>\n","      <td>4114275.0</td>\n","      <td>6.905047e+06</td>\n","      <td>12100000.0</td>\n","      <td>1945.0</td>\n","      <td>12300000.0</td>\n","      <td>6.171412e+06</td>\n","      <td>8364774.115</td>\n","      <td>12100000.0</td>\n","      <td>256624.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>80.0</td>\n","      <td>32.0</td>\n","      <td>0.243056</td>\n","      <td>0.081019</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>80.0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>65535</td>\n","      <td>14600</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21243</th>\n","      <td>6</td>\n","      <td>24583</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>122.035553</td>\n","      <td>12291.5</td>\n","      <td>1.659085e+04</td>\n","      <td>24023.0</td>\n","      <td>560.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>560.0</td>\n","      <td>560.0</td>\n","      <td>0.0</td>\n","      <td>560.0</td>\n","      <td>560.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>76.0</td>\n","      <td>40.678518</td>\n","      <td>81.357035</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>360</td>\n","      <td>1637</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>45891</th>\n","      <td>6</td>\n","      <td>54669</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>568</td>\n","      <td>416</td>\n","      <td>568</td>\n","      <td>0</td>\n","      <td>284.00</td>\n","      <td>401.636652</td>\n","      <td>416</td>\n","      <td>416</td>\n","      <td>416.0</td>\n","      <td>0.0</td>\n","      <td>17999.231740</td>\n","      <td>54.875707</td>\n","      <td>27334.5</td>\n","      <td>3.440994e+04</td>\n","      <td>51666.0</td>\n","      <td>3003.0</td>\n","      <td>54669.0</td>\n","      <td>5.466900e+04</td>\n","      <td>0.000</td>\n","      <td>54669.0</td>\n","      <td>54669.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>64.0</td>\n","      <td>32.0</td>\n","      <td>36.583804</td>\n","      <td>18.291902</td>\n","      <td>0</td>\n","      <td>568</td>\n","      <td>388.000000</td>\n","      <td>268.407650</td>\n","      <td>72042.666670</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>517.333333</td>\n","      <td>284.00</td>\n","      <td>416.0</td>\n","      <td>64.0</td>\n","      <td>2</td>\n","      <td>568</td>\n","      <td>1</td>\n","      <td>416</td>\n","      <td>1403</td>\n","      <td>378</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>42613</th>\n","      <td>17</td>\n","      <td>581191</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>45</td>\n","      <td>29</td>\n","      <td>29</td>\n","      <td>29.00</td>\n","      <td>0.000000</td>\n","      <td>45</td>\n","      <td>45</td>\n","      <td>45.0</td>\n","      <td>0.0</td>\n","      <td>127.324752</td>\n","      <td>3.441210</td>\n","      <td>581191.0</td>\n","      <td>0.000000e+00</td>\n","      <td>581191.0</td>\n","      <td>581191.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>1.720605</td>\n","      <td>1.720605</td>\n","      <td>29</td>\n","      <td>45</td>\n","      <td>34.333333</td>\n","      <td>9.237604</td>\n","      <td>85.333333</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>51.500000</td>\n","      <td>29.00</td>\n","      <td>45.0</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>1</td>\n","      <td>45</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>43567</th>\n","      <td>6</td>\n","      <td>75567564</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>429</td>\n","      <td>1362</td>\n","      <td>429</td>\n","      <td>0</td>\n","      <td>107.25</td>\n","      <td>214.500000</td>\n","      <td>1362</td>\n","      <td>0</td>\n","      <td>340.5</td>\n","      <td>681.0</td>\n","      <td>23.700645</td>\n","      <td>0.105866</td>\n","      <td>10800000.0</td>\n","      <td>2.830000e+07</td>\n","      <td>75000000.0</td>\n","      <td>2359.0</td>\n","      <td>567296.0</td>\n","      <td>1.890987e+05</td>\n","      <td>163197.234</td>\n","      <td>322036.0</td>\n","      <td>6962.0</td>\n","      <td>75300000.0</td>\n","      <td>25100000.0</td>\n","      <td>43200000.0</td>\n","      <td>75000000.0</td>\n","      <td>47407.0</td>\n","      <td>0</td>\n","      <td>100.0</td>\n","      <td>92.0</td>\n","      <td>0.052933</td>\n","      <td>0.052933</td>\n","      <td>0</td>\n","      <td>1362</td>\n","      <td>199.000000</td>\n","      <td>458.622394</td>\n","      <td>210334.500000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>223.875000</td>\n","      <td>107.25</td>\n","      <td>340.5</td>\n","      <td>100.0</td>\n","      <td>4</td>\n","      <td>429</td>\n","      <td>4</td>\n","      <td>1362</td>\n","      <td>65535</td>\n","      <td>123</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2732</th>\n","      <td>17</td>\n","      <td>94701</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>96</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37.00</td>\n","      <td>0.000000</td>\n","      <td>96</td>\n","      <td>96</td>\n","      <td>96.0</td>\n","      <td>0.0</td>\n","      <td>1404.420228</td>\n","      <td>21.119101</td>\n","      <td>94701.0</td>\n","      <td>0.000000e+00</td>\n","      <td>94701.0</td>\n","      <td>94701.0</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>32.0</td>\n","      <td>10.559551</td>\n","      <td>10.559551</td>\n","      <td>37</td>\n","      <td>96</td>\n","      <td>56.666667</td>\n","      <td>34.063666</td>\n","      <td>1160.333333</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>85.000000</td>\n","      <td>37.00</td>\n","      <td>96.0</td>\n","      <td>32.0</td>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>1</td>\n","      <td>96</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>47758 rows Ã— 66 columns</p>\n","</div>"],"text/plain":["       Protocol  FlowDuration  TotalFwdPackets  ...  IdleStd  IdleMax  IdleMin\n","16561         6      15409502                2  ...      0.0      0.0      0.0\n","25773         6       2520743                1  ...      0.0      0.0      0.0\n","19870         6           143                1  ...      0.0      0.0      0.0\n","27535         6            35                2  ...      0.0      0.0      0.0\n","10851         6      12342825                3  ...      0.0      0.0      0.0\n","...         ...           ...              ...  ...      ...      ...      ...\n","21243         6         24583                1  ...      0.0      0.0      0.0\n","45891         6         54669                2  ...      0.0      0.0      0.0\n","42613        17        581191                1  ...      0.0      0.0      0.0\n","43567         6      75567564                4  ...      0.0      0.0      0.0\n","2732         17         94701                1  ...      0.0      0.0      0.0\n","\n","[47758 rows x 66 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeEmxMw1NEVL","executionInfo":{"status":"ok","timestamp":1621869310891,"user_tz":-120,"elapsed":3,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"c1d8836f-c534-466e-80b9-bb70f238f186"},"source":["cat_names = []\n","cont_names = [col for col in train.columns if col not in cat_names and col != dep_var]\n","print(len(cont_names))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["66\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Oa_SzfA6bq7","executionInfo":{"status":"ok","timestamp":1621869313211,"user_tz":-120,"elapsed":382,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["\"\"\" Pytorch Dataset e DataLoader\n","Estendiamo la Datasetclasse (astratta) fornita da Pytorch per un accesso piÃ¹ facile al nostro set di dati durante l'addestramento \n","e per utilizzare efficacemente  il DataLoader modulo per gestire i batch. CiÃ² comporta la sovrascrittura dei metodi __len__e __getitem__\n","secondo il nostro particolare set di dati.\n","PoichÃ© abbiamo solo bisogno di incorporare colonne categoriali, dividiamo il nostro input in due parti: numerico e categoriale. \"\"\" \n","\n","class AndMal_Dataset(Dataset):\n","    def __init__(self, X, Y):\n","        X = X.copy()\n","        self.X = X.copy().values.astype(np.float32) #numerical columns\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","        \n","#creating train and valid datasets\n","train_ds = AndMal_Dataset(train, y_train)\n","valid_ds = AndMal_Dataset(validation, y_val)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oGGkh8o6icS","executionInfo":{"status":"ok","timestamp":1621869316635,"user_tz":-120,"elapsed":785,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"01885784-f77c-4f94-ecb4-42afabe57477"},"source":["\"\"\" Making device (GPU/CPU) compatible\n","\n","(borrowed from https://jovian.ml/aakashns/04-feedforward-nn)\n","\n","In order to make use of a GPU if available, we'll have to move our data and model to it. \"\"\" \n","\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","device"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"wZr4ZQflaCAw","executionInfo":{"status":"ok","timestamp":1621869319684,"user_tz":-120,"elapsed":439,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["class AndMalModel(nn.Module):\n","    def __init__(self, n_cont):\n","        super().__init__()\n","        self.n_cont =  n_cont\n","        self.lin1 = nn.Linear(self.n_cont, 200)\n","        self.lin2 = nn.Linear(200, 100)\n","        self.lin3 = nn.Linear(100, 5)\n","        self.bn1 = nn.BatchNorm1d(self.n_cont)\n","        self.bn2 = nn.BatchNorm1d(200)\n","        self.bn3 = nn.BatchNorm1d(100)\n","        self.drops = nn.Dropout(0.3)\n","        \n","\n","    def forward(self, x_cont):\n","        x = self.bn1(x_cont)\n","        x = F.relu(self.lin1(x))\n","        x = self.drops(x)\n","        x = self.bn2(x)\n","        x = F.relu(self.lin2(x))\n","        x = self.drops(x)\n","        x = self.bn3(x)\n","        x = self.lin3(x)\n","        return x"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQc6Y5qBapPL","executionInfo":{"status":"ok","timestamp":1621869321980,"user_tz":-120,"elapsed":3,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["\"\"\" Fase di preparazione per l'addestramento \"\"\"\n","\n","# Optimizer\n","def get_optimizer(model, lr = 0.001, wd = 0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim\n","\n","# Training function\n","def train_model(model, optim, train_dl):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x, y in train_dl:\n","        batch = y.shape[0]\n","        output = model(x)\n","        loss = F.cross_entropy(output, y)   \n","        optim.zero_grad()\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","# Evaluation function\n","def val_loss(model, valid_dl):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x, y in valid_dl:\n","        current_batch_size = y.shape[0]\n","        out = model(x)\n","        loss = F.cross_entropy(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.max(out, 1)[1]\n","        correct += (pred == y).float().sum().item()\n","    print('valid loss ', sum_loss/total, ' and accuracy ', correct/total)\n","    return sum_loss/total, correct/total\n","\n","# Funzione per l'addestramento \n","def train_loop(model, epochs, lr=0.01, wd=0.0):\n","    optim = get_optimizer(model, lr = lr, wd = wd)\n","    for i in range(epochs): \n","        loss = train_model(model, optim, train_dl)\n","        print('ep ', i, \" training loss: \", loss)\n","        val_loss(model, valid_dl)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNNiIdczasqp","executionInfo":{"status":"ok","timestamp":1621869325413,"user_tz":-120,"elapsed":777,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["\"\"\" Ora addestriamo il modello sul set di addestramento. Ho usato l'ottimizzatore Adam per ottimizzare la perdita di entropia incrociata. \n","L'addestramento Ã¨ piuttosto semplice: iterare attraverso ogni batch, eseguire un passaggio in avanti, calcolare i gradienti, \n","eseguire una discesa del gradiente e ripetere questo processo per tutte le epoche necessarie. \"\"\" \n","\n","batch_size = 512\n","train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIE4j_1iOOP4","executionInfo":{"status":"ok","timestamp":1621869338306,"user_tz":-120,"elapsed":9616,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"bac539a2-437c-43ac-982f-06a59bf22d5b"},"source":["model = AndMalModel(len(cont_names))\n","to_device(model, device)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AndMalModel(\n","  (lin1): Linear(in_features=66, out_features=200, bias=True)\n","  (lin2): Linear(in_features=200, out_features=100, bias=True)\n","  (lin3): Linear(in_features=100, out_features=5, bias=True)\n","  (bn1): BatchNorm1d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (drops): Dropout(p=0.3, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b976OcWbQLBe","executionInfo":{"status":"ok","timestamp":1621870070364,"user_tz":-120,"elapsed":718321,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"94906282-a2bc-4299-dfeb-3c58334405fb"},"source":["train_loop(model, epochs=1000, lr=0.002)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["ep  0  training loss:  1.0349202817885879\n","valid loss  0.5863864534809703  and accuracy  0.8632546981207517\n","ep  1  training loss:  0.5863286175354526\n","valid loss  0.57076424429389  and accuracy  0.8628548580567773\n","ep  2  training loss:  0.5663189485752131\n","valid loss  0.5698733895790667  and accuracy  0.8628548580567773\n","ep  3  training loss:  0.5585340600206632\n","valid loss  0.5636246098322375  and accuracy  0.8624550179928029\n","ep  4  training loss:  0.5512522057730317\n","valid loss  0.5626508239649239  and accuracy  0.8628548580567773\n","ep  5  training loss:  0.5492460022789784\n","valid loss  0.5598552690558984  and accuracy  0.8632546981207517\n","ep  6  training loss:  0.5428700823521084\n","valid loss  0.5569956303119469  and accuracy  0.8632546981207517\n","ep  7  training loss:  0.5409472546234388\n","valid loss  0.5551454186534843  and accuracy  0.8632546981207517\n","ep  8  training loss:  0.5385068868618773\n","valid loss  0.5508538467890737  and accuracy  0.8628548580567773\n","ep  9  training loss:  0.5348387076090996\n","valid loss  0.5499232022679362  and accuracy  0.8632546981207517\n","ep  10  training loss:  0.5317289731756039\n","valid loss  0.543260400662752  and accuracy  0.8636545381847262\n","ep  11  training loss:  0.5308621011815122\n","valid loss  0.5513679266023617  and accuracy  0.8632546981207517\n","ep  12  training loss:  0.529061246262238\n","valid loss  0.541722518617942  and accuracy  0.8640543782487006\n","ep  13  training loss:  0.5259800460466493\n","valid loss  0.5448502166802194  and accuracy  0.8632546981207517\n","ep  14  training loss:  0.5252104804885164\n","valid loss  0.5402194977712269  and accuracy  0.8640543782487006\n","ep  15  training loss:  0.5234773982743871\n","valid loss  0.5404904548023663  and accuracy  0.8640543782487006\n","ep  16  training loss:  0.522045947657822\n","valid loss  0.5448617828888495  and accuracy  0.8660535785685726\n","ep  17  training loss:  0.5217880314930764\n","valid loss  0.5366720258593798  and accuracy  0.8648540583766493\n","ep  18  training loss:  0.5207045728339887\n","valid loss  0.5375465756056548  and accuracy  0.8656537385045981\n","ep  19  training loss:  0.5187700114196666\n","valid loss  0.5414072817108814  and accuracy  0.8632546981207517\n","ep  20  training loss:  0.5162259752194562\n","valid loss  0.5396915167057719  and accuracy  0.8620551779288285\n","ep  21  training loss:  0.5162813821835975\n","valid loss  0.5371123147649509  and accuracy  0.866453418632547\n","ep  22  training loss:  0.5152477169125228\n","valid loss  0.5312451215230766  and accuracy  0.8672530987604958\n","ep  23  training loss:  0.5152188328783278\n","valid loss  0.5393027587682426  and accuracy  0.8640543782487006\n","ep  24  training loss:  0.5150018823252156\n","valid loss  0.5295074145253398  and accuracy  0.8688524590163934\n","ep  25  training loss:  0.513749690896837\n","valid loss  0.5308113662017341  and accuracy  0.8680527788884447\n","ep  26  training loss:  0.5106982276779038\n","valid loss  0.5311439533154519  and accuracy  0.8680527788884447\n","ep  27  training loss:  0.5114526397279621\n","valid loss  0.5290726928270516  and accuracy  0.866453418632547\n","ep  28  training loss:  0.50959495288692\n","valid loss  0.5269222957808606  and accuracy  0.8660535785685726\n","ep  29  training loss:  0.5097268404468499\n","valid loss  0.5283562651589984  and accuracy  0.866453418632547\n","ep  30  training loss:  0.5095816934980876\n","valid loss  0.5266400128066754  and accuracy  0.8644542183126749\n","ep  31  training loss:  0.5078025848762969\n","valid loss  0.5271999670571682  and accuracy  0.8668532586965214\n","ep  32  training loss:  0.5063457343290687\n","valid loss  0.5269852929690131  and accuracy  0.8676529388244703\n","ep  33  training loss:  0.5061832863165558\n","valid loss  0.5294435517113956  and accuracy  0.8676529388244703\n","ep  34  training loss:  0.5054067133349527\n","valid loss  0.5245226279633944  and accuracy  0.868452618952419\n","ep  35  training loss:  0.5041456876378088\n","valid loss  0.5248628609516963  and accuracy  0.866453418632547\n","ep  36  training loss:  0.5058786592654654\n","valid loss  0.5241396343264757  and accuracy  0.8672530987604958\n","ep  37  training loss:  0.5041720291853221\n","valid loss  0.5204812639811095  and accuracy  0.8672530987604958\n","ep  38  training loss:  0.5031382418756155\n","valid loss  0.5254858004622629  and accuracy  0.8672530987604958\n","ep  39  training loss:  0.5015121737718232\n","valid loss  0.5230195014441504  and accuracy  0.868452618952419\n","ep  40  training loss:  0.5018805848580667\n","valid loss  0.5262246975322954  and accuracy  0.8656537385045981\n","ep  41  training loss:  0.49994402194802917\n","valid loss  0.5211828929669664  and accuracy  0.868452618952419\n","ep  42  training loss:  0.5007164138918212\n","valid loss  0.5244841016277891  and accuracy  0.8668532586965214\n","ep  43  training loss:  0.5003710861077055\n","valid loss  0.5185720707978405  and accuracy  0.8696521391443423\n","ep  44  training loss:  0.5013425608287131\n","valid loss  0.5221088947843714  and accuracy  0.8676529388244703\n","ep  45  training loss:  0.5009230371620633\n","valid loss  0.5232916184064628  and accuracy  0.8680527788884447\n","ep  46  training loss:  0.4990069802255006\n","valid loss  0.5205050994042919  and accuracy  0.8668532586965214\n","ep  47  training loss:  0.4982219536658223\n","valid loss  0.518449224862324  and accuracy  0.8692522990803678\n","ep  48  training loss:  0.4966397710998502\n","valid loss  0.5176610379445938  and accuracy  0.8680527788884447\n","ep  49  training loss:  0.49790663947122143\n","valid loss  0.5182759121197408  and accuracy  0.868452618952419\n","ep  50  training loss:  0.49574162907983604\n","valid loss  0.5203212629266378  and accuracy  0.8680527788884447\n","ep  51  training loss:  0.49424444380564936\n","valid loss  0.5199386345320156  and accuracy  0.8688524590163934\n","ep  52  training loss:  0.4951742945157122\n","valid loss  0.5202358599330653  and accuracy  0.8676529388244703\n","ep  53  training loss:  0.4958159393479244\n","valid loss  0.5202423085073908  and accuracy  0.8672530987604958\n","ep  54  training loss:  0.4953406051695509\n","valid loss  0.5156047605410046  and accuracy  0.8676529388244703\n","ep  55  training loss:  0.4948367180574269\n","valid loss  0.5176579104643352  and accuracy  0.8688524590163934\n","ep  56  training loss:  0.49456589020717845\n","valid loss  0.5162054174950198  and accuracy  0.8688524590163934\n","ep  57  training loss:  0.4934340449896953\n","valid loss  0.5192666518978957  and accuracy  0.8680527788884447\n","ep  58  training loss:  0.4921710043427952\n","valid loss  0.5162543148719897  and accuracy  0.8692522990803678\n","ep  59  training loss:  0.4917087902739807\n","valid loss  0.5142229328699848  and accuracy  0.8688524590163934\n","ep  60  training loss:  0.4924411514685119\n","valid loss  0.5179113022616653  and accuracy  0.8680527788884447\n","ep  61  training loss:  0.4928398750261564\n","valid loss  0.5102924766873227  and accuracy  0.8692522990803678\n","ep  62  training loss:  0.49289208951370417\n","valid loss  0.5151656703060505  and accuracy  0.868452618952419\n","ep  63  training loss:  0.4904553062991983\n","valid loss  0.5138081857105676  and accuracy  0.8704518192722911\n","ep  64  training loss:  0.49171884159051554\n","valid loss  0.5145776774682126  and accuracy  0.8704518192722911\n","ep  65  training loss:  0.49053504853009167\n","valid loss  0.5144949227821727  and accuracy  0.8676529388244703\n","ep  66  training loss:  0.4911827597592543\n","valid loss  0.5115094957757788  and accuracy  0.8680527788884447\n","ep  67  training loss:  0.49286168362283694\n","valid loss  0.5162560640931463  and accuracy  0.8668532586965214\n","ep  68  training loss:  0.48950979415483475\n","valid loss  0.515734470901085  and accuracy  0.8676529388244703\n","ep  69  training loss:  0.491098055634508\n","valid loss  0.5136449465938493  and accuracy  0.8688524590163934\n","ep  70  training loss:  0.4886920533815647\n","valid loss  0.510048336384059  and accuracy  0.8688524590163934\n","ep  71  training loss:  0.4898408514914478\n","valid loss  0.5128664344084449  and accuracy  0.8688524590163934\n","ep  72  training loss:  0.489248724343005\n","valid loss  0.5122957190052598  and accuracy  0.8688524590163934\n","ep  73  training loss:  0.48803940672107443\n","valid loss  0.5135928627635326  and accuracy  0.8688524590163934\n","ep  74  training loss:  0.48808374384289344\n","valid loss  0.5097701376388188  and accuracy  0.8668532586965214\n","ep  75  training loss:  0.4868840278205902\n","valid loss  0.5115085154330907  and accuracy  0.868452618952419\n","ep  76  training loss:  0.48774179362952197\n","valid loss  0.5123673013833369  and accuracy  0.8716513394642144\n","ep  77  training loss:  0.48793480937327866\n","valid loss  0.5138984551099909  and accuracy  0.8692522990803678\n","ep  78  training loss:  0.48684852153538327\n","valid loss  0.5098366834244124  and accuracy  0.8704518192722911\n","ep  79  training loss:  0.48585665678278056\n","valid loss  0.5037443843282542  and accuracy  0.8724510195921631\n","ep  80  training loss:  0.4851154490223911\n","valid loss  0.508562859691462  and accuracy  0.8712514994002399\n","ep  81  training loss:  0.48528355036535087\n","valid loss  0.5093369524701029  and accuracy  0.8700519792083167\n","ep  82  training loss:  0.4849189205272318\n","valid loss  0.5073323320837223  and accuracy  0.8708516593362655\n","ep  83  training loss:  0.48311210165890855\n","valid loss  0.5095636093201803  and accuracy  0.8696521391443423\n","ep  84  training loss:  0.4861024882636771\n","valid loss  0.5127982612802047  and accuracy  0.8704518192722911\n","ep  85  training loss:  0.4829199072244234\n","valid loss  0.5134806410163367  and accuracy  0.8716513394642144\n","ep  86  training loss:  0.4844664958504917\n","valid loss  0.5109036793617285  and accuracy  0.8704518192722911\n","ep  87  training loss:  0.4836990069896713\n","valid loss  0.5087422817075601  and accuracy  0.8716513394642144\n","ep  88  training loss:  0.48437670497639884\n","valid loss  0.5112843411724742  and accuracy  0.8708516593362655\n","ep  89  training loss:  0.48080150327084825\n","valid loss  0.5170584244043624  and accuracy  0.8688524590163934\n","ep  90  training loss:  0.4830205737820592\n","valid loss  0.5135594084233296  and accuracy  0.8704518192722911\n","ep  91  training loss:  0.4836352263642064\n","valid loss  0.5097391844415416  and accuracy  0.8716513394642144\n","ep  92  training loss:  0.4823457500139739\n","valid loss  0.5171064394609969  and accuracy  0.8692522990803678\n","ep  93  training loss:  0.48235811843829735\n","valid loss  0.5123633632227117  and accuracy  0.8708516593362655\n","ep  94  training loss:  0.4824100085012507\n","valid loss  0.5099809491029028  and accuracy  0.8712514994002399\n","ep  95  training loss:  0.4821592031988328\n","valid loss  0.5059029495225149  and accuracy  0.8712514994002399\n","ep  96  training loss:  0.48178798267552686\n","valid loss  0.508902165375915  and accuracy  0.8704518192722911\n","ep  97  training loss:  0.4795085514399009\n","valid loss  0.506974583289853  and accuracy  0.8728508596561375\n","ep  98  training loss:  0.4810955174115341\n","valid loss  0.512575038811151  and accuracy  0.8720511795281887\n","ep  99  training loss:  0.48098173721800436\n","valid loss  0.5080958406289355  and accuracy  0.8712514994002399\n","ep  100  training loss:  0.47854207499731455\n","valid loss  0.5064864689018763  and accuracy  0.8708516593362655\n","ep  101  training loss:  0.4826379166365698\n","valid loss  0.5049745175705963  and accuracy  0.8724510195921631\n","ep  102  training loss:  0.4804186627672537\n","valid loss  0.5101623273477321  and accuracy  0.8708516593362655\n","ep  103  training loss:  0.4818067986280546\n","valid loss  0.5093191190749728  and accuracy  0.8716513394642144\n","ep  104  training loss:  0.48183075754360627\n","valid loss  0.5075542167609618  and accuracy  0.8704518192722911\n","ep  105  training loss:  0.48077996283141833\n","valid loss  0.5039379594565296  and accuracy  0.8712514994002399\n","ep  106  training loss:  0.480604446734549\n","valid loss  0.5058708687106975  and accuracy  0.8712514994002399\n","ep  107  training loss:  0.47898252208711434\n","valid loss  0.5075268255191439  and accuracy  0.8720511795281887\n","ep  108  training loss:  0.479477105042489\n","valid loss  0.5050028903467185  and accuracy  0.8716513394642144\n","ep  109  training loss:  0.4795316837282156\n","valid loss  0.5119465011351111  and accuracy  0.8720511795281887\n","ep  110  training loss:  0.47878023238082024\n","valid loss  0.5100607699463244  and accuracy  0.8704518192722911\n","ep  111  training loss:  0.4793297844724073\n","valid loss  0.5141272633874574  and accuracy  0.8704518192722911\n","ep  112  training loss:  0.4796899310011097\n","valid loss  0.5126838849120882  and accuracy  0.8700519792083167\n","ep  113  training loss:  0.4797370330568204\n","valid loss  0.5076325002883444  and accuracy  0.8704518192722911\n","ep  114  training loss:  0.4816692976043214\n","valid loss  0.508616955696893  and accuracy  0.8720511795281887\n","ep  115  training loss:  0.4777449564876067\n","valid loss  0.5110435978096516  and accuracy  0.8716513394642144\n","ep  116  training loss:  0.4773661891186705\n","valid loss  0.5082505857739531  and accuracy  0.8716513394642144\n","ep  117  training loss:  0.47874165572754285\n","valid loss  0.5055695627032162  and accuracy  0.8712514994002399\n","ep  118  training loss:  0.4787643694079867\n","valid loss  0.5123234669335696  and accuracy  0.8716513394642144\n","ep  119  training loss:  0.47559733886960176\n","valid loss  0.5081564273561586  and accuracy  0.8704518192722911\n","ep  120  training loss:  0.47664265163310526\n","valid loss  0.5149288343124893  and accuracy  0.8704518192722911\n","ep  121  training loss:  0.47852201663278104\n","valid loss  0.5095576244275697  and accuracy  0.8720511795281887\n","ep  122  training loss:  0.4754657722172453\n","valid loss  0.5104234628036756  and accuracy  0.8708516593362655\n","ep  123  training loss:  0.4757040338296278\n","valid loss  0.5081991657096355  and accuracy  0.8700519792083167\n","ep  124  training loss:  0.4765098181904708\n","valid loss  0.507041159985972  and accuracy  0.8724510195921631\n","ep  125  training loss:  0.47526655558480646\n","valid loss  0.5058607538310779  and accuracy  0.8712514994002399\n","ep  126  training loss:  0.4764393972408992\n","valid loss  0.5065135662434627  and accuracy  0.8716513394642144\n","ep  127  training loss:  0.47738527344311443\n","valid loss  0.5076265543139205  and accuracy  0.8712514994002399\n","ep  128  training loss:  0.4761006333824078\n","valid loss  0.5072674847564331  and accuracy  0.8724510195921631\n","ep  129  training loss:  0.47690990073476497\n","valid loss  0.5122994040546776  and accuracy  0.8700519792083167\n","ep  130  training loss:  0.47797147932351747\n","valid loss  0.5015434437110776  and accuracy  0.8712514994002399\n","ep  131  training loss:  0.4761960812075573\n","valid loss  0.5085105103332965  and accuracy  0.8720511795281887\n","ep  132  training loss:  0.4753837545698644\n","valid loss  0.5248908194505134  and accuracy  0.8692522990803678\n","ep  133  training loss:  0.4760620916447371\n","valid loss  0.5097683266800244  and accuracy  0.8708516593362655\n","ep  134  training loss:  0.47765283755278826\n","valid loss  0.5063652443985899  and accuracy  0.8712514994002399\n","ep  135  training loss:  0.4758960182026077\n","valid loss  0.5044532844968816  and accuracy  0.8712514994002399\n","ep  136  training loss:  0.4741632812406266\n","valid loss  0.5146887799540981  and accuracy  0.8712514994002399\n","ep  137  training loss:  0.4724802461067952\n","valid loss  0.5031137895412514  and accuracy  0.8732506997201119\n","ep  138  training loss:  0.4747670034968557\n","valid loss  0.5098316367151069  and accuracy  0.8708516593362655\n","ep  139  training loss:  0.4736424550583197\n","valid loss  0.5030611009656882  and accuracy  0.8716513394642144\n","ep  140  training loss:  0.4743775217106476\n","valid loss  0.5052086547201798  and accuracy  0.8712514994002399\n","ep  141  training loss:  0.47630479395916236\n","valid loss  0.5044087261164107  and accuracy  0.8712514994002399\n","ep  142  training loss:  0.4742785383745516\n","valid loss  0.5069345406464032  and accuracy  0.8696521391443423\n","ep  143  training loss:  0.47439162771557736\n","valid loss  0.4999303623038928  and accuracy  0.8704518192722911\n","ep  144  training loss:  0.4748785685599811\n","valid loss  0.5036921703972753  and accuracy  0.8716513394642144\n","ep  145  training loss:  0.472875504828392\n","valid loss  0.5008849580971444  and accuracy  0.8720511795281887\n","ep  146  training loss:  0.4734839305198643\n","valid loss  0.5077865319412167  and accuracy  0.8716513394642144\n","ep  147  training loss:  0.4739419468119444\n","valid loss  0.5061167740002006  and accuracy  0.8696521391443423\n","ep  148  training loss:  0.4737760318579139\n","valid loss  0.5071214480263765  and accuracy  0.8704518192722911\n","ep  149  training loss:  0.47496047730701263\n","valid loss  0.5073304647018985  and accuracy  0.8716513394642144\n","ep  150  training loss:  0.47173408696867025\n","valid loss  0.5068677909037724  and accuracy  0.8720511795281887\n","ep  151  training loss:  0.4736325355452046\n","valid loss  0.5063970160455715  and accuracy  0.8704518192722911\n","ep  152  training loss:  0.47199152417206686\n","valid loss  0.5035546033776126  and accuracy  0.8716513394642144\n","ep  153  training loss:  0.47429840528484246\n","valid loss  0.5027251774813832  and accuracy  0.8712514994002399\n","ep  154  training loss:  0.4718143265322927\n","valid loss  0.507261692691164  and accuracy  0.8704518192722911\n","ep  155  training loss:  0.4727834657441781\n","valid loss  0.5065803655573293  and accuracy  0.8704518192722911\n","ep  156  training loss:  0.4717410089327735\n","valid loss  0.5106719971322765  and accuracy  0.8708516593362655\n","ep  157  training loss:  0.47276429165457506\n","valid loss  0.5020932344282594  and accuracy  0.8732506997201119\n","ep  158  training loss:  0.47214942642120694\n","valid loss  0.5033051238113382  and accuracy  0.8724510195921631\n","ep  159  training loss:  0.4731865081262327\n","valid loss  0.5046070639680071  and accuracy  0.8696521391443423\n","ep  160  training loss:  0.4717092771636785\n","valid loss  0.5071787170675554  and accuracy  0.8700519792083167\n","ep  161  training loss:  0.47259297790107596\n","valid loss  0.5018978430504133  and accuracy  0.8728508596561375\n","ep  162  training loss:  0.471142993694604\n","valid loss  0.5019342282065293  and accuracy  0.8708516593362655\n","ep  163  training loss:  0.47046997027309495\n","valid loss  0.5047784032153397  and accuracy  0.8708516593362655\n","ep  164  training loss:  0.4712673609687773\n","valid loss  0.5279338929901978  and accuracy  0.8716513394642144\n","ep  165  training loss:  0.4721390854384293\n","valid loss  0.4995862104353167  and accuracy  0.8720511795281887\n","ep  166  training loss:  0.4735719243447442\n","valid loss  0.49728461091921644  and accuracy  0.8728508596561375\n","ep  167  training loss:  0.4705489852769006\n","valid loss  0.5015795749456871  and accuracy  0.8720511795281887\n","ep  168  training loss:  0.4700405397099253\n","valid loss  0.5018537639379979  and accuracy  0.8712514994002399\n","ep  169  training loss:  0.47005408383767666\n","valid loss  0.5005821654768955  and accuracy  0.8712514994002399\n","ep  170  training loss:  0.47103444073032386\n","valid loss  0.5028202629051224  and accuracy  0.8704518192722911\n","ep  171  training loss:  0.4718354496293803\n","valid loss  0.5000160111993944  and accuracy  0.8728508596561375\n","ep  172  training loss:  0.47069172211151444\n","valid loss  0.5009151772230637  and accuracy  0.8724510195921631\n","ep  173  training loss:  0.47127771810514\n","valid loss  0.497201029048925  and accuracy  0.8720511795281887\n","ep  174  training loss:  0.4684260684986391\n","valid loss  0.5047887474810873  and accuracy  0.8720511795281887\n","ep  175  training loss:  0.46796409541066203\n","valid loss  0.501075813909475  and accuracy  0.8712514994002399\n","ep  176  training loss:  0.47050213343015124\n","valid loss  0.5034724063798934  and accuracy  0.8700519792083167\n","ep  177  training loss:  0.4710114278347629\n","valid loss  0.4999097417183563  and accuracy  0.8716513394642144\n","ep  178  training loss:  0.4704504497831324\n","valid loss  0.5090827786031126  and accuracy  0.8708516593362655\n","ep  179  training loss:  0.4696114261204576\n","valid loss  0.5034609383842746  and accuracy  0.8720511795281887\n","ep  180  training loss:  0.46660126461206536\n","valid loss  0.5077239806677808  and accuracy  0.8704518192722911\n","ep  181  training loss:  0.46938370480909436\n","valid loss  0.4958530468542258  and accuracy  0.8732506997201119\n","ep  182  training loss:  0.4709855568716487\n","valid loss  0.5000135911697866  and accuracy  0.8728508596561375\n","ep  183  training loss:  0.47002110219569887\n","valid loss  0.5057425080943422  and accuracy  0.8708516593362655\n","ep  184  training loss:  0.4716014948338638\n","valid loss  0.49630009408285025  and accuracy  0.8724510195921631\n","ep  185  training loss:  0.4700314463747472\n","valid loss  0.49877495114586917  and accuracy  0.8712514994002399\n","ep  186  training loss:  0.4705914173015793\n","valid loss  0.49940920145403905  and accuracy  0.8724510195921631\n","ep  187  training loss:  0.4688693283291048\n","valid loss  0.508714274376309  and accuracy  0.8712514994002399\n","ep  188  training loss:  0.4702988541716831\n","valid loss  0.49867862356943593  and accuracy  0.8732506997201119\n","ep  189  training loss:  0.46924618547139163\n","valid loss  0.5014410642493682  and accuracy  0.8720511795281887\n","ep  190  training loss:  0.4704567673388146\n","valid loss  0.5012681325141642  and accuracy  0.8732506997201119\n","ep  191  training loss:  0.46809643074427676\n","valid loss  0.49758745587525105  and accuracy  0.8724510195921631\n","ep  192  training loss:  0.4692369920683544\n","valid loss  0.5010713642714072  and accuracy  0.8728508596561375\n","ep  193  training loss:  0.46872407195331356\n","valid loss  0.4980498122387245  and accuracy  0.8708516593362655\n","ep  194  training loss:  0.4686548080125433\n","valid loss  0.4955348234708573  and accuracy  0.8724510195921631\n","ep  195  training loss:  0.46830034483751615\n","valid loss  0.5017427865837346  and accuracy  0.8712514994002399\n","ep  196  training loss:  0.47187332051838404\n","valid loss  0.49773320456782805  and accuracy  0.8708516593362655\n","ep  197  training loss:  0.4688231898360558\n","valid loss  0.4995084091311977  and accuracy  0.8724510195921631\n","ep  198  training loss:  0.4697312549398724\n","valid loss  0.5020161442926339  and accuracy  0.8716513394642144\n","ep  199  training loss:  0.4689469458075483\n","valid loss  0.49967741580163894  and accuracy  0.8712514994002399\n","ep  200  training loss:  0.4678234169681276\n","valid loss  0.5042620943170316  and accuracy  0.8708516593362655\n","ep  201  training loss:  0.4676741475865062\n","valid loss  0.5042197902719291  and accuracy  0.8712514994002399\n","ep  202  training loss:  0.4660710586158718\n","valid loss  0.5050015854911774  and accuracy  0.8728508596561375\n","ep  203  training loss:  0.4691897611171937\n","valid loss  0.5050077337782081  and accuracy  0.8720511795281887\n","ep  204  training loss:  0.4681770263682099\n","valid loss  0.5050802957244226  and accuracy  0.8704518192722911\n","ep  205  training loss:  0.465634616977421\n","valid loss  0.5058223034967569  and accuracy  0.8700519792083167\n","ep  206  training loss:  0.4664328574055957\n","valid loss  0.49734669025303696  and accuracy  0.8728508596561375\n","ep  207  training loss:  0.4667523152250417\n","valid loss  0.5016140792666317  and accuracy  0.8716513394642144\n","ep  208  training loss:  0.4678292094555386\n","valid loss  0.4967422077103836  and accuracy  0.8708516593362655\n","ep  209  training loss:  0.4669391967837018\n","valid loss  0.5103916138207993  and accuracy  0.8712514994002399\n","ep  210  training loss:  0.46731398391825474\n","valid loss  0.5053193032241068  and accuracy  0.8712514994002399\n","ep  211  training loss:  0.4685014740404529\n","valid loss  0.5021098934140791  and accuracy  0.8724510195921631\n","ep  212  training loss:  0.46811315035759105\n","valid loss  0.5041669198795588  and accuracy  0.8724510195921631\n","ep  213  training loss:  0.4681966784968489\n","valid loss  0.501155928414805  and accuracy  0.8724510195921631\n","ep  214  training loss:  0.46859227452422314\n","valid loss  0.49938894295301595  and accuracy  0.8716513394642144\n","ep  215  training loss:  0.4667663189567979\n","valid loss  0.4985779689245823  and accuracy  0.8728508596561375\n","ep  216  training loss:  0.46737803503733055\n","valid loss  0.5000148870167089  and accuracy  0.8716513394642144\n","ep  217  training loss:  0.46659264726162336\n","valid loss  0.5031444496009313  and accuracy  0.8728508596561375\n","ep  218  training loss:  0.46579176989640664\n","valid loss  0.5021118248095278  and accuracy  0.8736505397840864\n","ep  219  training loss:  0.4683359937417936\n","valid loss  0.5032607660871274  and accuracy  0.8732506997201119\n","ep  220  training loss:  0.46723941995538537\n","valid loss  0.5079031443653084  and accuracy  0.8708516593362655\n","ep  221  training loss:  0.46586406150801335\n","valid loss  0.504203981134902  and accuracy  0.8732506997201119\n","ep  222  training loss:  0.466500353755012\n","valid loss  0.49923298681606915  and accuracy  0.8736505397840864\n","ep  223  training loss:  0.46671515853217593\n","valid loss  0.5050567964442679  and accuracy  0.8700519792083167\n","ep  224  training loss:  0.46718555205860474\n","valid loss  0.5017405806160697  and accuracy  0.8712514994002399\n","ep  225  training loss:  0.46718625679572845\n","valid loss  0.5009709974852908  and accuracy  0.8724510195921631\n","ep  226  training loss:  0.46615297680818896\n","valid loss  0.5003437175602019  and accuracy  0.8716513394642144\n","ep  227  training loss:  0.464708809553516\n","valid loss  0.5034761318370181  and accuracy  0.8720511795281887\n","ep  228  training loss:  0.4660553324811148\n","valid loss  0.5058597698777926  and accuracy  0.8708516593362655\n","ep  229  training loss:  0.46730841835732584\n","valid loss  0.5018658024079797  and accuracy  0.8732506997201119\n","ep  230  training loss:  0.4677495826810313\n","valid loss  0.5085401707818535  and accuracy  0.8716513394642144\n","ep  231  training loss:  0.4658016304838193\n","valid loss  0.5572971770735752  and accuracy  0.8712514994002399\n","ep  232  training loss:  0.46664557637713644\n","valid loss  0.5630965237615586  and accuracy  0.8716513394642144\n","ep  233  training loss:  0.4648744110483696\n","valid loss  0.5234249026977458  and accuracy  0.8712514994002399\n","ep  234  training loss:  0.46448253318489563\n","valid loss  0.5014039737160136  and accuracy  0.8724510195921631\n","ep  235  training loss:  0.46441206110248145\n","valid loss  0.4988368589226983  and accuracy  0.8708516593362655\n","ep  236  training loss:  0.4667596781047636\n","valid loss  0.49926144113449133  and accuracy  0.8724510195921631\n","ep  237  training loss:  0.46541116230551427\n","valid loss  0.503052911297029  and accuracy  0.8708516593362655\n","ep  238  training loss:  0.46457115509630953\n","valid loss  0.4968379496431789  and accuracy  0.8708516593362655\n","ep  239  training loss:  0.4648043699208414\n","valid loss  0.5105307469340336  and accuracy  0.8708516593362655\n","ep  240  training loss:  0.4642699537622084\n","valid loss  0.5031104328774395  and accuracy  0.8740503798480608\n","ep  241  training loss:  0.4662074177584869\n","valid loss  0.5013076325074904  and accuracy  0.8728508596561375\n","ep  242  training loss:  0.4637513502273454\n","valid loss  0.5031088973130764  and accuracy  0.8728508596561375\n","ep  243  training loss:  0.46525882893588316\n","valid loss  0.5015371332927401  and accuracy  0.8704518192722911\n","ep  244  training loss:  0.4662994238753091\n","valid loss  0.5033385896148895  and accuracy  0.8720511795281887\n","ep  245  training loss:  0.46474822147432004\n","valid loss  0.4949276411547655  and accuracy  0.8724510195921631\n","ep  246  training loss:  0.46542551364824397\n","valid loss  0.501259324706492  and accuracy  0.8716513394642144\n","ep  247  training loss:  0.4633793606920465\n","valid loss  0.5282706012848805  and accuracy  0.8716513394642144\n","ep  248  training loss:  0.464671968018431\n","valid loss  0.539257741627432  and accuracy  0.8720511795281887\n","ep  249  training loss:  0.4655140201943799\n","valid loss  0.5161846731482197  and accuracy  0.8696521391443423\n","ep  250  training loss:  0.46396249089639807\n","valid loss  0.5021363686485703  and accuracy  0.8724510195921631\n","ep  251  training loss:  0.46575917777760933\n","valid loss  0.5060540824735322  and accuracy  0.8708516593362655\n","ep  252  training loss:  0.46482821337875896\n","valid loss  0.5149775762312033  and accuracy  0.8708516593362655\n","ep  253  training loss:  0.4637696219377219\n","valid loss  0.5212052458288764  and accuracy  0.8716513394642144\n","ep  254  training loss:  0.4652125372290187\n","valid loss  0.5036978212917675  and accuracy  0.8712514994002399\n","ep  255  training loss:  0.4635334234675189\n","valid loss  0.5212610097991519  and accuracy  0.8704518192722911\n","ep  256  training loss:  0.46750162606702794\n","valid loss  0.5025488024995309  and accuracy  0.8716513394642144\n","ep  257  training loss:  0.4630917422615397\n","valid loss  0.514858351057408  and accuracy  0.8696521391443423\n","ep  258  training loss:  0.46433683135680326\n","valid loss  0.5065092638653309  and accuracy  0.8720511795281887\n","ep  259  training loss:  0.4654327456992777\n","valid loss  0.5828804694285921  and accuracy  0.8716513394642144\n","ep  260  training loss:  0.4663335887327621\n","valid loss  0.4994542235043086  and accuracy  0.8720511795281887\n","ep  261  training loss:  0.46315114491598613\n","valid loss  0.5080821599687685  and accuracy  0.8704518192722911\n","ep  262  training loss:  0.4658926166583108\n","valid loss  0.5026314172779069  and accuracy  0.8716513394642144\n","ep  263  training loss:  0.46226534909117517\n","valid loss  0.5136975275188005  and accuracy  0.8704518192722911\n","ep  264  training loss:  0.46546651123256866\n","valid loss  0.5006964525333741  and accuracy  0.8728508596561375\n","ep  265  training loss:  0.46414356311015875\n","valid loss  0.502407465706535  and accuracy  0.8716513394642144\n","ep  266  training loss:  0.46236598614976604\n","valid loss  0.5188324000658107  and accuracy  0.8704518192722911\n","ep  267  training loss:  0.46450208477141436\n","valid loss  0.5077290796890396  and accuracy  0.8692522990803678\n","ep  268  training loss:  0.46328901151395874\n","valid loss  0.5132876770799516  and accuracy  0.8732506997201119\n","ep  269  training loss:  0.46472162417647794\n","valid loss  0.5011097621865294  and accuracy  0.8728508596561375\n","ep  270  training loss:  0.46524195687755565\n","valid loss  0.5025932653481271  and accuracy  0.8728508596561375\n","ep  271  training loss:  0.4634969163414741\n","valid loss  0.5027500518271085  and accuracy  0.8724510195921631\n","ep  272  training loss:  0.4632514694576822\n","valid loss  0.502204993494698  and accuracy  0.8732506997201119\n","ep  273  training loss:  0.46242522822686827\n","valid loss  0.5040329102514649  and accuracy  0.8712514994002399\n","ep  274  training loss:  0.4619264046627351\n","valid loss  0.5060167209428104  and accuracy  0.8716513394642144\n","ep  275  training loss:  0.4632712804725499\n","valid loss  0.5181310041481759  and accuracy  0.8696521391443423\n","ep  276  training loss:  0.4622773585309934\n","valid loss  0.5057417256076161  and accuracy  0.8724510195921631\n","ep  277  training loss:  0.4614098380122074\n","valid loss  0.5100330653213492  and accuracy  0.8716513394642144\n","ep  278  training loss:  0.4650613997463725\n","valid loss  0.5010434999221899  and accuracy  0.8724510195921631\n","ep  279  training loss:  0.46300375459706283\n","valid loss  0.5038006570710034  and accuracy  0.8720511795281887\n","ep  280  training loss:  0.46342334883876707\n","valid loss  0.5123894820447827  and accuracy  0.8704518192722911\n","ep  281  training loss:  0.46160065055322824\n","valid loss  0.5100338201268297  and accuracy  0.8712514994002399\n","ep  282  training loss:  0.46361531091148744\n","valid loss  0.49878631387410854  and accuracy  0.8716513394642144\n","ep  283  training loss:  0.46439745999759424\n","valid loss  0.5012259785769606  and accuracy  0.8712514994002399\n","ep  284  training loss:  0.4637477307035284\n","valid loss  0.4980846892495672  and accuracy  0.8716513394642144\n","ep  285  training loss:  0.46259152085004607\n","valid loss  0.5045802771496992  and accuracy  0.8708516593362655\n","ep  286  training loss:  0.46287663322461753\n","valid loss  0.49148100812356027  and accuracy  0.8720511795281887\n","ep  287  training loss:  0.4613600024748569\n","valid loss  0.5053783290174569  and accuracy  0.8720511795281887\n","ep  288  training loss:  0.46114430423208036\n","valid loss  0.4983103649228251  and accuracy  0.8728508596561375\n","ep  289  training loss:  0.4638625714960517\n","valid loss  0.49804409302887276  and accuracy  0.8732506997201119\n","ep  290  training loss:  0.464775690871061\n","valid loss  0.497651514030847  and accuracy  0.8728508596561375\n","ep  291  training loss:  0.4636535493516509\n","valid loss  0.4988343128439237  and accuracy  0.8728508596561375\n","ep  292  training loss:  0.4610809706389552\n","valid loss  0.5085517910898614  and accuracy  0.8712514994002399\n","ep  293  training loss:  0.46185720950730746\n","valid loss  0.49707300648218344  and accuracy  0.8720511795281887\n","ep  294  training loss:  0.46107313476983086\n","valid loss  0.5046080434646477  and accuracy  0.8716513394642144\n","ep  295  training loss:  0.4630173069981755\n","valid loss  0.5031362133758253  and accuracy  0.8728508596561375\n","ep  296  training loss:  0.46158731613253434\n","valid loss  0.49914455485315334  and accuracy  0.8712514994002399\n","ep  297  training loss:  0.46347960477283867\n","valid loss  0.4988935138406109  and accuracy  0.8728508596561375\n","ep  298  training loss:  0.46367478669950263\n","valid loss  0.5033079589047178  and accuracy  0.8724510195921631\n","ep  299  training loss:  0.4628221602155124\n","valid loss  0.49642291222987583  and accuracy  0.8720511795281887\n","ep  300  training loss:  0.4624805839408497\n","valid loss  0.49984711829255646  and accuracy  0.8712514994002399\n","ep  301  training loss:  0.46442477927195364\n","valid loss  0.5002601986072484  and accuracy  0.8732506997201119\n","ep  302  training loss:  0.46286531537615216\n","valid loss  0.498492864049563  and accuracy  0.8740503798480608\n","ep  303  training loss:  0.4617492713190472\n","valid loss  0.5016554351426848  and accuracy  0.8720511795281887\n","ep  304  training loss:  0.4600817608276181\n","valid loss  0.5021548840771767  and accuracy  0.8744502199120352\n","ep  305  training loss:  0.46375878807402987\n","valid loss  0.5014226959972847  and accuracy  0.8716513394642144\n","ep  306  training loss:  0.4588866157007854\n","valid loss  0.5078732202239915  and accuracy  0.8720511795281887\n","ep  307  training loss:  0.46163174526601275\n","valid loss  0.5042886487820872  and accuracy  0.8720511795281887\n","ep  308  training loss:  0.46404676832961667\n","valid loss  0.4993433956740523  and accuracy  0.8720511795281887\n","ep  309  training loss:  0.4604334501888371\n","valid loss  0.5040793598460847  and accuracy  0.8720511795281887\n","ep  310  training loss:  0.4613979459682538\n","valid loss  0.5002034055929286  and accuracy  0.8748500599760096\n","ep  311  training loss:  0.4614041573792937\n","valid loss  0.5019064421703319  and accuracy  0.8724510195921631\n","ep  312  training loss:  0.45949844172129783\n","valid loss  0.5020142198085594  and accuracy  0.8728508596561375\n","ep  313  training loss:  0.46066937610488917\n","valid loss  0.49928057749049276  and accuracy  0.8716513394642144\n","ep  314  training loss:  0.4622359113970006\n","valid loss  0.4975287183696201  and accuracy  0.8716513394642144\n","ep  315  training loss:  0.4624814338992402\n","valid loss  0.5021994474266873  and accuracy  0.8716513394642144\n","ep  316  training loss:  0.46409868150406575\n","valid loss  0.49986690985875243  and accuracy  0.8724510195921631\n","ep  317  training loss:  0.46227108647443027\n","valid loss  0.497093295893732  and accuracy  0.8732506997201119\n","ep  318  training loss:  0.4603196487658796\n","valid loss  0.5030261984065931  and accuracy  0.8720511795281887\n","ep  319  training loss:  0.46220503735544294\n","valid loss  0.502931784577772  and accuracy  0.8728508596561375\n","ep  320  training loss:  0.46069881787521366\n","valid loss  0.4977538247720474  and accuracy  0.8732506997201119\n","ep  321  training loss:  0.4618727635988324\n","valid loss  0.49745140297085894  and accuracy  0.8724510195921631\n","ep  322  training loss:  0.461254213624496\n","valid loss  0.5016711296819774  and accuracy  0.8716513394642144\n","ep  323  training loss:  0.46039654377482386\n","valid loss  0.5004631592625859  and accuracy  0.8716513394642144\n","ep  324  training loss:  0.46162431520095565\n","valid loss  0.49902644873809354  and accuracy  0.8732506997201119\n","ep  325  training loss:  0.46057537638505575\n","valid loss  0.4989061825802592  and accuracy  0.8732506997201119\n","ep  326  training loss:  0.4621609507770643\n","valid loss  0.49517125270215095  and accuracy  0.8736505397840864\n","ep  327  training loss:  0.46284947845024144\n","valid loss  0.49552787483477295  and accuracy  0.8728508596561375\n","ep  328  training loss:  0.460192105684145\n","valid loss  0.5018136302074019  and accuracy  0.8724510195921631\n","ep  329  training loss:  0.45900563291830604\n","valid loss  0.49678172349262506  and accuracy  0.8740503798480608\n","ep  330  training loss:  0.4604855269646694\n","valid loss  0.4982494493452657  and accuracy  0.8728508596561375\n","ep  331  training loss:  0.4621845217694429\n","valid loss  0.4997692109226751  and accuracy  0.8728508596561375\n","ep  332  training loss:  0.4594957099507425\n","valid loss  0.5067459663502076  and accuracy  0.8708516593362655\n","ep  333  training loss:  0.46031128836514795\n","valid loss  0.5006468536519184  and accuracy  0.8724510195921631\n","ep  334  training loss:  0.459853571602695\n","valid loss  0.5052030466285432  and accuracy  0.8720511795281887\n","ep  335  training loss:  0.4604360690240789\n","valid loss  0.5020187691372425  and accuracy  0.8724510195921631\n","ep  336  training loss:  0.46054242007736085\n","valid loss  0.501191281095022  and accuracy  0.8724510195921631\n","ep  337  training loss:  0.45928370959345705\n","valid loss  0.5004339229579164  and accuracy  0.8732506997201119\n","ep  338  training loss:  0.45999914535031783\n","valid loss  0.4942118394903925  and accuracy  0.8724510195921631\n","ep  339  training loss:  0.4615524359407165\n","valid loss  0.5063710517046309  and accuracy  0.8728508596561375\n","ep  340  training loss:  0.460126185647816\n","valid loss  0.5021418468659518  and accuracy  0.8720511795281887\n","ep  341  training loss:  0.45962596263881106\n","valid loss  0.50099217634304  and accuracy  0.8716513394642144\n","ep  342  training loss:  0.4607713051445146\n","valid loss  0.4994472609620626  and accuracy  0.8720511795281887\n","ep  343  training loss:  0.46018782737037534\n","valid loss  0.49791021861347473  and accuracy  0.8720511795281887\n","ep  344  training loss:  0.45790637216404645\n","valid loss  0.4989058892758357  and accuracy  0.8728508596561375\n","ep  345  training loss:  0.4579249406396067\n","valid loss  0.4963439308443531  and accuracy  0.8724510195921631\n","ep  346  training loss:  0.4602679782384224\n","valid loss  0.4963225689900965  and accuracy  0.8732506997201119\n","ep  347  training loss:  0.45885993224341715\n","valid loss  0.49721199431356455  and accuracy  0.8716513394642144\n","ep  348  training loss:  0.4618590710298306\n","valid loss  0.49976925821792406  and accuracy  0.8720511795281887\n","ep  349  training loss:  0.4608012555911466\n","valid loss  0.5012542207876904  and accuracy  0.8716513394642144\n","ep  350  training loss:  0.46096413292109634\n","valid loss  0.5020517454963358  and accuracy  0.8720511795281887\n","ep  351  training loss:  0.4610705717027105\n","valid loss  0.5002128495926  and accuracy  0.8728508596561375\n","ep  352  training loss:  0.45909082542597396\n","valid loss  0.5033294463434109  and accuracy  0.8708516593362655\n","ep  353  training loss:  0.45906025884044527\n","valid loss  0.49901417699731476  and accuracy  0.8720511795281887\n","ep  354  training loss:  0.459813206420681\n","valid loss  0.49699440030086905  and accuracy  0.8716513394642144\n","ep  355  training loss:  0.45904488396597665\n","valid loss  0.4959950984501448  and accuracy  0.8724510195921631\n","ep  356  training loss:  0.4581442569334946\n","valid loss  0.4959717493851344  and accuracy  0.8724510195921631\n","ep  357  training loss:  0.45960866962501995\n","valid loss  0.5013069807410669  and accuracy  0.8724510195921631\n","ep  358  training loss:  0.4631112940140498\n","valid loss  0.49557904944997555  and accuracy  0.8728508596561375\n","ep  359  training loss:  0.46268932706492294\n","valid loss  0.4990093266782833  and accuracy  0.8716513394642144\n","ep  360  training loss:  0.45958334202043344\n","valid loss  0.49588853304312736  and accuracy  0.8732506997201119\n","ep  361  training loss:  0.46045289249185334\n","valid loss  0.5040433705925512  and accuracy  0.8740503798480608\n","ep  362  training loss:  0.4602629053599671\n","valid loss  0.4992518569650006  and accuracy  0.8712514994002399\n","ep  363  training loss:  0.4604098422003429\n","valid loss  0.4984555529480408  and accuracy  0.8744502199120352\n","ep  364  training loss:  0.4585230967383363\n","valid loss  0.49677670376151145  and accuracy  0.8716513394642144\n","ep  365  training loss:  0.4595380485933966\n","valid loss  0.49732707458131553  and accuracy  0.8732506997201119\n","ep  366  training loss:  0.45867897108414996\n","valid loss  0.5033550585379175  and accuracy  0.8720511795281887\n","ep  367  training loss:  0.45808245744046827\n","valid loss  0.5009520540996248  and accuracy  0.8712514994002399\n","ep  368  training loss:  0.45848919846433567\n","valid loss  0.49765254168070017  and accuracy  0.8720511795281887\n","ep  369  training loss:  0.4600897162779526\n","valid loss  0.49629044756799734  and accuracy  0.8724510195921631\n","ep  370  training loss:  0.461487962744175\n","valid loss  0.5013945734724908  and accuracy  0.8724510195921631\n","ep  371  training loss:  0.45930079034167653\n","valid loss  0.5005710860673355  and accuracy  0.8728508596561375\n","ep  372  training loss:  0.45883394618834916\n","valid loss  0.49731749975409617  and accuracy  0.8736505397840864\n","ep  373  training loss:  0.46005550120872213\n","valid loss  0.4996120254905736  and accuracy  0.8716513394642144\n","ep  374  training loss:  0.45961294388760077\n","valid loss  0.5010671073415193  and accuracy  0.8732506997201119\n","ep  375  training loss:  0.4606551762162164\n","valid loss  0.4977562645586526  and accuracy  0.8728508596561375\n","ep  376  training loss:  0.4598457458914961\n","valid loss  0.49654404165410176  and accuracy  0.8712514994002399\n","ep  377  training loss:  0.45976772168542407\n","valid loss  0.5013075718542234  and accuracy  0.8716513394642144\n","ep  378  training loss:  0.458286121634264\n","valid loss  0.4981773845890149  and accuracy  0.8724510195921631\n","ep  379  training loss:  0.4566419243542981\n","valid loss  0.49795372588116854  and accuracy  0.8728508596561375\n","ep  380  training loss:  0.4578852102549223\n","valid loss  0.49764973037198085  and accuracy  0.8724510195921631\n","ep  381  training loss:  0.45797890361576776\n","valid loss  0.5019113390934749  and accuracy  0.8716513394642144\n","ep  382  training loss:  0.4579921197245242\n","valid loss  0.49697610115061186  and accuracy  0.8716513394642144\n","ep  383  training loss:  0.45923782633029764\n","valid loss  0.4995210368315824  and accuracy  0.8716513394642144\n","ep  384  training loss:  0.4595512288704735\n","valid loss  0.4967964515596426  and accuracy  0.8740503798480608\n","ep  385  training loss:  0.45584857839127024\n","valid loss  0.49912750322977956  and accuracy  0.8712514994002399\n","ep  386  training loss:  0.4604578464368394\n","valid loss  0.49764201491892407  and accuracy  0.8724510195921631\n","ep  387  training loss:  0.4596739081542862\n","valid loss  0.4975155114126606  and accuracy  0.8720511795281887\n","ep  388  training loss:  0.46068230360200985\n","valid loss  0.49591419826455707  and accuracy  0.8712514994002399\n","ep  389  training loss:  0.4591612935595399\n","valid loss  0.5016862747670173  and accuracy  0.8724510195921631\n","ep  390  training loss:  0.45880535135237616\n","valid loss  0.5044645269076283  and accuracy  0.8712514994002399\n","ep  391  training loss:  0.45959675534445765\n","valid loss  0.5004547223573873  and accuracy  0.8724510195921631\n","ep  392  training loss:  0.45742589705697984\n","valid loss  0.5027556999213025  and accuracy  0.8724510195921631\n","ep  393  training loss:  0.4600404358907602\n","valid loss  0.49618448764502837  and accuracy  0.8732506997201119\n","ep  394  training loss:  0.45744821196989455\n","valid loss  0.5020223006826551  and accuracy  0.8732506997201119\n","ep  395  training loss:  0.45949600890251985\n","valid loss  0.5003805529208528  and accuracy  0.8728508596561375\n","ep  396  training loss:  0.45662764135759537\n","valid loss  0.4970173132581646  and accuracy  0.8716513394642144\n","ep  397  training loss:  0.45790704197557913\n","valid loss  0.49699695717568687  and accuracy  0.8712514994002399\n","ep  398  training loss:  0.45814465549759065\n","valid loss  0.49892566612557093  and accuracy  0.8720511795281887\n","ep  399  training loss:  0.45650720063643296\n","valid loss  0.5014043942731364  and accuracy  0.8716513394642144\n","ep  400  training loss:  0.4575954774520451\n","valid loss  0.4976589523854612  and accuracy  0.8724510195921631\n","ep  401  training loss:  0.45865665256310617\n","valid loss  0.4988911208726081  and accuracy  0.8732506997201119\n","ep  402  training loss:  0.45852681519802585\n","valid loss  0.5032262059627939  and accuracy  0.8716513394642144\n","ep  403  training loss:  0.45850333311713626\n","valid loss  0.49474252314150025  and accuracy  0.8724510195921631\n","ep  404  training loss:  0.458111518618699\n","valid loss  0.4953060177911143  and accuracy  0.8724510195921631\n","ep  405  training loss:  0.45799619434062466\n","valid loss  0.5016649808944678  and accuracy  0.8720511795281887\n","ep  406  training loss:  0.45721495765063963\n","valid loss  0.4953692088313982  and accuracy  0.8724510195921631\n","ep  407  training loss:  0.45670307489829104\n","valid loss  0.49219774207273803  and accuracy  0.8724510195921631\n","ep  408  training loss:  0.4574366688099312\n","valid loss  0.49672823953704803  and accuracy  0.8728508596561375\n","ep  409  training loss:  0.4586247330315174\n","valid loss  0.4976388336371918  and accuracy  0.8728508596561375\n","ep  410  training loss:  0.45838723865514464\n","valid loss  0.49898739464756775  and accuracy  0.8736505397840864\n","ep  411  training loss:  0.4581647867960039\n","valid loss  0.498977185153618  and accuracy  0.8732506997201119\n","ep  412  training loss:  0.45674406854375194\n","valid loss  0.4982281824628242  and accuracy  0.8720511795281887\n","ep  413  training loss:  0.45775398397980843\n","valid loss  0.49719506253103696  and accuracy  0.8720511795281887\n","ep  414  training loss:  0.4559649653563353\n","valid loss  0.5035221810009135  and accuracy  0.8728508596561375\n","ep  415  training loss:  0.45695607606362915\n","valid loss  0.5015020691504816  and accuracy  0.8732506997201119\n","ep  416  training loss:  0.4575159727411307\n","valid loss  0.5053959336937641  and accuracy  0.8728508596561375\n","ep  417  training loss:  0.45746522642389065\n","valid loss  0.49954148315515867  and accuracy  0.8696521391443423\n","ep  418  training loss:  0.45583874703332244\n","valid loss  0.5054521534215065  and accuracy  0.8716513394642144\n","ep  419  training loss:  0.4572541308595715\n","valid loss  0.49840070461569097  and accuracy  0.8728508596561375\n","ep  420  training loss:  0.458283409987359\n","valid loss  0.5020006097159068  and accuracy  0.8712514994002399\n","ep  421  training loss:  0.4565980986183156\n","valid loss  0.5029796760638015  and accuracy  0.8708516593362655\n","ep  422  training loss:  0.4571506025925559\n","valid loss  0.5045060930324525  and accuracy  0.8708516593362655\n","ep  423  training loss:  0.45795630632899975\n","valid loss  0.5022811540266553  and accuracy  0.8716513394642144\n","ep  424  training loss:  0.4567498313561422\n","valid loss  0.5062091958994677  and accuracy  0.8704518192722911\n","ep  425  training loss:  0.45580786020793007\n","valid loss  0.4985164316212449  and accuracy  0.8728508596561375\n","ep  426  training loss:  0.456720522745874\n","valid loss  0.5028640708414281  and accuracy  0.8728508596561375\n","ep  427  training loss:  0.4568133516671385\n","valid loss  0.5001818626725831  and accuracy  0.8736505397840864\n","ep  428  training loss:  0.45516650205661147\n","valid loss  0.4975220661123292  and accuracy  0.8732506997201119\n","ep  429  training loss:  0.45572445129668265\n","valid loss  0.49897875391640983  and accuracy  0.8732506997201119\n","ep  430  training loss:  0.4566913803903555\n","valid loss  0.4990843526128291  and accuracy  0.8732506997201119\n","ep  431  training loss:  0.4573889335419216\n","valid loss  0.49977442706789504  and accuracy  0.8724510195921631\n","ep  432  training loss:  0.4565548293392055\n","valid loss  0.4965244304390251  and accuracy  0.8724510195921631\n","ep  433  training loss:  0.458570627843152\n","valid loss  0.4988138124209697  and accuracy  0.8716513394642144\n","ep  434  training loss:  0.4577209773991814\n","valid loss  0.49810591491400263  and accuracy  0.8716513394642144\n","ep  435  training loss:  0.45664696267658683\n","valid loss  0.49817021471745776  and accuracy  0.8736505397840864\n","ep  436  training loss:  0.45757447826816905\n","valid loss  0.49781364198924544  and accuracy  0.8724510195921631\n","ep  437  training loss:  0.4565777368027639\n","valid loss  0.49855755988667844  and accuracy  0.8736505397840864\n","ep  438  training loss:  0.4572533123097511\n","valid loss  0.49901317062972783  and accuracy  0.8724510195921631\n","ep  439  training loss:  0.45799377027820554\n","valid loss  0.49162157610958457  and accuracy  0.8744502199120352\n","ep  440  training loss:  0.45548383101500817\n","valid loss  0.4960369247762931  and accuracy  0.8732506997201119\n","ep  441  training loss:  0.45751037405305905\n","valid loss  0.49695446864455667  and accuracy  0.8716513394642144\n","ep  442  training loss:  0.45589978175524903\n","valid loss  0.5002393175582321  and accuracy  0.8728508596561375\n","ep  443  training loss:  0.45662692914586595\n","valid loss  0.49388630352750484  and accuracy  0.8748500599760096\n","ep  444  training loss:  0.45579664956245836\n","valid loss  0.500288088206814  and accuracy  0.8728508596561375\n","ep  445  training loss:  0.4552881341505412\n","valid loss  0.4983746647977772  and accuracy  0.8728508596561375\n","ep  446  training loss:  0.45615612952332485\n","valid loss  0.4982707098763926  and accuracy  0.8732506997201119\n","ep  447  training loss:  0.4588154001353579\n","valid loss  0.5022127936716702  and accuracy  0.8716513394642144\n","ep  448  training loss:  0.45633374614788264\n","valid loss  0.5036000995243229  and accuracy  0.8724510195921631\n","ep  449  training loss:  0.4550221382640575\n","valid loss  0.5029511866165323  and accuracy  0.8720511795281887\n","ep  450  training loss:  0.4561797377314768\n","valid loss  0.49842190971282996  and accuracy  0.8716513394642144\n","ep  451  training loss:  0.45655410357483905\n","valid loss  0.49879169281794045  and accuracy  0.8712514994002399\n","ep  452  training loss:  0.45668240874310556\n","valid loss  0.5041669777325276  and accuracy  0.8720511795281887\n","ep  453  training loss:  0.4573460299368954\n","valid loss  0.4990005828008228  and accuracy  0.8708516593362655\n","ep  454  training loss:  0.45583698461016714\n","valid loss  0.4986704267868277  and accuracy  0.8720511795281887\n","ep  455  training loss:  0.4570584982138113\n","valid loss  0.4965412346709494  and accuracy  0.8732506997201119\n","ep  456  training loss:  0.45616954680618177\n","valid loss  0.5050210577637994  and accuracy  0.8712514994002399\n","ep  457  training loss:  0.4543363944165915\n","valid loss  0.5025474542810744  and accuracy  0.8712514994002399\n","ep  458  training loss:  0.4549484162121154\n","valid loss  0.4994191459682835  and accuracy  0.8708516593362655\n","ep  459  training loss:  0.45755706517020134\n","valid loss  0.4984474573455682  and accuracy  0.8728508596561375\n","ep  460  training loss:  0.4529546538266541\n","valid loss  0.5041790758548189  and accuracy  0.8708516593362655\n","ep  461  training loss:  0.45732450145009507\n","valid loss  0.4973001963064605  and accuracy  0.8736505397840864\n","ep  462  training loss:  0.45751412483489545\n","valid loss  0.49734808341925835  and accuracy  0.8728508596561375\n","ep  463  training loss:  0.4542743911085788\n","valid loss  0.49550346753112035  and accuracy  0.8724510195921631\n","ep  464  training loss:  0.45647913157887493\n","valid loss  0.49301328770116254  and accuracy  0.8744502199120352\n","ep  465  training loss:  0.45614838523528073\n","valid loss  0.4935044425575793  and accuracy  0.8728508596561375\n","ep  466  training loss:  0.4531646164833758\n","valid loss  0.4967273330030704  and accuracy  0.8736505397840864\n","ep  467  training loss:  0.4550664283391683\n","valid loss  0.49761740115584013  and accuracy  0.8732506997201119\n","ep  468  training loss:  0.45652372202105823\n","valid loss  0.5057561327247132  and accuracy  0.8720511795281887\n","ep  469  training loss:  0.45584223149629427\n","valid loss  0.4979785220568679  and accuracy  0.8720511795281887\n","ep  470  training loss:  0.457597542685966\n","valid loss  0.49833587768887194  and accuracy  0.8728508596561375\n","ep  471  training loss:  0.457226200568336\n","valid loss  0.49798538942472403  and accuracy  0.8724510195921631\n","ep  472  training loss:  0.4552454800555652\n","valid loss  0.49993790454361164  and accuracy  0.8728508596561375\n","ep  473  training loss:  0.45623406134207267\n","valid loss  0.49273712393857155  and accuracy  0.8748500599760096\n","ep  474  training loss:  0.45638272280948455\n","valid loss  0.5031236446127801  and accuracy  0.8736505397840864\n","ep  475  training loss:  0.4557014327773827\n","valid loss  0.5005141839605481  and accuracy  0.8744502199120352\n","ep  476  training loss:  0.4564323987205093\n","valid loss  0.4998691998591951  and accuracy  0.8732506997201119\n","ep  477  training loss:  0.45750588768715383\n","valid loss  0.4986119185243307  and accuracy  0.8728508596561375\n","ep  478  training loss:  0.4555707557266584\n","valid loss  0.4965182908865415  and accuracy  0.8732506997201119\n","ep  479  training loss:  0.4533551093786075\n","valid loss  0.49365979807227195  and accuracy  0.8732506997201119\n","ep  480  training loss:  0.4558494971300111\n","valid loss  0.5011848178161521  and accuracy  0.8724510195921631\n","ep  481  training loss:  0.45500547361012467\n","valid loss  0.4953747066985317  and accuracy  0.8732506997201119\n","ep  482  training loss:  0.4557201276135917\n","valid loss  0.5039944537207395  and accuracy  0.8724510195921631\n","ep  483  training loss:  0.45350336038296013\n","valid loss  0.5036953496055954  and accuracy  0.8716513394642144\n","ep  484  training loss:  0.4557749767105575\n","valid loss  0.4941229106711655  and accuracy  0.8736505397840864\n","ep  485  training loss:  0.4549076265888181\n","valid loss  0.4941743054017216  and accuracy  0.8736505397840864\n","ep  486  training loss:  0.4564548429220115\n","valid loss  0.49897669416005874  and accuracy  0.8732506997201119\n","ep  487  training loss:  0.4536801874625123\n","valid loss  0.5006650413383917  and accuracy  0.8724510195921631\n","ep  488  training loss:  0.45504472419601466\n","valid loss  0.49224179427845866  and accuracy  0.8744502199120352\n","ep  489  training loss:  0.45445276681809443\n","valid loss  0.4947677298409135  and accuracy  0.8740503798480608\n","ep  490  training loss:  0.45513273001595794\n","valid loss  0.4920143430969897  and accuracy  0.8728508596561375\n","ep  491  training loss:  0.45462438162010443\n","valid loss  0.4970254690968385  and accuracy  0.8736505397840864\n","ep  492  training loss:  0.4528553429907543\n","valid loss  0.5019565341902561  and accuracy  0.8736505397840864\n","ep  493  training loss:  0.45468964591298483\n","valid loss  0.4975467907433889  and accuracy  0.8728508596561375\n","ep  494  training loss:  0.45711852026957206\n","valid loss  0.49815032747639315  and accuracy  0.8724510195921631\n","ep  495  training loss:  0.4543452934132703\n","valid loss  0.4976423771225991  and accuracy  0.8732506997201119\n","ep  496  training loss:  0.45571401984484244\n","valid loss  0.5007835030079079  and accuracy  0.8732506997201119\n","ep  497  training loss:  0.45521314525340656\n","valid loss  0.4974729472019824  and accuracy  0.8732506997201119\n","ep  498  training loss:  0.455882667841197\n","valid loss  0.4973368516496447  and accuracy  0.8748500599760096\n","ep  499  training loss:  0.4531836877132192\n","valid loss  0.49766832883240747  and accuracy  0.8728508596561375\n","ep  500  training loss:  0.4540296353343903\n","valid loss  0.4965626119566364  and accuracy  0.8740503798480608\n","ep  501  training loss:  0.4551083117547489\n","valid loss  0.4987535653758745  and accuracy  0.8720511795281887\n","ep  502  training loss:  0.45424440526209453\n","valid loss  0.493723530189746  and accuracy  0.8732506997201119\n","ep  503  training loss:  0.454849040421156\n","valid loss  0.4936962846110983  and accuracy  0.8732506997201119\n","ep  504  training loss:  0.45424154662417976\n","valid loss  0.49669694669339143  and accuracy  0.8716513394642144\n","ep  505  training loss:  0.4563488045690886\n","valid loss  0.49495959813382234  and accuracy  0.8716513394642144\n","ep  506  training loss:  0.45487102297933024\n","valid loss  0.4983356795469221  and accuracy  0.8724510195921631\n","ep  507  training loss:  0.45364108666327685\n","valid loss  0.49130444152981506  and accuracy  0.8732506997201119\n","ep  508  training loss:  0.4546410302851956\n","valid loss  0.492544956681062  and accuracy  0.8724510195921631\n","ep  509  training loss:  0.4531987826600237\n","valid loss  0.49783521466424874  and accuracy  0.8708516593362655\n","ep  510  training loss:  0.45330049060283534\n","valid loss  0.4994970037550127  and accuracy  0.8732506997201119\n","ep  511  training loss:  0.45505582065312494\n","valid loss  0.4950634255403521  and accuracy  0.8720511795281887\n","ep  512  training loss:  0.4525224370564831\n","valid loss  0.5012608734262985  and accuracy  0.8732506997201119\n","ep  513  training loss:  0.4548232632257156\n","valid loss  0.4930293324564705  and accuracy  0.8740503798480608\n","ep  514  training loss:  0.4545751583676546\n","valid loss  0.4963674914093315  and accuracy  0.8744502199120352\n","ep  515  training loss:  0.45459602345168837\n","valid loss  0.5005685723171097  and accuracy  0.8720511795281887\n","ep  516  training loss:  0.45356751799863\n","valid loss  0.5002962482161447  and accuracy  0.8708516593362655\n","ep  517  training loss:  0.4561462663337161\n","valid loss  0.497401297330761  and accuracy  0.8748500599760096\n","ep  518  training loss:  0.45488996424164385\n","valid loss  0.49656827418339916  and accuracy  0.8740503798480608\n","ep  519  training loss:  0.45653774420157406\n","valid loss  0.49898991751365784  and accuracy  0.8716513394642144\n","ep  520  training loss:  0.45427946341168046\n","valid loss  0.5020384913513728  and accuracy  0.8728508596561375\n","ep  521  training loss:  0.4531525724362207\n","valid loss  0.49653889364549897  and accuracy  0.8732506997201119\n","ep  522  training loss:  0.4551098287202909\n","valid loss  0.498925287501424  and accuracy  0.8708516593362655\n","ep  523  training loss:  0.45348465390987963\n","valid loss  0.49629064382719357  and accuracy  0.8740503798480608\n","ep  524  training loss:  0.4525699579571953\n","valid loss  0.49927717392466536  and accuracy  0.8728508596561375\n","ep  525  training loss:  0.4523968650104163\n","valid loss  0.4937551114045349  and accuracy  0.8732506997201119\n","ep  526  training loss:  0.45610307944250783\n","valid loss  0.49730437223027585  and accuracy  0.8724510195921631\n","ep  527  training loss:  0.455419034911508\n","valid loss  0.4950136081879733  and accuracy  0.8720511795281887\n","ep  528  training loss:  0.4545075786581011\n","valid loss  0.49993526100254404  and accuracy  0.8708516593362655\n","ep  529  training loss:  0.4548494161333519\n","valid loss  0.49668379961514847  and accuracy  0.8716513394642144\n","ep  530  training loss:  0.4524581579151323\n","valid loss  0.4985986894152251  and accuracy  0.8724510195921631\n","ep  531  training loss:  0.4542567904677751\n","valid loss  0.49687478347856107  and accuracy  0.8724510195921631\n","ep  532  training loss:  0.45409429576549803\n","valid loss  0.4955555173407932  and accuracy  0.8724510195921631\n","ep  533  training loss:  0.4556657666187349\n","valid loss  0.49726002617198817  and accuracy  0.8724510195921631\n","ep  534  training loss:  0.45455412040294335\n","valid loss  0.49984634485019774  and accuracy  0.8728508596561375\n","ep  535  training loss:  0.454812614361322\n","valid loss  0.4935785427731259  and accuracy  0.8728508596561375\n","ep  536  training loss:  0.45390920148252656\n","valid loss  0.4964129123769727  and accuracy  0.8724510195921631\n","ep  537  training loss:  0.45225760917541963\n","valid loss  0.5012906297880475  and accuracy  0.8720511795281887\n","ep  538  training loss:  0.4535727945398089\n","valid loss  0.5023025055734314  and accuracy  0.8716513394642144\n","ep  539  training loss:  0.4545114366242103\n","valid loss  0.4974476393581819  and accuracy  0.8728508596561375\n","ep  540  training loss:  0.45184336941597364\n","valid loss  0.4968584797135452  and accuracy  0.8740503798480608\n","ep  541  training loss:  0.4553626951293094\n","valid loss  0.49648603705204475  and accuracy  0.8720511795281887\n","ep  542  training loss:  0.45377556245487444\n","valid loss  0.5014545351731591  and accuracy  0.8712514994002399\n","ep  543  training loss:  0.4540463786557709\n","valid loss  0.49427671204419765  and accuracy  0.8732506997201119\n","ep  544  training loss:  0.45349358025645137\n","valid loss  0.49354698212563347  and accuracy  0.8728508596561375\n","ep  545  training loss:  0.45296165813123307\n","valid loss  0.4997857379369953  and accuracy  0.8724510195921631\n","ep  546  training loss:  0.4578774403542788\n","valid loss  0.49714446369288967  and accuracy  0.8736505397840864\n","ep  547  training loss:  0.4550534787913993\n","valid loss  0.4938437651391508  and accuracy  0.8724510195921631\n","ep  548  training loss:  0.45303513239793797\n","valid loss  0.49877055560717914  and accuracy  0.8728508596561375\n","ep  549  training loss:  0.4536404686023744\n","valid loss  0.4976144478541286  and accuracy  0.8744502199120352\n","ep  550  training loss:  0.4525408936862825\n","valid loss  0.5000537578962365  and accuracy  0.8732506997201119\n","ep  551  training loss:  0.45416821198620394\n","valid loss  0.4955657075782244  and accuracy  0.8732506997201119\n","ep  552  training loss:  0.45328909574683635\n","valid loss  0.4953648799040564  and accuracy  0.8732506997201119\n","ep  553  training loss:  0.4539900452798423\n","valid loss  0.5003689639523524  and accuracy  0.8724510195921631\n","ep  554  training loss:  0.45281238560451126\n","valid loss  0.49826774080959807  and accuracy  0.8728508596561375\n","ep  555  training loss:  0.4529764979692094\n","valid loss  0.5004661166634573  and accuracy  0.8720511795281887\n","ep  556  training loss:  0.45496497160376276\n","valid loss  0.49773669040284124  and accuracy  0.8716513394642144\n","ep  557  training loss:  0.45396831180488323\n","valid loss  0.5006569029211474  and accuracy  0.8724510195921631\n","ep  558  training loss:  0.45422613823067887\n","valid loss  0.4963718599817458  and accuracy  0.8736505397840864\n","ep  559  training loss:  0.4522288736074363\n","valid loss  0.49788538904868807  and accuracy  0.8720511795281887\n","ep  560  training loss:  0.45250551846315024\n","valid loss  0.5007838181189087  and accuracy  0.8720511795281887\n","ep  561  training loss:  0.45320637320757284\n","valid loss  0.49459708816573317  and accuracy  0.8728508596561375\n","ep  562  training loss:  0.4528983634212837\n","valid loss  0.4950265446122958  and accuracy  0.8744502199120352\n","ep  563  training loss:  0.45116927796080314\n","valid loss  0.4971245502481838  and accuracy  0.8736505397840864\n","ep  564  training loss:  0.45449352786965225\n","valid loss  0.4961563101152285  and accuracy  0.8740503798480608\n","ep  565  training loss:  0.4528332064611947\n","valid loss  0.4950354215456266  and accuracy  0.8736505397840864\n","ep  566  training loss:  0.4539705814865547\n","valid loss  0.4968691041759375  and accuracy  0.8740503798480608\n","ep  567  training loss:  0.45231490712743133\n","valid loss  0.4970883141751768  and accuracy  0.8728508596561375\n","ep  568  training loss:  0.4558068812430127\n","valid loss  0.5003210551283065  and accuracy  0.8716513394642144\n","ep  569  training loss:  0.45435303790724363\n","valid loss  0.49870833917790913  and accuracy  0.8724510195921631\n","ep  570  training loss:  0.45390137351484294\n","valid loss  0.49384068062857406  and accuracy  0.8720511795281887\n","ep  571  training loss:  0.45411635134990824\n","valid loss  0.4943752719706795  and accuracy  0.8728508596561375\n","ep  572  training loss:  0.45156701933105037\n","valid loss  0.5014379852202262  and accuracy  0.8728508596561375\n","ep  573  training loss:  0.4535099275847307\n","valid loss  0.49821295613338834  and accuracy  0.8724510195921631\n","ep  574  training loss:  0.45226874704052245\n","valid loss  0.4971253593079522  and accuracy  0.8728508596561375\n","ep  575  training loss:  0.4525422257573627\n","valid loss  0.4958303674179666  and accuracy  0.8724510195921631\n","ep  576  training loss:  0.4533788817697366\n","valid loss  0.49703677948929226  and accuracy  0.8728508596561375\n","ep  577  training loss:  0.4541787840851867\n","valid loss  0.4977336078703475  and accuracy  0.8732506997201119\n","ep  578  training loss:  0.4520742039623769\n","valid loss  0.4964528802584191  and accuracy  0.8748500599760096\n","ep  579  training loss:  0.45373195699323704\n","valid loss  0.4953410668832595  and accuracy  0.8728508596561375\n","ep  580  training loss:  0.4544431104862319\n","valid loss  0.4954389999838459  and accuracy  0.8736505397840864\n","ep  581  training loss:  0.4509399484647539\n","valid loss  0.49767699906798374  and accuracy  0.8740503798480608\n","ep  582  training loss:  0.4534036472113399\n","valid loss  0.49466529394425857  and accuracy  0.8736505397840864\n","ep  583  training loss:  0.4545847348014865\n","valid loss  0.4961955112035348  and accuracy  0.8724510195921631\n","ep  584  training loss:  0.4532229546810885\n","valid loss  0.5034537678334962  and accuracy  0.8728508596561375\n","ep  585  training loss:  0.4538520379473633\n","valid loss  0.5023964010015196  and accuracy  0.8720511795281887\n","ep  586  training loss:  0.4518495896582558\n","valid loss  0.49309173994948985  and accuracy  0.8736505397840864\n","ep  587  training loss:  0.45205151611419025\n","valid loss  0.4943024995731192  and accuracy  0.8732506997201119\n","ep  588  training loss:  0.4529292243195666\n","valid loss  0.4983051758749587  and accuracy  0.8736505397840864\n","ep  589  training loss:  0.45243932790325697\n","valid loss  0.49899197918851107  and accuracy  0.8736505397840864\n","ep  590  training loss:  0.45466893394167407\n","valid loss  0.5023286549962077  and accuracy  0.8712514994002399\n","ep  591  training loss:  0.45211610602390984\n","valid loss  0.4983967346984927  and accuracy  0.8736505397840864\n","ep  592  training loss:  0.45251652724309865\n","valid loss  0.5001108403303107  and accuracy  0.8716513394642144\n","ep  593  training loss:  0.4522352379152856\n","valid loss  0.4982794005601037  and accuracy  0.8736505397840864\n","ep  594  training loss:  0.4541452036335658\n","valid loss  0.4961486779323152  and accuracy  0.8724510195921631\n","ep  595  training loss:  0.454868835963877\n","valid loss  0.4978016412696663  and accuracy  0.8736505397840864\n","ep  596  training loss:  0.451965223183319\n","valid loss  0.4943502036894097  and accuracy  0.8736505397840864\n","ep  597  training loss:  0.45477658159754086\n","valid loss  0.5041396758023475  and accuracy  0.8732506997201119\n","ep  598  training loss:  0.4537272753765038\n","valid loss  0.5030290093578276  and accuracy  0.8724510195921631\n","ep  599  training loss:  0.45256802547396846\n","valid loss  0.4981663897722924  and accuracy  0.8724510195921631\n","ep  600  training loss:  0.45099243133420236\n","valid loss  0.5049524205725272  and accuracy  0.8720511795281887\n","ep  601  training loss:  0.4507406067521714\n","valid loss  0.5008374348920329  and accuracy  0.8724510195921631\n","ep  602  training loss:  0.4513917855868173\n","valid loss  0.5011687825937741  and accuracy  0.8740503798480608\n","ep  603  training loss:  0.4526680913810789\n","valid loss  0.49572223756180817  and accuracy  0.8728508596561375\n","ep  604  training loss:  0.45223876222243603\n","valid loss  0.4981920606419831  and accuracy  0.8728508596561375\n","ep  605  training loss:  0.45482165530312124\n","valid loss  0.49811547200997225  and accuracy  0.8736505397840864\n","ep  606  training loss:  0.4525151165320279\n","valid loss  0.4962723949869744  and accuracy  0.8728508596561375\n","ep  607  training loss:  0.4541369627516415\n","valid loss  0.4946343928801923  and accuracy  0.8728508596561375\n","ep  608  training loss:  0.45111845903667386\n","valid loss  0.4968490465694978  and accuracy  0.8720511795281887\n","ep  609  training loss:  0.45172428517411706\n","valid loss  0.4936737512074104  and accuracy  0.8740503798480608\n","ep  610  training loss:  0.45246647729476874\n","valid loss  0.49267038209635655  and accuracy  0.8724510195921631\n","ep  611  training loss:  0.45385564739188233\n","valid loss  0.49642964457569483  and accuracy  0.8744502199120352\n","ep  612  training loss:  0.45058884784760866\n","valid loss  0.496136376162807  and accuracy  0.8736505397840864\n","ep  613  training loss:  0.4531728063210695\n","valid loss  0.4951190015927452  and accuracy  0.8736505397840864\n","ep  614  training loss:  0.4535575589421177\n","valid loss  0.49490418908644657  and accuracy  0.8740503798480608\n","ep  615  training loss:  0.4542945892878017\n","valid loss  0.4957140851311567  and accuracy  0.8728508596561375\n","ep  616  training loss:  0.45253171798317143\n","valid loss  0.500645751251978  and accuracy  0.8736505397840864\n","ep  617  training loss:  0.45248791943982675\n","valid loss  0.49524032180664873  and accuracy  0.8732506997201119\n","ep  618  training loss:  0.45162841462251296\n","valid loss  0.49160327146597643  and accuracy  0.8736505397840864\n","ep  619  training loss:  0.4531925647041797\n","valid loss  0.4968071630219372  and accuracy  0.8744502199120352\n","ep  620  training loss:  0.45285910147434255\n","valid loss  0.49481066798077067  and accuracy  0.8748500599760096\n","ep  621  training loss:  0.4516627685454299\n","valid loss  0.494357666031259  and accuracy  0.8740503798480608\n","ep  622  training loss:  0.4517827091319781\n","valid loss  0.5006735436633223  and accuracy  0.8740503798480608\n","ep  623  training loss:  0.4535448518154424\n","valid loss  0.4924521411671156  and accuracy  0.8756497401039585\n","ep  624  training loss:  0.4500988120471889\n","valid loss  0.49804691288148056  and accuracy  0.8748500599760096\n","ep  625  training loss:  0.45253228183227784\n","valid loss  0.4978320781515389  and accuracy  0.8736505397840864\n","ep  626  training loss:  0.45489217907875507\n","valid loss  0.4967534079855797  and accuracy  0.8736505397840864\n","ep  627  training loss:  0.4518523521784078\n","valid loss  0.4932493029571161  and accuracy  0.8740503798480608\n","ep  628  training loss:  0.45231886269024923\n","valid loss  0.492961212617309  and accuracy  0.8736505397840864\n","ep  629  training loss:  0.4542050300703221\n","valid loss  0.49081937514129326  and accuracy  0.8728508596561375\n","ep  630  training loss:  0.4519662333383967\n","valid loss  0.4951383888197155  and accuracy  0.8732506997201119\n","ep  631  training loss:  0.45088517790793414\n","valid loss  0.49470191088165105  and accuracy  0.8728508596561375\n","ep  632  training loss:  0.4526199917456998\n","valid loss  0.49409235629593073  and accuracy  0.8732506997201119\n","ep  633  training loss:  0.45302483232892277\n","valid loss  0.4928136290978642  and accuracy  0.8744502199120352\n","ep  634  training loss:  0.45138434010327994\n","valid loss  0.4943782451723824  and accuracy  0.8736505397840864\n","ep  635  training loss:  0.4528794441122701\n","valid loss  0.4987866412587568  and accuracy  0.8728508596561375\n","ep  636  training loss:  0.4521934703005544\n","valid loss  0.49248828345992385  and accuracy  0.8744502199120352\n","ep  637  training loss:  0.45057885483898874\n","valid loss  0.5023984379026709  and accuracy  0.8732506997201119\n","ep  638  training loss:  0.4552013288528617\n","valid loss  0.4953365078786524  and accuracy  0.8744502199120352\n","ep  639  training loss:  0.4534315021512117\n","valid loss  0.5029252544730628  and accuracy  0.8732506997201119\n","ep  640  training loss:  0.452902384292361\n","valid loss  0.4956708551168156  and accuracy  0.8736505397840864\n","ep  641  training loss:  0.4522457640071905\n","valid loss  0.49714631281915256  and accuracy  0.8724510195921631\n","ep  642  training loss:  0.451704174377515\n","valid loss  0.49385808659095565  and accuracy  0.8732506997201119\n","ep  643  training loss:  0.451652693951408\n","valid loss  0.4999053285914104  and accuracy  0.8728508596561375\n","ep  644  training loss:  0.45197693046011256\n","valid loss  0.49571469272436597  and accuracy  0.8724510195921631\n","ep  645  training loss:  0.45221692940723435\n","valid loss  0.4943359193683672  and accuracy  0.8724510195921631\n","ep  646  training loss:  0.4549344831877076\n","valid loss  0.4933819082058796  and accuracy  0.8732506997201119\n","ep  647  training loss:  0.4506851834010587\n","valid loss  0.49438230340026085  and accuracy  0.8760495801679328\n","ep  648  training loss:  0.4517225812523253\n","valid loss  0.4980430111485641  and accuracy  0.8732506997201119\n","ep  649  training loss:  0.4536474790548758\n","valid loss  0.49642995985352195  and accuracy  0.8728508596561375\n","ep  650  training loss:  0.4513901937965912\n","valid loss  0.5023353025466144  and accuracy  0.8728508596561375\n","ep  651  training loss:  0.4507207147659451\n","valid loss  0.4920533385242476  and accuracy  0.8748500599760096\n","ep  652  training loss:  0.4515346102346312\n","valid loss  0.4956639932423103  and accuracy  0.8732506997201119\n","ep  653  training loss:  0.45079879925560423\n","valid loss  0.49632094553211886  and accuracy  0.8724510195921631\n","ep  654  training loss:  0.45208232548524496\n","valid loss  0.494080959296808  and accuracy  0.8708516593362655\n","ep  655  training loss:  0.45020163881453723\n","valid loss  0.49887952989027623  and accuracy  0.8724510195921631\n","ep  656  training loss:  0.4523490155141075\n","valid loss  0.4994433925944011  and accuracy  0.8724510195921631\n","ep  657  training loss:  0.45189833240426364\n","valid loss  0.5015763617286393  and accuracy  0.8740503798480608\n","ep  658  training loss:  0.4518509225767658\n","valid loss  0.49806377452547573  and accuracy  0.8740503798480608\n","ep  659  training loss:  0.4532126054960887\n","valid loss  0.49901459097671585  and accuracy  0.8732506997201119\n","ep  660  training loss:  0.45182685504042924\n","valid loss  0.5043521464728966  and accuracy  0.8724510195921631\n","ep  661  training loss:  0.45366174971579587\n","valid loss  0.5000443137774034  and accuracy  0.8720511795281887\n","ep  662  training loss:  0.449886551558899\n","valid loss  0.4975035477046822  and accuracy  0.8724510195921631\n","ep  663  training loss:  0.45221734349464743\n","valid loss  0.5001481720825426  and accuracy  0.8732506997201119\n","ep  664  training loss:  0.453049204560589\n","valid loss  0.49633278762374294  and accuracy  0.8744502199120352\n","ep  665  training loss:  0.4517709804223238\n","valid loss  0.49686721967058817  and accuracy  0.8740503798480608\n","ep  666  training loss:  0.4514120860641748\n","valid loss  0.49399024808182807  and accuracy  0.8724510195921631\n","ep  667  training loss:  0.450772464587374\n","valid loss  0.49384664258257194  and accuracy  0.8724510195921631\n","ep  668  training loss:  0.4504315302626482\n","valid loss  0.49821218592030964  and accuracy  0.8744502199120352\n","ep  669  training loss:  0.4512275190061149\n","valid loss  0.5018509454080841  and accuracy  0.8712514994002399\n","ep  670  training loss:  0.4520333484142808\n","valid loss  0.498011948394089  and accuracy  0.8736505397840864\n","ep  671  training loss:  0.45183976538052806\n","valid loss  0.4964728229811362  and accuracy  0.8728508596561375\n","ep  672  training loss:  0.4515109041152605\n","valid loss  0.5009465737849986  and accuracy  0.8724510195921631\n","ep  673  training loss:  0.4502805480131717\n","valid loss  0.49291959727873375  and accuracy  0.8732506997201119\n","ep  674  training loss:  0.45288522172909723\n","valid loss  0.4972991794645667  and accuracy  0.8720511795281887\n","ep  675  training loss:  0.45169223468404224\n","valid loss  0.4952317581849783  and accuracy  0.8728508596561375\n","ep  676  training loss:  0.4502211181523587\n","valid loss  0.49672509793613684  and accuracy  0.8732506997201119\n","ep  677  training loss:  0.45119855409042137\n","valid loss  0.4936335692521049  and accuracy  0.8740503798480608\n","ep  678  training loss:  0.4508841872125411\n","valid loss  0.4982451480038402  and accuracy  0.8736505397840864\n","ep  679  training loss:  0.45332760167404446\n","valid loss  0.4952480637493347  and accuracy  0.8740503798480608\n","ep  680  training loss:  0.45021562906729173\n","valid loss  0.4972110997672464  and accuracy  0.8728508596561375\n","ep  681  training loss:  0.44984853191538277\n","valid loss  0.4935022032723242  and accuracy  0.8740503798480608\n","ep  682  training loss:  0.45257005756327373\n","valid loss  0.4942134656414229  and accuracy  0.8736505397840864\n","ep  683  training loss:  0.4517503416719456\n","valid loss  0.4979918440882085  and accuracy  0.8728508596561375\n","ep  684  training loss:  0.4508809082656556\n","valid loss  0.49496023158510416  and accuracy  0.8728508596561375\n","ep  685  training loss:  0.44911641586769907\n","valid loss  0.5007015330845811  and accuracy  0.8708516593362655\n","ep  686  training loss:  0.4514591771657006\n","valid loss  0.49711259771756583  and accuracy  0.8732506997201119\n","ep  687  training loss:  0.45087226959467785\n","valid loss  0.49743538780338237  and accuracy  0.8728508596561375\n","ep  688  training loss:  0.45077006645705653\n","valid loss  0.4963322670423522  and accuracy  0.8728508596561375\n","ep  689  training loss:  0.44997788734624544\n","valid loss  0.48737441306588936  and accuracy  0.8764494202319072\n","ep  690  training loss:  0.45287803161523166\n","valid loss  0.4965395236530098  and accuracy  0.8744502199120352\n","ep  691  training loss:  0.45217567170188977\n","valid loss  0.49500043873117716  and accuracy  0.8724510195921631\n","ep  692  training loss:  0.4499675064581344\n","valid loss  0.4891761503949827  and accuracy  0.8760495801679328\n","ep  693  training loss:  0.45291606049560823\n","valid loss  0.48926712264541816  and accuracy  0.8744502199120352\n","ep  694  training loss:  0.44977498038654085\n","valid loss  0.5003178084625907  and accuracy  0.8712514994002399\n","ep  695  training loss:  0.45189976732761516\n","valid loss  0.4937417450212374  and accuracy  0.8728508596561375\n","ep  696  training loss:  0.4521783521174486\n","valid loss  0.4921869195041443  and accuracy  0.8740503798480608\n","ep  697  training loss:  0.4511979549013693\n","valid loss  0.48985477980495884  and accuracy  0.8756497401039585\n","ep  698  training loss:  0.4504435863111071\n","valid loss  0.49785018654786695  and accuracy  0.8748500599760096\n","ep  699  training loss:  0.4497314459866997\n","valid loss  0.49429057498590223  and accuracy  0.8744502199120352\n","ep  700  training loss:  0.45048377723459015\n","valid loss  0.49187783997614637  and accuracy  0.8736505397840864\n","ep  701  training loss:  0.4519357315968507\n","valid loss  0.5003261874552395  and accuracy  0.8740503798480608\n","ep  702  training loss:  0.44795140771441705\n","valid loss  0.49782505826871903  and accuracy  0.8740503798480608\n","ep  703  training loss:  0.44952882259523774\n","valid loss  0.49608340829622743  and accuracy  0.8728508596561375\n","ep  704  training loss:  0.44994114758885584\n","valid loss  0.49854761028423256  and accuracy  0.8732506997201119\n","ep  705  training loss:  0.45051909568866016\n","valid loss  0.4954781993681409  and accuracy  0.8744502199120352\n","ep  706  training loss:  0.44980381829758886\n","valid loss  0.4965196942053333  and accuracy  0.8736505397840864\n","ep  707  training loss:  0.4503995434358954\n","valid loss  0.4956481560665529  and accuracy  0.8732506997201119\n","ep  708  training loss:  0.45013272446540487\n","valid loss  0.49139870675217384  and accuracy  0.8740503798480608\n","ep  709  training loss:  0.4499349380998217\n","valid loss  0.49428051291871483  and accuracy  0.8736505397840864\n","ep  710  training loss:  0.4532454176683253\n","valid loss  0.49601149968936986  and accuracy  0.8732506997201119\n","ep  711  training loss:  0.45234290683053335\n","valid loss  0.4896651757497494  and accuracy  0.8728508596561375\n","ep  712  training loss:  0.44956240237290856\n","valid loss  0.4922225026310277  and accuracy  0.8740503798480608\n","ep  713  training loss:  0.4513268293427924\n","valid loss  0.49241500503060726  and accuracy  0.8748500599760096\n","ep  714  training loss:  0.45045454495166526\n","valid loss  0.4949311222709784  and accuracy  0.8740503798480608\n","ep  715  training loss:  0.4518895431703407\n","valid loss  0.4982345032482231  and accuracy  0.8744502199120352\n","ep  716  training loss:  0.45354407301619654\n","valid loss  0.49672746381870225  and accuracy  0.8740503798480608\n","ep  717  training loss:  0.4493312998964008\n","valid loss  0.4948962623002481  and accuracy  0.8744502199120352\n","ep  718  training loss:  0.4495694990660598\n","valid loss  0.5004292813743033  and accuracy  0.8740503798480608\n","ep  719  training loss:  0.45079955882355965\n","valid loss  0.4924436388183527  and accuracy  0.8736505397840864\n","ep  720  training loss:  0.45279867615430786\n","valid loss  0.49320430159330464  and accuracy  0.8740503798480608\n","ep  721  training loss:  0.4492327441566728\n","valid loss  0.4992882934440283  and accuracy  0.8736505397840864\n","ep  722  training loss:  0.4513154786954229\n","valid loss  0.4899823790023633  and accuracy  0.8748500599760096\n","ep  723  training loss:  0.450173296054617\n","valid loss  0.4922110278193639  and accuracy  0.8744502199120352\n","ep  724  training loss:  0.44913441038989377\n","valid loss  0.4881809255639251  and accuracy  0.8748500599760096\n","ep  725  training loss:  0.4508758629754718\n","valid loss  0.4939530481938504  and accuracy  0.8728508596561375\n","ep  726  training loss:  0.4490091980746695\n","valid loss  0.49063007646157997  and accuracy  0.8744502199120352\n","ep  727  training loss:  0.4494728347834712\n","valid loss  0.49401835897644725  and accuracy  0.8744502199120352\n","ep  728  training loss:  0.4496893409813233\n","valid loss  0.4905841837449819  and accuracy  0.8756497401039585\n","ep  729  training loss:  0.4508522288002727\n","valid loss  0.49458616210526823  and accuracy  0.8740503798480608\n","ep  730  training loss:  0.44771162974928796\n","valid loss  0.4967253355682492  and accuracy  0.8744502199120352\n","ep  731  training loss:  0.4501590231372122\n","valid loss  0.4983331598266989  and accuracy  0.8736505397840864\n","ep  732  training loss:  0.4517928897707979\n","valid loss  0.4971692107144188  and accuracy  0.8732506997201119\n","ep  733  training loss:  0.44996271236537216\n","valid loss  0.49968364019719946  and accuracy  0.8724510195921631\n","ep  734  training loss:  0.44990312705922003\n","valid loss  0.4954819803188344  and accuracy  0.8736505397840864\n","ep  735  training loss:  0.4492097800051758\n","valid loss  0.4938250977437242  and accuracy  0.8748500599760096\n","ep  736  training loss:  0.45217314531637487\n","valid loss  0.4962333758227208  and accuracy  0.8740503798480608\n","ep  737  training loss:  0.4506984900824663\n","valid loss  0.49376684992087455  and accuracy  0.8724510195921631\n","ep  738  training loss:  0.4514454317228185\n","valid loss  0.49923098870202665  and accuracy  0.8720511795281887\n","ep  739  training loss:  0.45005177235182986\n","valid loss  0.4956685446205734  and accuracy  0.8744502199120352\n","ep  740  training loss:  0.4510347854277862\n","valid loss  0.49291496162936954  and accuracy  0.8724510195921631\n","ep  741  training loss:  0.45094751863030513\n","valid loss  0.49401653460005385  and accuracy  0.8732506997201119\n","ep  742  training loss:  0.4501921895376137\n","valid loss  0.49338340911804224  and accuracy  0.8736505397840864\n","ep  743  training loss:  0.44998425817892995\n","valid loss  0.49205413753869104  and accuracy  0.8740503798480608\n","ep  744  training loss:  0.4497270323212879\n","valid loss  0.49976538837265844  and accuracy  0.8728508596561375\n","ep  745  training loss:  0.4488725741283294\n","valid loss  0.4926392221774925  and accuracy  0.8728508596561375\n","ep  746  training loss:  0.450905204000243\n","valid loss  0.4897984539757057  and accuracy  0.8756497401039585\n","ep  747  training loss:  0.4502830885054603\n","valid loss  0.4997916858418376  and accuracy  0.8728508596561375\n","ep  748  training loss:  0.45297243696066036\n","valid loss  0.4941627649439187  and accuracy  0.8732506997201119\n","ep  749  training loss:  0.45012249509874586\n","valid loss  0.4926834528158303  and accuracy  0.8736505397840864\n","ep  750  training loss:  0.4505671568889155\n","valid loss  0.49338259586378463  and accuracy  0.8748500599760096\n","ep  751  training loss:  0.4496294095431437\n","valid loss  0.49317729835365354  and accuracy  0.8736505397840864\n","ep  752  training loss:  0.45116725491490955\n","valid loss  0.49307319146544876  and accuracy  0.8740503798480608\n","ep  753  training loss:  0.45010624885049877\n","valid loss  0.4907989686772805  and accuracy  0.875249900039984\n","ep  754  training loss:  0.45064289195987167\n","valid loss  0.4945752280371421  and accuracy  0.8748500599760096\n","ep  755  training loss:  0.45239149648253907\n","valid loss  0.4900540190904153  and accuracy  0.8728508596561375\n","ep  756  training loss:  0.4487808309406525\n","valid loss  0.49646601520600864  and accuracy  0.8736505397840864\n","ep  757  training loss:  0.45153694230660785\n","valid loss  0.48915154425824275  and accuracy  0.8732506997201119\n","ep  758  training loss:  0.4503103599780378\n","valid loss  0.48853396083630835  and accuracy  0.8748500599760096\n","ep  759  training loss:  0.450906397493457\n","valid loss  0.49415312694912195  and accuracy  0.875249900039984\n","ep  760  training loss:  0.45090071136410587\n","valid loss  0.49239631912938026  and accuracy  0.8748500599760096\n","ep  761  training loss:  0.4487952190673283\n","valid loss  0.49417407497888755  and accuracy  0.8736505397840864\n","ep  762  training loss:  0.4493569163946836\n","valid loss  0.4923404896321272  and accuracy  0.8728508596561375\n","ep  763  training loss:  0.448947417505912\n","valid loss  0.493564347823302  and accuracy  0.8736505397840864\n","ep  764  training loss:  0.44831074471050153\n","valid loss  0.49761014830012745  and accuracy  0.8732506997201119\n","ep  765  training loss:  0.4501708671647184\n","valid loss  0.4970369080646855  and accuracy  0.8740503798480608\n","ep  766  training loss:  0.4496973343438462\n","valid loss  0.4958632620083528  and accuracy  0.8728508596561375\n","ep  767  training loss:  0.44863324711817915\n","valid loss  0.5002807054649301  and accuracy  0.8724510195921631\n","ep  768  training loss:  0.44687661943815477\n","valid loss  0.49329030549511915  and accuracy  0.8724510195921631\n","ep  769  training loss:  0.45015124045523014\n","valid loss  0.4982393751235925  and accuracy  0.8740503798480608\n","ep  770  training loss:  0.44874159316231027\n","valid loss  0.49581185371720377  and accuracy  0.8744502199120352\n","ep  771  training loss:  0.4514681256690054\n","valid loss  0.4951573706350056  and accuracy  0.8724510195921631\n","ep  772  training loss:  0.4506593396093654\n","valid loss  0.4984067016747035  and accuracy  0.8732506997201119\n","ep  773  training loss:  0.44982260415325304\n","valid loss  0.49600329586430963  and accuracy  0.8728508596561375\n","ep  774  training loss:  0.4506076484864928\n","valid loss  0.4950076072562079  and accuracy  0.8728508596561375\n","ep  775  training loss:  0.4491760739567162\n","valid loss  0.49448356998772297  and accuracy  0.8740503798480608\n","ep  776  training loss:  0.45011029051168816\n","valid loss  0.49896579632898275  and accuracy  0.8724510195921631\n","ep  777  training loss:  0.44873076286056923\n","valid loss  0.49502093584143797  and accuracy  0.8732506997201119\n","ep  778  training loss:  0.44961846057002514\n","valid loss  0.4940917957834795  and accuracy  0.8724510195921631\n","ep  779  training loss:  0.4483307386728641\n","valid loss  0.4960935814935462  and accuracy  0.8724510195921631\n","ep  780  training loss:  0.45024328297409333\n","valid loss  0.49225506399784597  and accuracy  0.8728508596561375\n","ep  781  training loss:  0.44882470505127453\n","valid loss  0.4942507213327895  and accuracy  0.8724510195921631\n","ep  782  training loss:  0.45083803881117784\n","valid loss  0.4964001531340703  and accuracy  0.8724510195921631\n","ep  783  training loss:  0.4519912200615261\n","valid loss  0.49503673029775286  and accuracy  0.8732506997201119\n","ep  784  training loss:  0.4506266060459179\n","valid loss  0.4914062257911243  and accuracy  0.8756497401039585\n","ep  785  training loss:  0.4484744709092891\n","valid loss  0.49462693066942076  and accuracy  0.8740503798480608\n","ep  786  training loss:  0.4511971476239881\n","valid loss  0.49662149767549646  and accuracy  0.8744502199120352\n","ep  787  training loss:  0.44918856606710506\n","valid loss  0.49162023806753086  and accuracy  0.875249900039984\n","ep  788  training loss:  0.4488221953984426\n","valid loss  0.49100336132169675  and accuracy  0.875249900039984\n","ep  789  training loss:  0.44859628974605115\n","valid loss  0.4914630751450602  and accuracy  0.8740503798480608\n","ep  790  training loss:  0.4509251688080241\n","valid loss  0.4937304995242046  and accuracy  0.8748500599760096\n","ep  791  training loss:  0.4451742957567788\n","valid loss  0.49569740967720044  and accuracy  0.8740503798480608\n","ep  792  training loss:  0.45081265303970675\n","valid loss  0.492005651269327  and accuracy  0.8736505397840864\n","ep  793  training loss:  0.447859487306573\n","valid loss  0.48907964850892455  and accuracy  0.8748500599760096\n","ep  794  training loss:  0.4499721807238195\n","valid loss  0.49810501405211843  and accuracy  0.8740503798480608\n","ep  795  training loss:  0.4478994596730355\n","valid loss  0.4966306671148679  and accuracy  0.8740503798480608\n","ep  796  training loss:  0.4498586096832104\n","valid loss  0.4935212491012392  and accuracy  0.8732506997201119\n","ep  797  training loss:  0.4514385469520235\n","valid loss  0.49501371501637  and accuracy  0.8740503798480608\n","ep  798  training loss:  0.4489045214129331\n","valid loss  0.4911685794627652  and accuracy  0.8760495801679328\n","ep  799  training loss:  0.4483688835163373\n","valid loss  0.4978863755162837  and accuracy  0.8748500599760096\n","ep  800  training loss:  0.4479881154030151\n","valid loss  0.49401095150852625  and accuracy  0.8740503798480608\n","ep  801  training loss:  0.44965090343009545\n","valid loss  0.49472747016839624  and accuracy  0.875249900039984\n","ep  802  training loss:  0.4479281889757792\n","valid loss  0.4961705562211379  and accuracy  0.8728508596561375\n","ep  803  training loss:  0.44945247631146085\n","valid loss  0.49575664812638637  and accuracy  0.8736505397840864\n","ep  804  training loss:  0.448312624610645\n","valid loss  0.49496322861913206  and accuracy  0.8736505397840864\n","ep  805  training loss:  0.4492879730394884\n","valid loss  0.49488639611093965  and accuracy  0.8728508596561375\n","ep  806  training loss:  0.44964580457960074\n","valid loss  0.4939326418489992  and accuracy  0.8736505397840864\n","ep  807  training loss:  0.4506981626775148\n","valid loss  0.4986971968986758  and accuracy  0.8740503798480608\n","ep  808  training loss:  0.45104348660513827\n","valid loss  0.498244549359669  and accuracy  0.8740503798480608\n","ep  809  training loss:  0.44949079552828614\n","valid loss  0.4965104460954571  and accuracy  0.8732506997201119\n","ep  810  training loss:  0.4494517343610599\n","valid loss  0.49478632578702986  and accuracy  0.8744502199120352\n","ep  811  training loss:  0.4498091317874572\n","valid loss  0.4955456889018875  and accuracy  0.8748500599760096\n","ep  812  training loss:  0.45039283216106935\n","valid loss  0.49584282869722596  and accuracy  0.8736505397840864\n","ep  813  training loss:  0.44805547578050553\n","valid loss  0.48802188185394785  and accuracy  0.8748500599760096\n","ep  814  training loss:  0.4483278217956775\n","valid loss  0.498180925417118  and accuracy  0.8728508596561375\n","ep  815  training loss:  0.44701961893607806\n","valid loss  0.49379093749005526  and accuracy  0.8736505397840864\n","ep  816  training loss:  0.44883040434262117\n","valid loss  0.4937202144937008  and accuracy  0.875249900039984\n","ep  817  training loss:  0.4495418896337236\n","valid loss  0.4953358830189238  and accuracy  0.8740503798480608\n","ep  818  training loss:  0.4475057511314874\n","valid loss  0.4898406234277529  and accuracy  0.8744502199120352\n","ep  819  training loss:  0.4493839433032557\n","valid loss  0.49668640930239844  and accuracy  0.8736505397840864\n","ep  820  training loss:  0.449317851299826\n","valid loss  0.49491602292017  and accuracy  0.8732506997201119\n","ep  821  training loss:  0.4490306528176019\n","valid loss  0.4975727680967789  and accuracy  0.8728508596561375\n","ep  822  training loss:  0.4465746943378245\n","valid loss  0.4958176028247072  and accuracy  0.8740503798480608\n","ep  823  training loss:  0.44866736123467904\n","valid loss  0.4952123237580883  and accuracy  0.8736505397840864\n","ep  824  training loss:  0.45050398651901274\n","valid loss  0.4967780278020742  and accuracy  0.8732506997201119\n","ep  825  training loss:  0.446094543312137\n","valid loss  0.49594415991795343  and accuracy  0.8756497401039585\n","ep  826  training loss:  0.4478770585169749\n","valid loss  0.4974546125773095  and accuracy  0.8744502199120352\n","ep  827  training loss:  0.448676622911356\n","valid loss  0.4942687013682152  and accuracy  0.8736505397840864\n","ep  828  training loss:  0.44989612298053916\n","valid loss  0.5010842754692137  and accuracy  0.8732506997201119\n","ep  829  training loss:  0.4491475694512716\n","valid loss  0.49750126212513573  and accuracy  0.8732506997201119\n","ep  830  training loss:  0.4489044367161282\n","valid loss  0.49747992141015146  and accuracy  0.8744502199120352\n","ep  831  training loss:  0.4453600219815284\n","valid loss  0.4979300610021037  and accuracy  0.8740503798480608\n","ep  832  training loss:  0.44990686979359323\n","valid loss  0.5007869790240032  and accuracy  0.8732506997201119\n","ep  833  training loss:  0.44823831529731933\n","valid loss  0.49390750028333014  and accuracy  0.8740503798480608\n","ep  834  training loss:  0.4476318419131908\n","valid loss  0.49853534347674505  and accuracy  0.8740503798480608\n","ep  835  training loss:  0.4487598306959291\n","valid loss  0.495072656538619  and accuracy  0.8740503798480608\n","ep  836  training loss:  0.44916196558172866\n","valid loss  0.49596283019709136  and accuracy  0.8736505397840864\n","ep  837  training loss:  0.4489408197991637\n","valid loss  0.49611058746610154  and accuracy  0.8724510195921631\n","ep  838  training loss:  0.44753331806191027\n","valid loss  0.5086696469416765  and accuracy  0.8740503798480608\n","ep  839  training loss:  0.4488496267968793\n","valid loss  0.5114778903830962  and accuracy  0.8744502199120352\n","ep  840  training loss:  0.4506232901129013\n","valid loss  0.49038332261022977  and accuracy  0.8732506997201119\n","ep  841  training loss:  0.44846991073212095\n","valid loss  0.49765965873982515  and accuracy  0.8728508596561375\n","ep  842  training loss:  0.44854647881037485\n","valid loss  0.4963093849479175  and accuracy  0.875249900039984\n","ep  843  training loss:  0.45090626560766556\n","valid loss  0.4959106306131722  and accuracy  0.8736505397840864\n","ep  844  training loss:  0.4491532231311741\n","valid loss  0.4920607399625904  and accuracy  0.8736505397840864\n","ep  845  training loss:  0.4486589577148705\n","valid loss  0.5170314195560294  and accuracy  0.8728508596561375\n","ep  846  training loss:  0.4495205125697789\n","valid loss  0.49439306082319423  and accuracy  0.8744502199120352\n","ep  847  training loss:  0.44691798369126373\n","valid loss  0.4933926224923048  and accuracy  0.8748500599760096\n","ep  848  training loss:  0.4500711963595974\n","valid loss  0.49169793359187547  and accuracy  0.8732506997201119\n","ep  849  training loss:  0.45198732357168125\n","valid loss  0.49226481406415096  and accuracy  0.8740503798480608\n","ep  850  training loss:  0.44830365354748514\n","valid loss  0.5020920686581668  and accuracy  0.8736505397840864\n","ep  851  training loss:  0.4494269086046681\n","valid loss  0.5077670469183009  and accuracy  0.8744502199120352\n","ep  852  training loss:  0.45004092741164337\n","valid loss  0.4968031679854685  and accuracy  0.8740503798480608\n","ep  853  training loss:  0.44967178034569744\n","valid loss  0.49248006040217734  and accuracy  0.8744502199120352\n","ep  854  training loss:  0.4500311081735646\n","valid loss  0.49617282610709834  and accuracy  0.8732506997201119\n","ep  855  training loss:  0.44823172799436345\n","valid loss  0.49622665334348437  and accuracy  0.8728508596561375\n","ep  856  training loss:  0.44547142791755034\n","valid loss  0.49278052910858516  and accuracy  0.8732506997201119\n","ep  857  training loss:  0.4463602522618757\n","valid loss  0.5003848897128618  and accuracy  0.8728508596561375\n","ep  858  training loss:  0.44870970286517853\n","valid loss  0.4945674766926039  and accuracy  0.8764494202319072\n","ep  859  training loss:  0.445766564399794\n","valid loss  0.4983952708694278  and accuracy  0.875249900039984\n","ep  860  training loss:  0.4472321454611719\n","valid loss  0.4940272379712742  and accuracy  0.8732506997201119\n","ep  861  training loss:  0.44844260558824833\n","valid loss  0.4953418028016226  and accuracy  0.8748500599760096\n","ep  862  training loss:  0.4458136611475238\n","valid loss  0.49224716158019977  and accuracy  0.8748500599760096\n","ep  863  training loss:  0.4469930345381398\n","valid loss  0.4957886206322029  and accuracy  0.8732506997201119\n","ep  864  training loss:  0.4461116246369582\n","valid loss  0.5043927689806456  and accuracy  0.8732506997201119\n","ep  865  training loss:  0.4478696500719869\n","valid loss  0.49239418670827034  and accuracy  0.8740503798480608\n","ep  866  training loss:  0.4487021115263621\n","valid loss  0.49805396468769975  and accuracy  0.8740503798480608\n","ep  867  training loss:  0.44823916960768007\n","valid loss  0.4984632244662064  and accuracy  0.8720511795281887\n","ep  868  training loss:  0.4483327693918042\n","valid loss  0.498276774976693  and accuracy  0.8748500599760096\n","ep  869  training loss:  0.44735605959876495\n","valid loss  0.4928394141482239  and accuracy  0.8748500599760096\n","ep  870  training loss:  0.4504651277502296\n","valid loss  0.4980276126424964  and accuracy  0.8732506997201119\n","ep  871  training loss:  0.44751951642719173\n","valid loss  0.4917116579366941  and accuracy  0.8740503798480608\n","ep  872  training loss:  0.4455966506827451\n","valid loss  0.49569693978716495  and accuracy  0.8748500599760096\n","ep  873  training loss:  0.44913607349277535\n","valid loss  0.4950384737872353  and accuracy  0.8732506997201119\n","ep  874  training loss:  0.44879938758339655\n","valid loss  0.4905671943120602  and accuracy  0.8744502199120352\n","ep  875  training loss:  0.44823698242124316\n","valid loss  0.49707522138697013  and accuracy  0.8736505397840864\n","ep  876  training loss:  0.4495275179339571\n","valid loss  0.49274323962298167  and accuracy  0.8744502199120352\n","ep  877  training loss:  0.4445541367553188\n","valid loss  0.4953456472654621  and accuracy  0.8724510195921631\n","ep  878  training loss:  0.44904431436493086\n","valid loss  0.4977748868990688  and accuracy  0.8732506997201119\n","ep  879  training loss:  0.44972132985363944\n","valid loss  0.4979187641225782  and accuracy  0.8724510195921631\n","ep  880  training loss:  0.4477161767731969\n","valid loss  0.49195252154551233  and accuracy  0.8724510195921631\n","ep  881  training loss:  0.44870984962904215\n","valid loss  0.49458007139713467  and accuracy  0.8740503798480608\n","ep  882  training loss:  0.4460157535149926\n","valid loss  0.49450277421818595  and accuracy  0.8736505397840864\n","ep  883  training loss:  0.4472549254258547\n","valid loss  0.4914942469920029  and accuracy  0.8736505397840864\n","ep  884  training loss:  0.44883081234451455\n","valid loss  0.49755810765398356  and accuracy  0.8736505397840864\n","ep  885  training loss:  0.44875403140037423\n","valid loss  0.4968604004678608  and accuracy  0.8728508596561375\n","ep  886  training loss:  0.4494040244473021\n","valid loss  0.49580327933523854  and accuracy  0.8736505397840864\n","ep  887  training loss:  0.4458818006986769\n","valid loss  0.4946926334341828  and accuracy  0.8740503798480608\n","ep  888  training loss:  0.44668266919520966\n","valid loss  0.49514702092357177  and accuracy  0.8740503798480608\n","ep  889  training loss:  0.448533034019122\n","valid loss  0.49220575465530647  and accuracy  0.8748500599760096\n","ep  890  training loss:  0.44941783540312474\n","valid loss  0.4934386713582961  and accuracy  0.8748500599760096\n","ep  891  training loss:  0.4467379594149714\n","valid loss  0.4973399774973891  and accuracy  0.8736505397840864\n","ep  892  training loss:  0.4481968320907722\n","valid loss  0.4950983532782985  and accuracy  0.8740503798480608\n","ep  893  training loss:  0.44938074431305947\n","valid loss  0.49525526497231537  and accuracy  0.8736505397840864\n","ep  894  training loss:  0.44964736879528394\n","valid loss  0.49493163857446676  and accuracy  0.8736505397840864\n","ep  895  training loss:  0.4479713876286645\n","valid loss  0.49242307048518863  and accuracy  0.8744502199120352\n","ep  896  training loss:  0.44838266148230327\n","valid loss  0.4922466930247745  and accuracy  0.8736505397840864\n","ep  897  training loss:  0.4468310091684619\n","valid loss  0.49005060024806757  and accuracy  0.8744502199120352\n","ep  898  training loss:  0.4487493001771705\n","valid loss  0.4874008609170391  and accuracy  0.8740503798480608\n","ep  899  training loss:  0.4457061438810297\n","valid loss  0.49212480482698584  and accuracy  0.8740503798480608\n","ep  900  training loss:  0.4450238931842513\n","valid loss  0.49417168068819073  and accuracy  0.8744502199120352\n","ep  901  training loss:  0.44869425202280666\n","valid loss  0.49507169597676065  and accuracy  0.8732506997201119\n","ep  902  training loss:  0.44549264158978524\n","valid loss  0.49524909859655  and accuracy  0.8724510195921631\n","ep  903  training loss:  0.4506727837653579\n","valid loss  0.48981826560871927  and accuracy  0.8724510195921631\n","ep  904  training loss:  0.44809355273973234\n","valid loss  0.495096924387422  and accuracy  0.875249900039984\n","ep  905  training loss:  0.4478359639736979\n","valid loss  0.49897956564539864  and accuracy  0.8728508596561375\n","ep  906  training loss:  0.44465192747808013\n","valid loss  0.49187396927291704  and accuracy  0.875249900039984\n","ep  907  training loss:  0.45030044974920924\n","valid loss  0.494127150775432  and accuracy  0.8744502199120352\n","ep  908  training loss:  0.4495465796623524\n","valid loss  0.4932440119903119  and accuracy  0.8744502199120352\n","ep  909  training loss:  0.448198367151874\n","valid loss  0.4943505684550597  and accuracy  0.8744502199120352\n","ep  910  training loss:  0.44737891845554556\n","valid loss  0.4941450619592709  and accuracy  0.8728508596561375\n","ep  911  training loss:  0.4487842295386393\n","valid loss  0.49358815327733574  and accuracy  0.8740503798480608\n","ep  912  training loss:  0.4487702662988026\n","valid loss  0.49303057669449313  and accuracy  0.8728508596561375\n","ep  913  training loss:  0.4468820478065593\n","valid loss  0.49672805169066253  and accuracy  0.8740503798480608\n","ep  914  training loss:  0.4470740027416491\n","valid loss  0.49149181110198287  and accuracy  0.8744502199120352\n","ep  915  training loss:  0.4449235147402771\n","valid loss  0.4981565109637679  and accuracy  0.8736505397840864\n","ep  916  training loss:  0.4475311511669872\n","valid loss  0.503290766325058  and accuracy  0.8736505397840864\n","ep  917  training loss:  0.44826613433686846\n","valid loss  0.4898674053485181  and accuracy  0.8736505397840864\n","ep  918  training loss:  0.44974252677322246\n","valid loss  0.49823010711372495  and accuracy  0.8728508596561375\n","ep  919  training loss:  0.4482313780620063\n","valid loss  0.4981958289138797  and accuracy  0.8732506997201119\n","ep  920  training loss:  0.44552177366257273\n","valid loss  0.48977532493071957  and accuracy  0.8744502199120352\n","ep  921  training loss:  0.44967563534393007\n","valid loss  0.4941772970377279  and accuracy  0.8740503798480608\n","ep  922  training loss:  0.4464428362557225\n","valid loss  0.4964032961410887  and accuracy  0.8740503798480608\n","ep  923  training loss:  0.4479531294247469\n","valid loss  0.4961118330387343  and accuracy  0.8736505397840864\n","ep  924  training loss:  0.44782221083031204\n","valid loss  0.4959188550412774  and accuracy  0.8740503798480608\n","ep  925  training loss:  0.4473728704287272\n","valid loss  0.4971752567607753  and accuracy  0.8744502199120352\n","ep  926  training loss:  0.44801264767785776\n","valid loss  0.49730854199343516  and accuracy  0.8744502199120352\n","ep  927  training loss:  0.4476197257221751\n","valid loss  0.4956006812648933  and accuracy  0.8736505397840864\n","ep  928  training loss:  0.44894602774595255\n","valid loss  0.49959666340029846  and accuracy  0.8736505397840864\n","ep  929  training loss:  0.4501606118197796\n","valid loss  0.4963867431733666  and accuracy  0.8728508596561375\n","ep  930  training loss:  0.4484265379217972\n","valid loss  0.5066364292190915  and accuracy  0.8716513394642144\n","ep  931  training loss:  0.4472407602431152\n","valid loss  0.4924133510672536  and accuracy  0.8756497401039585\n","ep  932  training loss:  0.448188217042993\n","valid loss  0.49028598746267715  and accuracy  0.875249900039984\n","ep  933  training loss:  0.4468687782098792\n","valid loss  0.492812837364196  and accuracy  0.8748500599760096\n","ep  934  training loss:  0.4475830127368286\n","valid loss  0.4923975301332256  and accuracy  0.8736505397840864\n","ep  935  training loss:  0.45014070070709955\n","valid loss  0.492778585510986  and accuracy  0.8744502199120352\n","ep  936  training loss:  0.4471655529824088\n","valid loss  0.49439182056516995  and accuracy  0.8740503798480608\n","ep  937  training loss:  0.4487201801443765\n","valid loss  0.4928756187506458  and accuracy  0.8728508596561375\n","ep  938  training loss:  0.448143621768338\n","valid loss  0.4956457262537757  and accuracy  0.8732506997201119\n","ep  939  training loss:  0.4484900292130021\n","valid loss  0.4934634943334449  and accuracy  0.8764494202319072\n","ep  940  training loss:  0.4485673673054377\n","valid loss  0.4995823345366405  and accuracy  0.8736505397840864\n","ep  941  training loss:  0.44817661512831086\n","valid loss  0.49530534044068986  and accuracy  0.8744502199120352\n","ep  942  training loss:  0.4493491227859738\n","valid loss  0.4954002417120539  and accuracy  0.8724510195921631\n","ep  943  training loss:  0.4455475266769337\n","valid loss  0.4920364930981495  and accuracy  0.8736505397840864\n","ep  944  training loss:  0.4448352165770114\n","valid loss  0.4922926638637338  and accuracy  0.8732506997201119\n","ep  945  training loss:  0.4463432281641531\n","valid loss  0.495016545367117  and accuracy  0.8736505397840864\n","ep  946  training loss:  0.4481202772331725\n","valid loss  0.4958389607227978  and accuracy  0.8744502199120352\n","ep  947  training loss:  0.4476739148372951\n","valid loss  0.49340009454582656  and accuracy  0.8744502199120352\n","ep  948  training loss:  0.44809527077578615\n","valid loss  0.49702697961342807  and accuracy  0.8744502199120352\n","ep  949  training loss:  0.44817627027179635\n","valid loss  0.49631380853725404  and accuracy  0.8736505397840864\n","ep  950  training loss:  0.44837387565682046\n","valid loss  0.4982595392486087  and accuracy  0.8736505397840864\n","ep  951  training loss:  0.44811050075223446\n","valid loss  0.48787220064900677  and accuracy  0.8744502199120352\n","ep  952  training loss:  0.44754386796359813\n","valid loss  0.4964105410844695  and accuracy  0.8728508596561375\n","ep  953  training loss:  0.4458360859442558\n","valid loss  0.5046021409484683  and accuracy  0.8732506997201119\n","ep  954  training loss:  0.4470722075164695\n","valid loss  0.5027823936624652  and accuracy  0.8724510195921631\n","ep  955  training loss:  0.448765839062033\n","valid loss  0.4966418960484349  and accuracy  0.8748500599760096\n","ep  956  training loss:  0.4473350632529676\n","valid loss  0.49842612910680606  and accuracy  0.8732506997201119\n","ep  957  training loss:  0.4445481907283497\n","valid loss  0.4961875969888877  and accuracy  0.8740503798480608\n","ep  958  training loss:  0.4477216392434758\n","valid loss  0.504668811853768  and accuracy  0.8744502199120352\n","ep  959  training loss:  0.4470121068475514\n","valid loss  0.5027958478082994  and accuracy  0.8732506997201119\n","ep  960  training loss:  0.44612742255987986\n","valid loss  0.5053065551466486  and accuracy  0.8728508596561375\n","ep  961  training loss:  0.44881814147260973\n","valid loss  0.4957175377439089  and accuracy  0.8740503798480608\n","ep  962  training loss:  0.4462160234889811\n","valid loss  0.498591876468483  and accuracy  0.8732506997201119\n","ep  963  training loss:  0.44708281962375024\n","valid loss  0.49176108856193546  and accuracy  0.8732506997201119\n","ep  964  training loss:  0.44477362929339426\n","valid loss  0.49545546247214617  and accuracy  0.8736505397840864\n","ep  965  training loss:  0.4485300672209549\n","valid loss  0.5014130771040964  and accuracy  0.8724510195921631\n","ep  966  training loss:  0.4488396075079502\n","valid loss  0.49575980822118176  and accuracy  0.8744502199120352\n","ep  967  training loss:  0.44667634558396113\n","valid loss  0.4967160349676772  and accuracy  0.8724510195921631\n","ep  968  training loss:  0.44565894339990114\n","valid loss  0.4967427107154346  and accuracy  0.8728508596561375\n","ep  969  training loss:  0.44999819376684425\n","valid loss  0.5034892600996406  and accuracy  0.8732506997201119\n","ep  970  training loss:  0.44690253317089024\n","valid loss  0.4924507818070472  and accuracy  0.8732506997201119\n","ep  971  training loss:  0.4445630455392505\n","valid loss  0.49658227457803805  and accuracy  0.8744502199120352\n","ep  972  training loss:  0.4480117441042418\n","valid loss  0.49521738941409216  and accuracy  0.8736505397840864\n","ep  973  training loss:  0.4457595079826487\n","valid loss  0.49517187793128065  and accuracy  0.8736505397840864\n","ep  974  training loss:  0.4479946338769464\n","valid loss  0.49516017331642326  and accuracy  0.8744502199120352\n","ep  975  training loss:  0.44632552818435484\n","valid loss  0.49802381056444683  and accuracy  0.8744502199120352\n","ep  976  training loss:  0.44520040359213114\n","valid loss  0.49519474732308616  and accuracy  0.8736505397840864\n","ep  977  training loss:  0.44671915093080905\n","valid loss  0.4974191121366776  and accuracy  0.8740503798480608\n","ep  978  training loss:  0.44426027202063045\n","valid loss  0.4965443901422738  and accuracy  0.8744502199120352\n","ep  979  training loss:  0.44562197311289264\n","valid loss  0.49848455642281125  and accuracy  0.8744502199120352\n","ep  980  training loss:  0.4471387762965267\n","valid loss  0.494648044190851  and accuracy  0.8744502199120352\n","ep  981  training loss:  0.4477788257453603\n","valid loss  0.4993949204814382  and accuracy  0.8732506997201119\n","ep  982  training loss:  0.4488402852046988\n","valid loss  0.49845685786316274  and accuracy  0.8732506997201119\n","ep  983  training loss:  0.44817457794194604\n","valid loss  0.4946609296974112  and accuracy  0.8744502199120352\n","ep  984  training loss:  0.4465225472702463\n","valid loss  0.5000396203703996  and accuracy  0.8728508596561375\n","ep  985  training loss:  0.44592988050369875\n","valid loss  0.4957313029492488  and accuracy  0.8732506997201119\n","ep  986  training loss:  0.44679995873045386\n","valid loss  0.4908184879234151  and accuracy  0.8744502199120352\n","ep  987  training loss:  0.448261514678613\n","valid loss  0.502894224452286  and accuracy  0.8728508596561375\n","ep  988  training loss:  0.4478414878757751\n","valid loss  0.500037657075384  and accuracy  0.8724510195921631\n","ep  989  training loss:  0.44699665428910296\n","valid loss  0.49938036662871627  and accuracy  0.8732506997201119\n","ep  990  training loss:  0.4488783085948783\n","valid loss  0.4952988605626056  and accuracy  0.8724510195921631\n","ep  991  training loss:  0.4474021543399728\n","valid loss  0.49702843581614903  and accuracy  0.8736505397840864\n","ep  992  training loss:  0.44674888703760085\n","valid loss  0.49596405467811655  and accuracy  0.8732506997201119\n","ep  993  training loss:  0.4464159234082101\n","valid loss  0.4999179278240829  and accuracy  0.8740503798480608\n","ep  994  training loss:  0.4459823820265879\n","valid loss  0.4945981121573244  and accuracy  0.8744502199120352\n","ep  995  training loss:  0.4483831387213354\n","valid loss  0.49238114064333677  and accuracy  0.8740503798480608\n","ep  996  training loss:  0.4467417787175626\n","valid loss  0.4921909111206744  and accuracy  0.8732506997201119\n","ep  997  training loss:  0.447808114856006\n","valid loss  0.4892573021545929  and accuracy  0.8736505397840864\n","ep  998  training loss:  0.4445233357824309\n","valid loss  0.491649138479412  and accuracy  0.8740503798480608\n","ep  999  training loss:  0.4456659967692944\n","valid loss  0.4966934024334335  and accuracy  0.8728508596561375\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u3jQwMzoR2_z"},"source":["11 min 58 sec"]},{"cell_type":"code","metadata":{"id":"1WL6v0uDgDJA","executionInfo":{"status":"ok","timestamp":1621870424689,"user_tz":-120,"elapsed":1348,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["\"\"\" Effettuiamo le predizioni sul dataset di test \"\"\"\n","\n","test_ds = AndMal_Dataset(test, np.zeros(len(test)))\n","test_dl = DataLoader(test_ds, batch_size=batch_size)\n","test_dl = DeviceDataLoader(test_dl, device)\n","\n","# Utilizziamo la funzione softmax poichÃ© siamo interessati alla probabilitÃ  per ogni classe\n","preds = []\n","model.eval()\n","with torch.no_grad():\n","    for x, y in test_dl:\n","        out = model(x)\n","        prob = F.softmax(out, dim=1)\n","        preds.append(prob)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"syhoruAUlMAX","executionInfo":{"status":"ok","timestamp":1621870427337,"user_tz":-120,"elapsed":278,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["y_pred = []\n","for i in range(0, len(preds)):\n","  pred = preds[i].cpu()\n","  temp = np.argmax(pred, 1)\n","  temp = np.array(temp)\n","  y_pred = np.append(y_pred, temp)\n","\n","y_pred = y_pred.astype(int)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3hmjVkNmFkX","executionInfo":{"status":"ok","timestamp":1621870429756,"user_tz":-120,"elapsed":5,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"da2afee1-9545-4e27-879f-15153d04f49b"},"source":["y_pred"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 4, 4, ..., 4, 4, 4])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"WOciCjPisC_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621870431674,"user_tz":-120,"elapsed":3,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"36684e45-b42e-4293-8656-2bc1ef5c3d94"},"source":["print('Test:', Counter(y_test))\n","print('Pred:', Counter(y_pred))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Test: Counter({4: 43653, 1: 2201, 0: 1793, 2: 1477, 3: 1138})\n","Pred: Counter({4: 49648, 0: 200, 2: 199, 1: 135, 3: 80})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oqp05uc1-d4t","executionInfo":{"status":"ok","timestamp":1621870434920,"user_tz":-120,"elapsed":521,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}}},"source":["# Matrice di confusione, accuracy, classification_report\n","from sklearn.metrics import *\n","\n","# y_test Ã¨ la variabile che contiene i valori effettivi\n","# y_pred contiene i valori predetti dal modello\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","acc = accuracy_score(y_test, y_pred)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","# non presente nella libreria, calcolo mediante formula\n","f2 = (1+2**2)*((precision*recall)/((2**2*precision)+recall))"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"8rOxIo2L-d4z","executionInfo":{"status":"ok","timestamp":1621870437229,"user_tz":-120,"elapsed":545,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"fab35d8d-bc3a-4365-cce0-f56065da0984"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","# dizionario\n","target_dict = {'A_S' : 0,\n","               'R_S' : 1,\n","               'S_A' : 2,\n","               'SMS_F' : 3,\n","               'normal' : 4\n","               }\n","\n","disp = ConfusionMatrixDisplay(cm, target_dict)\n","disp.plot()"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fcf36afc250>"]},"metadata":{"tags":[]},"execution_count":25},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELCQniZh-d40","executionInfo":{"status":"ok","timestamp":1621870440389,"user_tz":-120,"elapsed":405,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"4e73fa27-e21f-49f5-afab-5149718da1bd"},"source":["mcm = multilabel_confusion_matrix(y_test, y_pred)\n","print(mcm)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[[[48423    46]\n","  [ 1639   154]]\n","\n"," [[48041    20]\n","  [ 2086   115]]\n","\n"," [[48704    81]\n","  [ 1359   118]]\n","\n"," [[49080    44]\n","  [ 1102    36]]\n","\n"," [[  553  6056]\n","  [   61 43592]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7P4q-xyB7_n","executionInfo":{"status":"ok","timestamp":1621870442531,"user_tz":-120,"elapsed":263,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"401153a4-86d0-4359-8210-2b602d0675e4"},"source":["FP = cm.sum (axis = 0) - np.diag (cm) \n","FN = cm.sum (axis = 1) - np.diag (cm) \n","TP = np.diag (cm) \n","TN = cm.sum () - (FP + FN + TP)\n","\n","print('True positive: ', TP)\n","print('True negative: ', TN)\n","print('False positive: ', FP)\n","print('False negative: ', FN)\n","\n","FP = FP.astype(float)\n","FN = FN.astype(float)\n","TP = TP.astype(float)\n","TN = TN.astype(float)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","# False negative rate\n","FNR = FN/(TP+FN)\n","\n","print('True positive rate: ', TPR)\n","print('True negative rate: ', TNR)\n","print('False positive rate: ', FPR)\n","print('False negative rate: ', FNR)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["True positive:  [  154   115   118    36 43592]\n","True negative:  [48423 48041 48704 49080   553]\n","False positive:  [  46   20   81   44 6056]\n","False negative:  [1639 2086 1359 1102   61]\n","True positive rate:  [0.08588957 0.05224898 0.07989167 0.03163445 0.99860262]\n","True negative rate:  [0.99905094 0.99958386 0.99833965 0.99910431 0.08367378]\n","False positive rate:  [9.49060224e-04 4.16137825e-04 1.66034642e-03 8.95692533e-04\n"," 9.16326222e-01]\n","False negative rate:  [0.91411043 0.94775102 0.92010833 0.96836555 0.00139738]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89Nap2dd-d40","executionInfo":{"status":"ok","timestamp":1621870448859,"user_tz":-120,"elapsed":466,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"f58387b5-eb3e-4e2f-d69d-227a13bffcce"},"source":["print(cm)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[[  154     4    48     4  1583]\n"," [   12   115    14     9  2051]\n"," [   11     2   118     4  1342]\n"," [    4    14     4    36  1080]\n"," [   19     0    15    27 43592]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wE1uRjas-d41","executionInfo":{"status":"ok","timestamp":1621870451206,"user_tz":-120,"elapsed":5,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"7f37eabf-b9ad-48a8-f1f7-c97d54d725b9"},"source":["print(report)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.09      0.15      1793\n","           1       0.85      0.05      0.10      2201\n","           2       0.59      0.08      0.14      1477\n","           3       0.45      0.03      0.06      1138\n","           4       0.88      1.00      0.93     43653\n","\n","    accuracy                           0.88     50262\n","   macro avg       0.71      0.25      0.28     50262\n","weighted avg       0.85      0.88      0.83     50262\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJICMuXI-d41","executionInfo":{"status":"ok","timestamp":1621870454033,"user_tz":-120,"elapsed":295,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"2d4e46af-154c-4efd-f45d-62a4148bdd81"},"source":["print('Accuracy: ', acc)\n","print('Precision_weighted: ', precision)\n","print('Recall_weighted: ', recall)\n","print('mcc: ', mcc)\n","print('f2: ', f2)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Accuracy:  0.8757112729298476\n","Precision_weighted:  0.8549541892297756\n","Recall_weighted:  0.8757112729298476\n","mcc:  0.2275872575128455\n","f2:  0.8714796133846204\n"],"name":"stdout"}]}]}