TabNet parameters: n_d=32, n_a=32, n_shared=2, n_ind=2, n_steps=4, relax=1.2, vbs=128

- train_loop(model, epochs=5, lr=0.02)

training loss:  0.6015733969999275
valid loss 0.563 and accuracy 0.869
training loss:  0.5656398772943027
valid loss 0.567 and accuracy 0.869
training loss:  0.5632736563996079
valid loss 0.552 and accuracy 0.869
training loss:  0.5576041457183928
valid loss 0.551 and accuracy 0.869
training loss:  0.5539402098873855
valid loss 0.549 and accuracy 0.869

[[    5     0     0     0  1825]
 [    0     0     0     0  2118]
 [    1     0     0     0  1433]
 [    0     0     0     0  1145]
 [    8     0     2     0 43724]]

              precision    recall  f1-score   support

           0       0.36      0.00      0.01      1830
           1       0.00      0.00      0.00      2118
           2       0.00      0.00      0.00      1434
           3       0.00      0.00      0.00      1145
           4       0.87      1.00      0.93     43734

    accuracy                           0.87     50261
   macro avg       0.25      0.20      0.19     50261
weighted avg       0.77      0.87      0.81     50261

Accuracy:  0.8700383995543264
Precision_weighted:  0.7702114050846527
Recall_weighted:  0.8700383995543264
mcc:  0.013494663870931067
f2:  0.8480551375645029

- train_loop(model, epochs=5, lr=0.02, wd=0.1)

training loss:  0.6230418586680404
valid loss  0.6070215889582645  and accuracy  0.8687035507844756
training loss:  0.6044069522758875
valid loss  0.614043209298793  and accuracy  0.8687035507844756
training loss:  0.6053081326908641
valid loss  0.59939580385494  and accuracy  0.8687035507844756
training loss:  0.6055551111332373
valid loss  0.5874753909402401  and accuracy  0.8687035507844756
training loss:  0.6050351907938185
valid loss  0.6124006598375732  and accuracy  0.8687035507844756

print(cm)
[[    0     0     0     0  1830]
 [    0     0     0     0  2118]
 [    0     0     0     0  1434]
 [    0     0     0     0  1145]
 [    0     0     0     0 43734]]

              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1830
           1       0.00      0.00      0.00      2118
           2       0.00      0.00      0.00      1434
           3       0.00      0.00      0.00      1145
           4       0.87      1.00      0.93     43734

    accuracy                           0.87     50261
   macro avg       0.17      0.20      0.19     50261
weighted avg       0.76      0.87      0.81     50261

Accuracy:  0.8701378802650166
Precision_weighted:  0.7571399306720964
Recall_weighted:  0.8701378802650166
mcc:  0.0
f2:  0.8449182283847083

2) Impostando la wd e abbassando il lr con 50 epoche da risultati migliori

train_loop(model, epochs=50, lr=0.001, wd=0.0000001)

training loss:  0.75332454113973
valid loss  0.8283540188626353  and accuracy  0.8687035507844756
training loss:  0.5695823893340182
valid loss  0.563536062602855  and accuracy  0.8687035507844756
training loss:  0.5658575025107212
valid loss  0.5589447038711151  and accuracy  0.8687035507844756
training loss:  0.5618722810300766
valid loss  0.5568476562456687  and accuracy  0.8687035507844756
training loss:  0.5600444236288556
valid loss  0.5588825400903341  and accuracy  0.8687035507844756
training loss:  0.5594991859452737
valid loss  0.5589601077588504  and accuracy  0.8687035507844756
training loss:  0.556310000553338
valid loss  0.5559180571361576  and accuracy  0.8687035507844756
training loss:  0.5561806875200319
valid loss  0.5520175271640623  and accuracy  0.8687035507844756
training loss:  0.5523198605842694
valid loss  0.5534313960965265  and accuracy  0.8687035507844756
training loss:  0.5510746781747128
valid loss  0.5547842382400318  and accuracy  0.8687035507844756
training loss:  0.5504240920685656
valid loss  0.5488114025994434  and accuracy  0.8687035507844756
training loss:  0.5469247483133397
valid loss  0.548556336137896  and accuracy  0.8687035507844756
training loss:  0.5467882356066891
valid loss  0.5458942161523833  and accuracy  0.8687035507844756
training loss:  0.5475247153356225
valid loss  0.5472498730034202  and accuracy  0.8687035507844756
training loss:  0.545472824032021
valid loss  0.5420732439351811  and accuracy  0.8687035507844756
training loss:  0.5450538863340803
valid loss  0.541494547336383  and accuracy  0.8687035507844756
training loss:  0.5427628313112911
valid loss  0.5396044557002042  and accuracy  0.8687035507844756
training loss:  0.5409861239636646
valid loss  0.5404708603509294  and accuracy  0.8687035507844756
training loss:  0.540504786953114
valid loss  0.5430126736028248  and accuracy  0.8687035507844756
training loss:  0.5386158730815355
valid loss  0.5403994955030185  and accuracy  0.8687035507844756
training loss:  0.5373051670796354
valid loss  0.5365874796342889  and accuracy  0.8695293146160198
training loss:  0.5367015450126512
valid loss  0.5419651516009519  and accuracy  0.8678777869529315
training loss:  0.5355836257380144
valid loss  0.5377511534190789  and accuracy  0.8687035507844756
training loss:  0.5347441296691294
valid loss  0.532672900242022  and accuracy  0.8711808422791082
training loss:  0.5329979491145359
valid loss  0.5343822447058586  and accuracy  0.8687035507844756
training loss:  0.5338451320620623
valid loss  0.5362549434999707  and accuracy  0.8687035507844756
training loss:  0.5340562540239635
valid loss  0.5371010387662419  and accuracy  0.8687035507844756
training loss:  0.5345764692841851
valid loss  0.5276547075302319  and accuracy  0.8687035507844756
training loss:  0.5340569608801962
valid loss  0.526637289073229  and accuracy  0.8687035507844756
training loss:  0.5337894386444503
valid loss  0.5314813515649759  and accuracy  0.8678777869529315
training loss:  0.5344253414758603
valid loss  0.5292029657165045  and accuracy  0.8687035507844756
training loss:  0.5324343736692949
valid loss  0.5301580623985813  and accuracy  0.8678777869529315
training loss:  0.5331675189060645
valid loss  0.5358012526316843  and accuracy  0.8678777869529315
training loss:  0.5324090496888911
valid loss  0.5421126263875985  and accuracy  0.8670520231213873
training loss:  0.5368042995364453
valid loss  0.538668570097176  and accuracy  0.8687035507844756
training loss:  0.5337829389116003
valid loss  0.5433677481679656  and accuracy  0.8678777869529315
training loss:  0.5307031377243746
valid loss  0.5400394767793518  and accuracy  0.8678777869529315
training loss:  0.5298092543438413
valid loss  0.5347585910908158  and accuracy  0.8687035507844756
training loss:  0.5270007795331315
valid loss  0.5308015185579793  and accuracy  0.8678777869529315
training loss:  0.5253019262622525
valid loss  0.5349025127072653  and accuracy  0.8695293146160198
training loss:  0.5243416634841142
valid loss  0.5322053375862336  and accuracy  0.8662262592898431
training loss:  0.5241907082636151
valid loss  0.5305230120407659  and accuracy  0.870355078447564
training loss:  0.5240363583356666
valid loss  0.5239023171506767  and accuracy  0.8711808422791082
training loss:  0.5238471261786477
valid loss  0.5210904531199316  and accuracy  0.8720066061106524
training loss:  0.5212609044270728
valid loss  0.5234894998353143  and accuracy  0.8695293146160198
training loss:  0.5214673156339596
valid loss  0.523911958977174  and accuracy  0.8711808422791082
training loss:  0.5197376117133645
valid loss  0.5285606277471333  and accuracy  0.8687035507844756
training loss:  0.519206962955917
valid loss  0.5297023646601166  and accuracy  0.8687035507844756
training loss:  0.518826525173274
valid loss  0.5286663944912193  and accuracy  0.870355078447564
training loss:  0.5168213822182565
valid loss  0.5227051872935401  and accuracy  0.8695293146160198

[[   89     1     2     0  1738]
 [    5    26     2     3  2082]
 [    5     5    12     0  1412]
 [    0     4     0     1  1140]
 [   60    10     3     0 43661]]

              precision    recall  f1-score   support

           0       0.56      0.05      0.09      1830
           1       0.57      0.01      0.02      2118
           2       0.63      0.01      0.02      1434
           3       0.25      0.00      0.00      1145
           4       0.87      1.00      0.93     43734

    accuracy                           0.87     50261
   macro avg       0.58      0.21      0.21     50261
weighted avg       0.83      0.87      0.82     50261

Accuracy:  0.8712321680826087
Precision_weighted:  0.8272342246669531
Recall_weighted:  0.8712321680826087
mcc:  0.10500328425165176
f2:  0.862062102704608

- train_loop(model, epochs=100, lr=0.001, wd=0.0000001)

training loss:  0.7426552523842194
valid loss  0.7985052856702041  and accuracy  0.8687035507844756
training loss:  0.5703471527811429
valid loss  0.5716104829340902  and accuracy  0.8687035507844756
training loss:  0.5622205573809266
valid loss  0.5569182472421944  and accuracy  0.8687035507844756
training loss:  0.5590065932509128
valid loss  0.5579356853869413  and accuracy  0.8687035507844756
training loss:  0.5577049007894368
valid loss  0.5521995067891752  and accuracy  0.8687035507844756
training loss:  0.5548695662815945
valid loss  0.5466160667671043  and accuracy  0.8687035507844756
training loss:  0.5527230178647071
valid loss  0.5492681007460067  and accuracy  0.8687035507844756
training loss:  0.5528399871745463
valid loss  0.5531628698381287  and accuracy  0.8687035507844756
training loss:  0.5526434824925761
valid loss  0.5526312327306198  and accuracy  0.8687035507844756
training loss:  0.5470602813110981
valid loss  0.5407725450029853  and accuracy  0.8687035507844756
training loss:  0.5455226712704297
valid loss  0.5419633624772214  and accuracy  0.8687035507844756
training loss:  0.5430484545245691
valid loss  0.5431268523292636  and accuracy  0.8687035507844756
training loss:  0.5428544868205082
valid loss  0.546399390175754  and accuracy  0.8695293146160198
training loss:  0.5409906501337309
valid loss  0.5428699204785877  and accuracy  0.8695293146160198
training loss:  0.540971656473685
valid loss  0.5477707569112864  and accuracy  0.8687035507844756
training loss:  0.5395257282435473
valid loss  0.5476672744376909  and accuracy  0.8695293146160198
training loss:  0.5395799170439852
valid loss  0.5397184430617719  and accuracy  0.8678777869529315
training loss:  0.5393818846443805
valid loss  0.5405616328107927  and accuracy  0.8687035507844756
training loss:  0.5381635849376311
valid loss  0.5390271796459605  and accuracy  0.8687035507844756
training loss:  0.5389495632191988
valid loss  0.5446395585597988  and accuracy  0.8687035507844756
training loss:  0.5369531729818788
valid loss  0.540334809328484  and accuracy  0.8687035507844756
training loss:  0.5339399198172967
valid loss  0.5376127477916778  and accuracy  0.8687035507844756
training loss:  0.5333947125381717
valid loss  0.5402532421932252  and accuracy  0.8695293146160198
training loss:  0.5311610357227032
valid loss  0.5411525717655752  and accuracy  0.8687035507844756
training loss:  0.5316513144543888
valid loss  0.5368111933390037  and accuracy  0.8695293146160198
training loss:  0.5329349588423311
valid loss  0.5349214704542687  and accuracy  0.8695293146160198
training loss:  0.532642706165659
valid loss  0.5339399510725386  and accuracy  0.8695293146160198
training loss:  0.530843150438515
valid loss  0.5359881544634885  and accuracy  0.8678777869529315
training loss:  0.529792569799621
valid loss  0.5349553493209165  and accuracy  0.8695293146160198
training loss:  0.5292421585242559
valid loss  0.532093768076499  and accuracy  0.8695293146160198
training loss:  0.5270926331975034
valid loss  0.5330581414512521  and accuracy  0.8687035507844756
training loss:  0.5270287914534622
valid loss  0.5284134018627894  and accuracy  0.8695293146160198
training loss:  0.5262568034674453
valid loss  0.5382869140261171  and accuracy  0.8678777869529315
training loss:  0.5318634697753172
valid loss  0.5340673526292198  and accuracy  0.8678777869529315
training loss:  0.5285906115680001
valid loss  0.5309521197583634  and accuracy  0.8678777869529315
training loss:  0.5255510577260105
valid loss  0.530924135796006  and accuracy  0.8687035507844756
training loss:  0.5246357765289429
valid loss  0.5370801965822177  and accuracy  0.8687035507844756
training loss:  0.5246994258438805
valid loss  0.5328423181513339  and accuracy  0.8687035507844756
training loss:  0.524129052066558
valid loss  0.5310240292923201  and accuracy  0.8695293146160198
training loss:  0.5260236972651716
valid loss  0.5295100848995684  and accuracy  0.870355078447564
training loss:  0.5222132062605269
valid loss  0.5300070347616636  and accuracy  0.8678777869529315
training loss:  0.5210164837578235
valid loss  0.5316083978563177  and accuracy  0.870355078447564
training loss:  0.5208424321968219
valid loss  0.5300208479781273  and accuracy  0.8711808422791082
training loss:  0.5199141616883167
valid loss  0.5353507715547311  and accuracy  0.870355078447564
training loss:  0.5200953310706321
valid loss  0.5245624537019076  and accuracy  0.870355078447564
training loss:  0.5183791250878171
valid loss  0.5288536980760481  and accuracy  0.8711808422791082
training loss:  0.5185278068697269
valid loss  0.535095313873102  and accuracy  0.8711808422791082
training loss:  0.5164726436588994
valid loss  0.5319540136417606  and accuracy  0.8687035507844756
training loss:  0.5160798497850293
valid loss  0.5331367847842682  and accuracy  0.8695293146160198
training loss:  0.5163654122051909
valid loss  0.5291805453875358  and accuracy  0.8711808422791082
training loss:  0.5157912926607393
valid loss  0.5317203522860561  and accuracy  0.8711808422791082
training loss:  0.5157797876859008
valid loss  0.5338421252323122  and accuracy  0.870355078447564
training loss:  0.5133920687475093
valid loss  0.535780537704315  and accuracy  0.870355078447564
training loss:  0.5134223559966824
valid loss  0.5285947540887224  and accuracy  0.8695293146160198
training loss:  0.5135315645840577
valid loss  0.5363275308829512  and accuracy  0.8695293146160198
training loss:  0.5128616195481721
valid loss  0.5272423657477148  and accuracy  0.870355078447564
training loss:  0.5120683977814338
valid loss  0.5189558896370706  and accuracy  0.8695293146160198
training loss:  0.5123333592576867
valid loss  0.5331068320475169  and accuracy  0.8711808422791082
training loss:  0.5150718870382964
valid loss  0.529600642369268  and accuracy  0.870355078447564
training loss:  0.514624688306417
valid loss  0.5285743166354941  and accuracy  0.8728323699421965
training loss:  0.5118361547619379
valid loss  0.5315674160700214  and accuracy  0.870355078447564
training loss:  0.5127335109664078
valid loss  0.5264469076049515  and accuracy  0.870355078447564
training loss:  0.5108233096098963
valid loss  0.5267525592389725  and accuracy  0.8711808422791082
training loss:  0.5107277536013554
valid loss  0.5281145310323165  and accuracy  0.8720066061106524
training loss:  0.5109844723642972
valid loss  0.5294175009703853  and accuracy  0.870355078447564
training loss:  0.5100939540183105
valid loss  0.5289292592435312  and accuracy  0.8711808422791082
training loss:  0.5120688202952329
valid loss  0.5334779295322818  and accuracy  0.870355078447564
training loss:  0.5089349630688373
valid loss  0.5285208153389979  and accuracy  0.8728323699421965
training loss:  0.5092009397878247
valid loss  0.5374235134790202  and accuracy  0.8678777869529315
training loss:  0.5066673125853031
valid loss  0.5258649061230172  and accuracy  0.8687035507844756
training loss:  0.5081450090344842
valid loss  0.526257675211849  and accuracy  0.870355078447564
training loss:  0.5078196098756886
valid loss  0.526394725091983  and accuracy  0.870355078447564
training loss:  0.5070398966870752
valid loss  0.5261351987904108  and accuracy  0.8720066061106524
training loss:  0.5057480647519264
valid loss  0.5295553102314128  and accuracy  0.870355078447564
training loss:  0.5046829143822935
valid loss  0.5255047602657441  and accuracy  0.8720066061106524
training loss:  0.5045546448525552
valid loss  0.5211384285588977  and accuracy  0.8728323699421965
training loss:  0.504302168900348
valid loss  0.5282975420983343  and accuracy  0.8720066061106524
training loss:  0.5034632759054477
valid loss  0.5308353372047402  and accuracy  0.8720066061106524
training loss:  0.5028260845654737
valid loss  0.5292198333633921  and accuracy  0.8711808422791082
training loss:  0.5019371469468428
valid loss  0.5215315493200555  and accuracy  0.8728323699421965
training loss:  0.5013500642689237
valid loss  0.5186905197916102  and accuracy  0.8753096614368291
training loss:  0.5015304319578908
valid loss  0.5326381800196369  and accuracy  0.8720066061106524
training loss:  0.5011100933241619
valid loss  0.5263480170761426  and accuracy  0.8728323699421965
training loss:  0.5004595788276807
valid loss  0.5317973619546899  and accuracy  0.8728323699421965
training loss:  0.5028221566267957
valid loss  0.5275011422415591  and accuracy  0.8711808422791082
training loss:  0.5023389468699989
valid loss  0.5258258906009274  and accuracy  0.8711808422791082
training loss:  0.49925931374942467
valid loss  0.5218457847366049  and accuracy  0.8728323699421965
training loss:  0.4995555300632771
valid loss  0.52465529064321  and accuracy  0.8728323699421965
training loss:  0.5001743595367635
valid loss  0.5128532495506533  and accuracy  0.8753096614368291
training loss:  0.4979173035944305
valid loss  0.5313033150012192  and accuracy  0.8711808422791082
training loss:  0.4975550140705592
valid loss  0.5180782853917185  and accuracy  0.8728323699421965
training loss:  0.4974720019104891
valid loss  0.518141786585555  and accuracy  0.8720066061106524
training loss:  0.49763747400086833
valid loss  0.5210265276552133  and accuracy  0.8711808422791082
training loss:  0.49766423840761337
valid loss  0.5236011528949124  and accuracy  0.870355078447564
training loss:  0.49787027395479555
valid loss  0.524486772094651  and accuracy  0.8720066061106524
training loss:  0.49904853861342485
valid loss  0.5157218378107967  and accuracy  0.8728323699421965
training loss:  0.4959580274209735
valid loss  0.5242849060762628  and accuracy  0.870355078447564
training loss:  0.4978169253746006
valid loss  0.5159000303014855  and accuracy  0.8728323699421965
training loss:  0.4949218763770716
valid loss  0.5139413462701068  and accuracy  0.8711808422791082
training loss:  0.49599839307381627
valid loss  0.5160414751997474  and accuracy  0.8736581337737407

[[  155     4     9     2  1660]
 [   11    69     7     0  2031]
 [   32     8    45     3  1346]
 [    3     7     2     2  1131]
 [   22     7    24     4 43677]]

              precision    recall  f1-score   support

           0       0.70      0.08      0.15      1830
           1       0.73      0.03      0.06      2118
           2       0.52      0.03      0.06      1434
           3       0.18      0.00      0.00      1145
           4       0.88      1.00      0.93     43734

    accuracy                           0.87     50261
   macro avg       0.60      0.23      0.24     50261
weighted avg       0.84      0.87      0.82     50261

Accuracy:  0.8743956546825571
Precision_weighted:  0.8372776718254579
Recall_weighted:  0.8743956546825571
mcc:  0.17821766531753658
f2:  0.8667110911249752

- train_loop(model, epochs=200, lr=0.001, wd=0.0000001)

training loss:  0.9807266100845259
valid loss  0.8852077413451859  and accuracy  0.8678777869529315
training loss:  0.5801997423480659
valid loss  0.5852351396365366  and accuracy  0.8687035507844756
training loss:  0.5696165833371809
valid loss  0.5705938278693585  and accuracy  0.8678777869529315
training loss:  0.5625431890779223
valid loss  0.5622537533770309  and accuracy  0.8687035507844756
training loss:  0.5612511124719589
valid loss  0.5618153936010466  and accuracy  0.8687035507844756
training loss:  0.5568715685424815
valid loss  0.5564827467418721  and accuracy  0.8687035507844756
training loss:  0.5552509659478787
valid loss  0.5601618368206291  and accuracy  0.8687035507844756
training loss:  0.5532261259162236
valid loss  0.5557069275912718  and accuracy  0.8687035507844756
training loss:  0.5519918477006147
valid loss  0.5524124740372995  and accuracy  0.8687035507844756
training loss:  0.5487502689619099
valid loss  0.5592820622722143  and accuracy  0.8687035507844756
training loss:  0.546872325260857
valid loss  0.553833317057738  and accuracy  0.8687035507844756
training loss:  0.5447697284600842
valid loss  0.5521156800177155  and accuracy  0.8687035507844756
training loss:  0.5427683137878103
valid loss  0.5477345111348431  and accuracy  0.8687035507844756
training loss:  0.5428787952495152
valid loss  0.5503745701665429  and accuracy  0.8687035507844756
training loss:  0.5412569861508876
valid loss  0.5478460278322045  and accuracy  0.8687035507844756
training loss:  0.5394097030078963
valid loss  0.5491929587798114  and accuracy  0.8687035507844756
training loss:  0.5358109937844022
valid loss  0.5495271643363769  and accuracy  0.8687035507844756
training loss:  0.5347331299624114
valid loss  0.549508125057504  and accuracy  0.8687035507844756
training loss:  0.5334513275061264
valid loss  0.548509326776525  and accuracy  0.8687035507844756
training loss:  0.5327002003048583
valid loss  0.5442641689007583  and accuracy  0.8687035507844756
training loss:  0.5333116589219528
valid loss  0.5414688389326894  and accuracy  0.8687035507844756
training loss:  0.5316476153359846
valid loss  0.5432479457359094  and accuracy  0.8687035507844756
training loss:  0.533102841252724
valid loss  0.5489204701906782  and accuracy  0.8687035507844756
training loss:  0.5348986122155593
valid loss  0.5401430681753907  and accuracy  0.8687035507844756
training loss:  0.5317636567733711
valid loss  0.5418469306853663  and accuracy  0.8687035507844756
training loss:  0.5302459497964989
valid loss  0.541303065963071  and accuracy  0.8687035507844756
training loss:  0.5270060887808037
valid loss  0.5346432694908413  and accuracy  0.8687035507844756
training loss:  0.5264996596001377
valid loss  0.536432112424852  and accuracy  0.8678777869529315
training loss:  0.5259098048693699
valid loss  0.541864521044724  and accuracy  0.8678777869529315
training loss:  0.537229889508414
valid loss  0.5423736113048998  and accuracy  0.8687035507844756
training loss:  0.536953495834194
valid loss  0.5414226594835544  and accuracy  0.8678777869529315
training loss:  0.5324357656429379
valid loss  0.540645917469957  and accuracy  0.8687035507844756
training loss:  0.5292749315565273
valid loss  0.5407247796322471  and accuracy  0.8695293146160198
training loss:  0.5278619969228242
valid loss  0.5368268393367701  and accuracy  0.8695293146160198
training loss:  0.5300054000715453
valid loss  0.5428174923610136  and accuracy  0.8678777869529315
training loss:  0.532841640096301
valid loss  0.5497493824124041  and accuracy  0.8678777869529315
training loss:  0.5285462784743818
valid loss  0.5609228758158318  and accuracy  0.8678777869529315
training loss:  0.5275815352817256
valid loss  0.5448745049345504  and accuracy  0.8687035507844756
training loss:  0.5262295374205777
valid loss  0.540926124220737  and accuracy  0.8687035507844756
training loss:  0.5222455689000725
valid loss  0.5425131220053682  and accuracy  0.8695293146160198
training loss:  0.5236180380043574
valid loss  0.5364214846853576  and accuracy  0.8678777869529315
training loss:  0.5209030394759605
valid loss  0.5425523722053657  and accuracy  0.8695293146160198
training loss:  0.5193520687453212
valid loss  0.5543680406866932  and accuracy  0.8670520231213873
training loss:  0.5227658114738742
valid loss  0.547468365321565  and accuracy  0.8687035507844756
training loss:  0.522198743603422
valid loss  0.539992589039019  and accuracy  0.8695293146160198
training loss:  0.5201550751092706
valid loss  0.5398636569079045  and accuracy  0.8695293146160198
training loss:  0.5197754973383251
valid loss  0.5349450202937169  and accuracy  0.8678777869529315
training loss:  0.5201137711283419
valid loss  0.5316317665685217  and accuracy  0.8687035507844756
training loss:  0.5195517869007236
valid loss  0.5295320559788301  and accuracy  0.8678777869529315
training loss:  0.5192453846654099
valid loss  0.5370778982596  and accuracy  0.8695293146160198
training loss:  0.5193626690387007
valid loss  0.5416795921955692  and accuracy  0.8687035507844756
training loss:  0.5211633311903545
valid loss  0.5414403543602229  and accuracy  0.8662262592898431
training loss:  0.5206383981445436
valid loss  0.5357206797274945  and accuracy  0.8670520231213873
training loss:  0.516542754908916
valid loss  0.5466492148000873  and accuracy  0.8687035507844756
training loss:  0.5175545495626634
valid loss  0.5388002897667353  and accuracy  0.8687035507844756
training loss:  0.5155925436474045
valid loss  0.5393908395932588  and accuracy  0.8662262592898431
training loss:  0.5144660938512553
valid loss  0.5407029583274974  and accuracy  0.8695293146160198
training loss:  0.5177370988680304
valid loss  0.5432386639982486  and accuracy  0.8695293146160198
training loss:  0.5178170199405066
valid loss  0.5430750244910022  and accuracy  0.8687035507844756
training loss:  0.5159272145839998
valid loss  0.5339251078917505  and accuracy  0.8695293146160198
training loss:  0.515806662821012
valid loss  0.544170262299718  and accuracy  0.8695293146160198
training loss:  0.5146925865384999
valid loss  0.5409514109621749  and accuracy  0.8687035507844756
training loss:  0.5124783158355776
valid loss  0.5446980070664211  and accuracy  0.8678777869529315
training loss:  0.5135475974952646
valid loss  0.540628628136204  and accuracy  0.8687035507844756
training loss:  0.512173078210058
valid loss  0.5441277026637933  and accuracy  0.8678777869529315
training loss:  0.5119485493761
valid loss  0.5385177579918349  and accuracy  0.8687035507844756
training loss:  0.5110938613107414
valid loss  0.5395790650566977  and accuracy  0.8687035507844756
training loss:  0.5090999044410344
valid loss  0.542097876895862  and accuracy  0.8678777869529315
training loss:  0.5090834676676194
valid loss  0.5358508921279875  and accuracy  0.8695293146160198
training loss:  0.5095424654687999
valid loss  0.5356597572097495  and accuracy  0.8695293146160198
training loss:  0.5079849072473781
valid loss  0.5467613781906179  and accuracy  0.8670520231213873
training loss:  0.5049195684979126
valid loss  0.5365400971901703  and accuracy  0.8678777869529315
training loss:  0.5048539757852528
valid loss  0.5399688072287279  and accuracy  0.8678777869529315
training loss:  0.5080681774054282
valid loss  0.54179437356385  and accuracy  0.8687035507844756
training loss:  0.5082479247930409
valid loss  0.5393619448266475  and accuracy  0.8678777869529315
training loss:  0.5063719772017061
valid loss  0.535445194565295  and accuracy  0.8662262592898431
training loss:  0.5058524547979352
valid loss  0.5320934270601643  and accuracy  0.8687035507844756
training loss:  0.5062677813286217
valid loss  0.5382227504873551  and accuracy  0.8687035507844756
training loss:  0.5066007849933122
valid loss  0.5416862057517325  and accuracy  0.8687035507844756
training loss:  0.5056421285973536
valid loss  0.5332188949813339  and accuracy  0.8654004954582989
training loss:  0.5050089344166283
valid loss  0.5283277606442386  and accuracy  0.8654004954582989
training loss:  0.5062906906804984
valid loss  0.5228757032867702  and accuracy  0.8662262592898431
training loss:  0.506218115024682
valid loss  0.5345397842117423  and accuracy  0.8678777869529315
training loss:  0.5047128128955366
valid loss  0.5365833193678978  and accuracy  0.8654004954582989
training loss:  0.5058340337369349
valid loss  0.5276824590787722  and accuracy  0.8687035507844756
training loss:  0.5074631851060536
valid loss  0.5347880209210881  and accuracy  0.8654004954582989
training loss:  0.5073865734931028
valid loss  0.5267489825173117  and accuracy  0.8695293146160198
training loss:  0.5055485908626749
valid loss  0.532652320446405  and accuracy  0.8678777869529315
training loss:  0.503155066176203
valid loss  0.5296264333138281  and accuracy  0.8670520231213873
training loss:  0.5022535865568221
valid loss  0.5277719483899244  and accuracy  0.8629232039636664
training loss:  0.5022842926516946
valid loss  0.5304968869951912  and accuracy  0.8645747316267548
training loss:  0.5020004888614186
valid loss  0.5326701868279329  and accuracy  0.8678777869529315
training loss:  0.5018578800963768
valid loss  0.5339484443652738  and accuracy  0.8670520231213873
training loss:  0.49815827927100126
valid loss  0.5292196089231112  and accuracy  0.8678777869529315
training loss:  0.4977713109743374
valid loss  0.5296534491313774  and accuracy  0.8662262592898431
training loss:  0.5041427956311311
valid loss  0.5236213006183789  and accuracy  0.8670520231213873
training loss:  0.5060582197612475
valid loss  0.5255650992633684  and accuracy  0.8654004954582989
training loss:  0.5020323704209138
valid loss  0.5243870547052064  and accuracy  0.8654004954582989
training loss:  0.49875454988661216
valid loss  0.5175061149108912  and accuracy  0.870355078447564
training loss:  0.4982600000481175
valid loss  0.5256008798184225  and accuracy  0.8670520231213873
training loss:  0.4970401502195859
valid loss  0.5223131657237361  and accuracy  0.8662262592898431
training loss:  0.4952374616661636
valid loss  0.5334983916936287  and accuracy  0.8654004954582989
training loss:  0.49579910186589266
valid loss  0.5260774700923166  and accuracy  0.8645747316267548
training loss:  0.495101554422593
valid loss  0.5232807940731946  and accuracy  0.8678777869529315
training loss:  0.49513807979018254
valid loss  0.5182452085193379  and accuracy  0.8670520231213873
training loss:  0.49469669710039454
valid loss  0.5248654586140722  and accuracy  0.8662262592898431
training loss:  0.4944198679522021
valid loss  0.5217391730929878  and accuracy  0.8678777869529315
training loss:  0.4948343817688361
valid loss  0.5235753009675653  and accuracy  0.8678777869529315
training loss:  0.4971090218445273
valid loss  0.5196727298440075  and accuracy  0.8687035507844756
training loss:  0.496471319612459
valid loss  0.5314316436537625  and accuracy  0.8662262592898431
training loss:  0.4960486999517863
valid loss  0.5274688482382984  and accuracy  0.8670520231213873
training loss:  0.49386753126234256
valid loss  0.5195072183030189  and accuracy  0.8670520231213873
training loss:  0.49442117269956537
valid loss  0.5300472386359775  and accuracy  0.8687035507844756
training loss:  0.4955052785808861
valid loss  0.5505754306334095  and accuracy  0.8678777869529315
training loss:  0.4944855610469632
valid loss  0.5271580652891359  and accuracy  0.8662262592898431
training loss:  0.49357127733664563
valid loss  0.5337061849288168  and accuracy  0.8687035507844756
training loss:  0.4929180319791574
valid loss  0.5341210461765356  and accuracy  0.8678777869529315
training loss:  0.4917411283179597
valid loss  0.5366764937817411  and accuracy  0.8687035507844756
training loss:  0.4895010136983677
valid loss  0.5338270783079841  and accuracy  0.8695293146160198
training loss:  0.49197303970163053
valid loss  0.5339015587167637  and accuracy  0.870355078447564
training loss:  0.491566071635068
valid loss  0.5241373943750962  and accuracy  0.8662262592898431
training loss:  0.4883778724120602
valid loss  0.5220712638659284  and accuracy  0.8695293146160198
training loss:  0.49034809259204765
valid loss  0.5268877242637212  and accuracy  0.8678777869529315
training loss:  0.4907427137045178
valid loss  0.5244335876723935  and accuracy  0.8711808422791082
training loss:  0.4903373353286714
valid loss  0.5239763135854909  and accuracy  0.8695293146160198
training loss:  0.4870973991974406
valid loss  0.5202414070545988  and accuracy  0.8695293146160198
training loss:  0.49076485976963546
valid loss  0.5260873343411012  and accuracy  0.8687035507844756
training loss:  0.48948324489621214
valid loss  0.5144567905527614  and accuracy  0.870355078447564
training loss:  0.48831779929135233
valid loss  0.5218598489964136  and accuracy  0.8687035507844756
training loss:  0.48765006755289775
valid loss  0.5271678245530258  and accuracy  0.8670520231213873
training loss:  0.48749471709775344
valid loss  0.5262319443641272  and accuracy  0.8662262592898431
training loss:  0.48672523904584125
valid loss  0.5235048054565978  and accuracy  0.8662262592898431
training loss:  0.4851270682441245
valid loss  0.521782093359162  and accuracy  0.8678777869529315
training loss:  0.4855957654000602
valid loss  0.5177042166818576  and accuracy  0.8678777869529315
training loss:  0.4849966349912887
valid loss  0.5212042914944971  and accuracy  0.8670520231213873
training loss:  0.4898588971412715
valid loss  0.5222204078533352  and accuracy  0.870355078447564
training loss:  0.4885316259204927
valid loss  0.5202486063703636  and accuracy  0.8678777869529315
training loss:  0.4880718357472024
valid loss  0.5192965631343234  and accuracy  0.870355078447564
training loss:  0.48669295261004847
valid loss  0.5160607637500684  and accuracy  0.8687035507844756
training loss:  0.48667320132428116
valid loss  0.5154014303603515  and accuracy  0.8720066061106524
training loss:  0.48469769424122405
valid loss  0.5202798952404671  and accuracy  0.8695293146160198
training loss:  0.4852561526503747
valid loss  0.5237348931668515  and accuracy  0.8678777869529315
training loss:  0.4838031981002981
valid loss  0.5206893304967368  and accuracy  0.8662262592898431
training loss:  0.4851146836672508
valid loss  0.5154532769019696  and accuracy  0.8711808422791082
training loss:  0.48458901863292186
valid loss  0.515348571548769  and accuracy  0.8695293146160198
training loss:  0.4820924607886532
valid loss  0.5173931504260598  and accuracy  0.870355078447564
training loss:  0.48363660727051017
valid loss  0.5196700829991027  and accuracy  0.8728323699421965
training loss:  0.48145478202160735
valid loss  0.5168326664915959  and accuracy  0.8728323699421965
training loss:  0.4819081205665894
valid loss  0.5198995240654067  and accuracy  0.8695293146160198
training loss:  0.48164348170568644
valid loss  0.5178041315885933  and accuracy  0.8687035507844756
training loss:  0.4789092212076321
valid loss  0.5135289854889168  and accuracy  0.8720066061106524
training loss:  0.4801316149656594
valid loss  0.5204459874714632  and accuracy  0.870355078447564
training loss:  0.4808511883989528
valid loss  0.5177773968135887  and accuracy  0.8711808422791082
training loss:  0.48281648041443753
valid loss  0.522929999475928  and accuracy  0.870355078447564
training loss:  0.48328083292536617
valid loss  0.5138142487906308  and accuracy  0.8695293146160198
training loss:  0.4802991613095858
valid loss  0.5129737610368075  and accuracy  0.8678777869529315
training loss:  0.47875266385109055
valid loss  0.5184467960545881  and accuracy  0.8670520231213873
training loss:  0.4791993084635637
valid loss  0.5161618830545396  and accuracy  0.8662262592898431
training loss:  0.47838115318948876
valid loss  0.5194562742229732  and accuracy  0.8728323699421965
training loss:  0.47705848301734666
valid loss  0.5191018407646041  and accuracy  0.8711808422791082
training loss:  0.47863805526323516
valid loss  0.5103283294658835  and accuracy  0.8711808422791082
training loss:  0.477744172352379
valid loss  0.5136887325076994  and accuracy  0.8711808422791082
training loss:  0.4771256054180979
valid loss  0.5121213838298099  and accuracy  0.8695293146160198
training loss:  0.4770269218688177
valid loss  0.5085028793868892  and accuracy  0.8720066061106524
training loss:  0.4786583201321708
valid loss  0.512759520090089  and accuracy  0.8720066061106524
training loss:  0.47767223466458325
valid loss  0.5115578129801447  and accuracy  0.8711808422791082
training loss:  0.4767628524024619
valid loss  0.5138672804359953  and accuracy  0.8687035507844756
training loss:  0.4774141746148531
valid loss  0.5094426180782838  and accuracy  0.8728323699421965
training loss:  0.4767781954448025
valid loss  0.5209641051036479  and accuracy  0.8670520231213873
training loss:  0.4748781177188942
valid loss  0.5192717768061171  and accuracy  0.8720066061106524
training loss:  0.47528683513055997
valid loss  0.518760258948675  and accuracy  0.8720066061106524
training loss:  0.47374128994160286
valid loss  0.513932010883344  and accuracy  0.8687035507844756
training loss:  0.4739050796599259
valid loss  0.514780472517998  and accuracy  0.8695293146160198
training loss:  0.47467117260998526
valid loss  0.5169633626937866  and accuracy  0.8678777869529315
training loss:  0.4720500798195655
valid loss  0.5113925014666345  and accuracy  0.8711808422791082
training loss:  0.4714521198730206
valid loss  0.5126620290153386  and accuracy  0.8720066061106524
training loss:  0.4706638330729011
valid loss  0.5224184949569324  and accuracy  0.8687035507844756
training loss:  0.4714350295204395
valid loss  0.5174853625768083  and accuracy  0.8720066061106524
training loss:  0.47092526351623687
valid loss  0.505595840716736  and accuracy  0.8728323699421965
training loss:  0.47197149166293034
valid loss  0.5223667167317857  and accuracy  0.8678777869529315
training loss:  0.47211218973326924
valid loss  0.5084090145730854  and accuracy  0.8720066061106524
training loss:  0.4703597887627743
valid loss  0.5173571172772509  and accuracy  0.8678777869529315
training loss:  0.4700076910188729
valid loss  0.5245925984433818  and accuracy  0.870355078447564
training loss:  0.4698821434864281
valid loss  0.5177321863952105  and accuracy  0.8687035507844756
training loss:  0.46926154884403204
valid loss  0.5106829185804781  and accuracy  0.8711808422791082
training loss:  0.46977386390954584
valid loss  0.5086844258324161  and accuracy  0.8720066061106524
training loss:  0.4686273638097593
valid loss  0.5064684316011424  and accuracy  0.8678777869529315
training loss:  0.4690859689110074
valid loss  0.5106760162051112  and accuracy  0.870355078447564
training loss:  0.46862903290751146
valid loss  0.5093646468848457  and accuracy  0.870355078447564
training loss:  0.468592194768349
valid loss  0.5103460490998899  and accuracy  0.870355078447564
training loss:  0.46674037486659287
valid loss  0.5077886065602008  and accuracy  0.8720066061106524
training loss:  0.4669955779181365
valid loss  0.50772414691777  and accuracy  0.8728323699421965
training loss:  0.46557382474631187
valid loss  0.511823411031167  and accuracy  0.870355078447564
training loss:  0.4659903478357765
valid loss  0.5197452942169373  and accuracy  0.8687035507844756
training loss:  0.4653735779707807
valid loss  0.5200629159008344  and accuracy  0.8645747316267548
training loss:  0.4664337034418584
valid loss  0.5172541250530892  and accuracy  0.8711808422791082
training loss:  0.4653409203028868
valid loss  0.5136993769571665  and accuracy  0.8720066061106524
training loss:  0.4634762550180978
valid loss  0.506902942620064  and accuracy  0.8728323699421965
training loss:  0.46342580291023727
valid loss  0.5163862303600737  and accuracy  0.870355078447564
training loss:  0.46550729316966016
valid loss  0.5096314396373897  and accuracy  0.8720066061106524

[[  160    13    55     3  1599]
 [   13   144    24     3  1934]
 [   11    13   104     1  1305]
 [    8    15     3    20  1099]
 [   17    70    45    13 43589]]

              precision    recall  f1-score   support

           0       0.77      0.09      0.16      1830
           1       0.56      0.07      0.12      2118
           2       0.45      0.07      0.12      1434
           3       0.50      0.02      0.03      1145
           4       0.88      1.00      0.93     43734

    accuracy                           0.88     50261
   macro avg       0.63      0.25      0.27     50261
weighted avg       0.84      0.88      0.83     50261

Accuracy:  0.8757684884900818
Precision_weighted:  0.8417349169406021
Recall_weighted:  0.8757684884900818
mcc:  0.21467046262438091
f2:  0.86874336952924

- train_loop(model, epochs=300, lr=0.001, wd=0.0000001)

training loss:  0.730952371159359
valid loss  0.8086906981113822  and accuracy  0.8687035507844756
training loss:  0.5746456636911299
valid loss  0.5615120128498503  and accuracy  0.8687035507844756
training loss:  0.5694150112876324
valid loss  0.5562345123704459  and accuracy  0.8687035507844756
training loss:  0.5645922889947459
valid loss  0.5582943897322128  and accuracy  0.8687035507844756
training loss:  0.5620579807779573
valid loss  0.5560238935502475  and accuracy  0.8687035507844756
training loss:  0.5592181644064637
valid loss  0.5522934363756565  and accuracy  0.8687035507844756
training loss:  0.5559056205203544
valid loss  0.5488439407553385  and accuracy  0.8687035507844756
training loss:  0.5552389721053415
valid loss  0.5562149563375137  and accuracy  0.8687035507844756
training loss:  0.5539721045419417
valid loss  0.552457039362138  and accuracy  0.8687035507844756
training loss:  0.5496981009974942
valid loss  0.5538623934437872  and accuracy  0.8687035507844756
training loss:  0.5477137559573219
valid loss  0.5496748070594794  and accuracy  0.8687035507844756
training loss:  0.5476385780014067
valid loss  0.5486510042116526  and accuracy  0.8687035507844756
training loss:  0.5438997365628332
valid loss  0.5459447451409577  and accuracy  0.8687035507844756
training loss:  0.5422377344429361
valid loss  0.5512956128102703  and accuracy  0.8687035507844756
training loss:  0.5402143715003908
valid loss  0.5498494115326643  and accuracy  0.8687035507844756
training loss:  0.5432181318916116
valid loss  0.553449403589959  and accuracy  0.8687035507844756
training loss:  0.5404514046691774
valid loss  0.546330583676339  and accuracy  0.8687035507844756
training loss:  0.5382438300666917
valid loss  0.5477982657591633  and accuracy  0.8687035507844756
training loss:  0.535419871936931
valid loss  0.548334509257341  and accuracy  0.8695293146160198
training loss:  0.5333449714218903
valid loss  0.5477619587538363  and accuracy  0.8695293146160198
training loss:  0.5321674167267756
valid loss  0.5503882117550989  and accuracy  0.8695293146160198
training loss:  0.5332373050951288
valid loss  0.5419858346587858  and accuracy  0.8695293146160198
training loss:  0.5333370868602016
valid loss  0.5487344104036824  and accuracy  0.8687035507844756
training loss:  0.5310725714371021
valid loss  0.5433811408542188  and accuracy  0.8695293146160198
training loss:  0.5312322746062221
valid loss  0.5446358504962764  and accuracy  0.8695293146160198
training loss:  0.531898946787021
valid loss  0.5441583705578433  and accuracy  0.8695293146160198
training loss:  0.5326994521802261
valid loss  0.5451302118630177  and accuracy  0.8687035507844756
training loss:  0.5298488077705367
valid loss  0.5417262437075844  and accuracy  0.8695293146160198
training loss:  0.5330804907322153
valid loss  0.543029648523307  and accuracy  0.8695293146160198
training loss:  0.5303959373465449
valid loss  0.5391048768844415  and accuracy  0.8695293146160198
training loss:  0.5303546963339516
valid loss  0.5475347651419415  and accuracy  0.8695293146160198
training loss:  0.5288798997280865
valid loss  0.5420836632060768  and accuracy  0.8695293146160198
training loss:  0.5318167999788717
valid loss  0.5383672090870206  and accuracy  0.8695293146160198
training loss:  0.5298819232867781
valid loss  0.5358104161936621  and accuracy  0.8687035507844756
training loss:  0.5291688741134473
valid loss  0.5387433977485392  and accuracy  0.8687035507844756
training loss:  0.5276352261842682
valid loss  0.5354081915490003  and accuracy  0.8695293146160198
training loss:  0.5268939447968268
valid loss  0.5373664334920888  and accuracy  0.8695293146160198
training loss:  0.5259704757627258
valid loss  0.5400737269322405  and accuracy  0.8695293146160198
training loss:  0.525442065726644
valid loss  0.539402040935173  and accuracy  0.8695293146160198
training loss:  0.5221696795029519
valid loss  0.5338199251845885  and accuracy  0.8695293146160198
training loss:  0.5225871385557114
valid loss  0.5338122144304555  and accuracy  0.870355078447564
training loss:  0.5224681719829237
valid loss  0.5375892430371632  and accuracy  0.8695293146160198
training loss:  0.5192301645356996
valid loss  0.5323259504150498  and accuracy  0.8695293146160198
training loss:  0.5185026997015375
valid loss  0.5331219785426098  and accuracy  0.8695293146160198
training loss:  0.518293691483129
valid loss  0.5327363623556867  and accuracy  0.8695293146160198
training loss:  0.52029164947791
valid loss  0.5402282835726497  and accuracy  0.8678777869529315
training loss:  0.5199082261969324
valid loss  0.5353295623911598  and accuracy  0.8695293146160198
training loss:  0.5188042933018449
valid loss  0.5371140666139903  and accuracy  0.8695293146160198
training loss:  0.5181411213047408
valid loss  0.5422809695675784  and accuracy  0.8695293146160198
training loss:  0.5182140117724612
valid loss  0.5372800601109505  and accuracy  0.8695293146160198
training loss:  0.516431028570015
valid loss  0.5324293409961004  and accuracy  0.8687035507844756
training loss:  0.514953050096406
valid loss  0.5280772851184811  and accuracy  0.8687035507844756
training loss:  0.5141509095406872
valid loss  0.5380636251829165  and accuracy  0.8687035507844756
training loss:  0.5156306681673049
valid loss  0.5363433723061859  and accuracy  0.8687035507844756
training loss:  0.5170541193733696
valid loss  0.5353495427934125  and accuracy  0.8695293146160198
training loss:  0.514397693765781
valid loss  0.5278092166985686  and accuracy  0.8695293146160198
training loss:  0.5123597041284951
valid loss  0.5331599997844113  and accuracy  0.8695293146160198
training loss:  0.5133829024204355
valid loss  0.5327806406332185  and accuracy  0.8678777869529315
training loss:  0.5112497137286456
valid loss  0.5342325405283077  and accuracy  0.870355078447564
training loss:  0.512374628371314
valid loss  0.5320246602069436  and accuracy  0.8695293146160198
training loss:  0.510898748195928
valid loss  0.5329141777768596  and accuracy  0.8695293146160198
training loss:  0.5113409737332346
valid loss  0.5340155844202128  and accuracy  0.8687035507844756
training loss:  0.5112950322183207
valid loss  0.5331863166624411  and accuracy  0.8687035507844756
training loss:  0.510448965472245
valid loss  0.5311795238322409  and accuracy  0.8687035507844756
training loss:  0.508389761088011
valid loss  0.5355126478768497  and accuracy  0.8687035507844756
training loss:  0.5077477229197103
valid loss  0.5267849501059333  and accuracy  0.8687035507844756
training loss:  0.508689445263506
valid loss  0.5301957288426274  and accuracy  0.8695293146160198
training loss:  0.5076813672052571
valid loss  0.5272282168059975  and accuracy  0.870355078447564
training loss:  0.5062695749801399
valid loss  0.5337368467934953  and accuracy  0.8687035507844756
training loss:  0.5061617547803344
valid loss  0.5308136497028203  and accuracy  0.870355078447564
training loss:  0.5067251477964029
valid loss  0.5368099686136726  and accuracy  0.8670520231213873
training loss:  0.506747375423117
valid loss  0.536883216766559  and accuracy  0.8695293146160198
training loss:  0.5141871095240795
valid loss  0.5317981166843537  and accuracy  0.8678777869529315
training loss:  0.512124133372774
valid loss  0.534702042574532  and accuracy  0.8687035507844756
training loss:  0.5093816963160142
valid loss  0.5256184102778194  and accuracy  0.870355078447564
training loss:  0.5110091362495996
valid loss  0.5284423194179645  and accuracy  0.8695293146160198
training loss:  0.5090129139081946
valid loss  0.5246319287478874  and accuracy  0.870355078447564
training loss:  0.512885892050218
valid loss  0.5325604183037944  and accuracy  0.8695293146160198
training loss:  0.5095201016363158
valid loss  0.5351479061372412  and accuracy  0.8695293146160198
training loss:  0.5090062084767978
valid loss  0.5280690639349767  and accuracy  0.870355078447564
training loss:  0.5101618952729432
valid loss  0.5258226864206014  and accuracy  0.8678777869529315
training loss:  0.5057054931773461
valid loss  0.5315191501137838  and accuracy  0.8678777869529315
training loss:  0.5046790098703591
valid loss  0.5308942126893091  and accuracy  0.8687035507844756
training loss:  0.5035093175776496
valid loss  0.5260711693744046  and accuracy  0.870355078447564
training loss:  0.504262789635778
valid loss  0.5240923055285368  and accuracy  0.870355078447564
training loss:  0.5004593813092938
valid loss  0.5347321007391522  and accuracy  0.8695293146160198
training loss:  0.5037194689519957
valid loss  0.5329596995322986  and accuracy  0.8687035507844756
training loss:  0.5021827173280669
valid loss  0.5270704455261482  and accuracy  0.8695293146160198
training loss:  0.5003280469252236
valid loss  0.5291833221961604  and accuracy  0.8695293146160198
training loss:  0.49939444857153187
valid loss  0.5288910194923817  and accuracy  0.870355078447564
training loss:  0.49762944053146785
valid loss  0.5291663048485898  and accuracy  0.8687035507844756
training loss:  0.49802731187553245
valid loss  0.5265340897093914  and accuracy  0.870355078447564
training loss:  0.49697184285008505
valid loss  0.5265250732640803  and accuracy  0.8687035507844756
training loss:  0.4953144032264106
valid loss  0.5282791497095078  and accuracy  0.870355078447564
training loss:  0.4960974645284331
valid loss  0.5173514105385734  and accuracy  0.8695293146160198
training loss:  0.4988180701788632
valid loss  0.5243164212815925  and accuracy  0.8695293146160198
training loss:  0.5000997525685308
valid loss  0.5246794763229584  and accuracy  0.8695293146160198
training loss:  0.4996618160501713
valid loss  0.5159613709130827  and accuracy  0.8687035507844756
training loss:  0.5020913705844957
valid loss  0.5264279947706139  and accuracy  0.8695293146160198
training loss:  0.500929769172992
valid loss  0.5245315205647274  and accuracy  0.8720066061106524
training loss:  0.49870663958042144
valid loss  0.527251022891699  and accuracy  0.8695293146160198
training loss:  0.49889371361218987
valid loss  0.524558214586102  and accuracy  0.870355078447564
training loss:  0.49723552542640886
valid loss  0.5287336251836094  and accuracy  0.8695293146160198
training loss:  0.496316674676213
valid loss  0.5225068840431242  and accuracy  0.8687035507844756
training loss:  0.4939981727583668
valid loss  0.5151858720328569  and accuracy  0.870355078447564
training loss:  0.49510658912239297
valid loss  0.5236858344589945  and accuracy  0.8645747316267548
training loss:  0.49581763103162896
valid loss  0.5284660344375843  and accuracy  0.870355078447564
training loss:  0.49415547110749597
valid loss  0.521234150468368  and accuracy  0.8711808422791082
training loss:  0.49284091073974934
valid loss  0.517886131066709  and accuracy  0.870355078447564
training loss:  0.49089143275193825
valid loss  0.5185942983056375  and accuracy  0.8711808422791082
training loss:  0.49131401832547184
valid loss  0.5240108043964395  and accuracy  0.8687035507844756
training loss:  0.49198738718332263
valid loss  0.5202804910900374  and accuracy  0.870355078447564
training loss:  0.49038093887755124
valid loss  0.5310363483566769  and accuracy  0.870355078447564
training loss:  0.49071685524486897
valid loss  0.5328945460445521  and accuracy  0.8720066061106524
training loss:  0.49102913513930024
valid loss  0.5226438255176182  and accuracy  0.8695293146160198
training loss:  0.48945994653376884
valid loss  0.5270052302681051  and accuracy  0.8662262592898431
training loss:  0.4888158806500618
valid loss  0.5204909566411492  and accuracy  0.870355078447564
training loss:  0.48804965873321543
valid loss  0.5211979792346451  and accuracy  0.8711808422791082
training loss:  0.48959175503961916
valid loss  0.5129413861169193  and accuracy  0.8728323699421965
training loss:  0.488395202563087
valid loss  0.5159790610893022  and accuracy  0.870355078447564
training loss:  0.4876831872214984
valid loss  0.5149279892887585  and accuracy  0.8728323699421965
training loss:  0.48927642712328356
valid loss  0.5233334581975992  and accuracy  0.8728323699421965
training loss:  0.4925981033866458
valid loss  0.5271820409498759  and accuracy  0.8695293146160198
training loss:  0.48969756784044194
valid loss  0.5258632027640213  and accuracy  0.8711808422791082
training loss:  0.4888540044098509
valid loss  0.5204908480632413  and accuracy  0.8720066061106524
training loss:  0.4875399185983206
valid loss  0.5214175938034137  and accuracy  0.8744838976052849
training loss:  0.4893047076716098
valid loss  0.5157728659821187  and accuracy  0.8720066061106524
training loss:  0.48787031583060386
valid loss  0.5136098053512447  and accuracy  0.8711808422791082
training loss:  0.4859649259715166
valid loss  0.5085633544268242  and accuracy  0.8761354252683733
training loss:  0.48613452895032433
valid loss  0.5054055861359682  and accuracy  0.8744838976052849
training loss:  0.4853465832816504
valid loss  0.5113720935204126  and accuracy  0.8728323699421965
training loss:  0.4848168440464355
valid loss  0.5109979713829365  and accuracy  0.8736581337737407
training loss:  0.4851037969589039
valid loss  0.5177534918072398  and accuracy  0.8687035507844756
training loss:  0.48419933864087433
valid loss  0.5093134008440274  and accuracy  0.8728323699421965
training loss:  0.4839253837395041
valid loss  0.5172864452656589  and accuracy  0.8711808422791082
training loss:  0.4831314386997061
valid loss  0.5143205465904255  and accuracy  0.8695293146160198
training loss:  0.4820124855714534
valid loss  0.5128071595823145  and accuracy  0.8728323699421965
training loss:  0.4796461589441049
valid loss  0.5092766837183844  and accuracy  0.8720066061106524
training loss:  0.4829400106937932
valid loss  0.5157667577119035  and accuracy  0.8744838976052849
training loss:  0.4824811556769009
valid loss  0.5117303514460904  and accuracy  0.8728323699421965
training loss:  0.48103584371059516
valid loss  0.5061568625695837  and accuracy  0.8736581337737407
training loss:  0.4799732155419652
valid loss  0.502853927412474  and accuracy  0.8744838976052849
training loss:  0.4797857654719555
valid loss  0.5107838669707222  and accuracy  0.870355078447564
training loss:  0.48054773944139106
valid loss  0.5140815388801963  and accuracy  0.8720066061106524
training loss:  0.47832760370777305
valid loss  0.5238829105969405  and accuracy  0.8728323699421965
training loss:  0.48207071003494584
valid loss  0.5115430337353052  and accuracy  0.8736581337737407
training loss:  0.47946869137552106
valid loss  0.5100672360691131  and accuracy  0.8711808422791082
training loss:  0.4784430104539422
valid loss  0.517808448741694  and accuracy  0.870355078447564
training loss:  0.47726563801301036
valid loss  0.5114306361590605  and accuracy  0.8736581337737407
training loss:  0.47712378111665105
valid loss  0.5163186238778711  and accuracy  0.8695293146160198
training loss:  0.4761095495803069
valid loss  0.5178601500123715  and accuracy  0.8720066061106524
training loss:  0.47677759541308623
valid loss  0.5172220099375919  and accuracy  0.8720066061106524
training loss:  0.4776504245729689
valid loss  0.5100496382284125  and accuracy  0.8736581337737407
training loss:  0.47558661521072915
valid loss  0.511881619051997  and accuracy  0.8744838976052849
training loss:  0.47314952953736394
valid loss  0.5150833269665794  and accuracy  0.8711808422791082
training loss:  0.47437218658741676
valid loss  0.5127196410389403  and accuracy  0.8753096614368291
training loss:  0.4722427035202448
valid loss  0.5134568714977393  and accuracy  0.8736581337737407
training loss:  0.4730103986466555
valid loss  0.5093389193102115  and accuracy  0.8728323699421965
training loss:  0.47457571064260007
valid loss  0.5211663618745339  and accuracy  0.8736581337737407
training loss:  0.4745810185079752
valid loss  0.5115840143649466  and accuracy  0.8769611890999174
training loss:  0.4706639319098677
valid loss  0.5050665367299323  and accuracy  0.8736581337737407
training loss:  0.47108392412600436
valid loss  0.5166154121633998  and accuracy  0.8753096614368291
training loss:  0.4688834705219888
valid loss  0.5092477145813202  and accuracy  0.8736581337737407
training loss:  0.46836701632348055
valid loss  0.5159469258676965  and accuracy  0.8728323699421965
training loss:  0.4691928421316746
valid loss  0.5111403494802612  and accuracy  0.8720066061106524
training loss:  0.4707714077275512
valid loss  0.5070611953636913  and accuracy  0.8753096614368291
training loss:  0.4729527948259901
valid loss  0.5150575025528547  and accuracy  0.8736581337737407
training loss:  0.46923325061263405
valid loss  0.5109824953642293  and accuracy  0.8711808422791082
training loss:  0.46857953431295246
valid loss  0.5054541849627365  and accuracy  0.8687035507844756
training loss:  0.4672203145395806
valid loss  0.5168918238062193  and accuracy  0.8728323699421965
training loss:  0.46909057871095095
valid loss  0.5149845712249875  and accuracy  0.8670520231213873
training loss:  0.4644158789378967
valid loss  0.5159236938622646  and accuracy  0.8744838976052849
training loss:  0.46573442205862353
valid loss  0.528266520710842  and accuracy  0.8662262592898431
training loss:  0.46467366645031327
valid loss  0.5068098665465017  and accuracy  0.8720066061106524
training loss:  0.4655400100095855
valid loss  0.5134264369526744  and accuracy  0.8761354252683733
training loss:  0.46387844162530845
valid loss  0.5152568067035628  and accuracy  0.8711808422791082
training loss:  0.4646479828333052
valid loss  0.5042851636421573  and accuracy  0.8736581337737407
training loss:  0.464476808211415
valid loss  0.5019698385656816  and accuracy  0.8728323699421965
training loss:  0.46418499385290535
valid loss  0.508669595891636  and accuracy  0.8720066061106524
training loss:  0.4633558094355671
valid loss  0.5037100761021395  and accuracy  0.8744838976052849
training loss:  0.4650330436379419
valid loss  0.5028792471554927  and accuracy  0.8736581337737407
training loss:  0.46507921594176327
valid loss  0.5156674883662916  and accuracy  0.8711808422791082
training loss:  0.46653624086239626
valid loss  0.5100564617051456  and accuracy  0.8728323699421965
training loss:  0.4637657146881221
valid loss  0.5083934440532073  and accuracy  0.8720066061106524
training loss:  0.4630193963515343
valid loss  0.5130975004762387  and accuracy  0.8736581337737407
training loss:  0.46236121797479496
valid loss  0.5159432972196111  and accuracy  0.8736581337737407
training loss:  0.4636231751387903
valid loss  0.5008331453573792  and accuracy  0.8720066061106524
training loss:  0.4625998950197309
valid loss  0.5063580027254626  and accuracy  0.8761354252683733
training loss:  0.46325466145739796
valid loss  0.4964857216121538  and accuracy  0.8736581337737407
training loss:  0.46081180964802604
valid loss  0.5011127817542173  and accuracy  0.8761354252683733
training loss:  0.46168123745510453
valid loss  0.49580578785312757  and accuracy  0.8769611890999174
training loss:  0.45970285678386696
valid loss  0.49999733501776106  and accuracy  0.8728323699421965
training loss:  0.459286121174296
valid loss  0.5034481229662009  and accuracy  0.8777869529314616
training loss:  0.4607182170698802
valid loss  0.5107772074190671  and accuracy  0.8662262592898431
training loss:  0.4615199004747799
valid loss  0.5127139047686469  and accuracy  0.8711808422791082
training loss:  0.46132447977246277
valid loss  0.5016557341956778  and accuracy  0.8728323699421965
training loss:  0.4586781803996641
valid loss  0.5144152923077222  and accuracy  0.870355078447564
training loss:  0.45825568729116395
valid loss  0.5081887797911634  and accuracy  0.8711808422791082
training loss:  0.45865178392634326
valid loss  0.4976832700307265  and accuracy  0.8695293146160198
training loss:  0.4571717596696435
valid loss  0.5146311565335382  and accuracy  0.8720066061106524
training loss:  0.4564853310591098
valid loss  0.5031619941174345  and accuracy  0.8736581337737407
training loss:  0.4554503420961579
valid loss  0.5151398291920553  and accuracy  0.8761354252683733
training loss:  0.4563753211034538
valid loss  0.5174713911939317  and accuracy  0.8720066061106524
training loss:  0.4557583168039322
valid loss  0.49571305080251593  and accuracy  0.8728323699421965
training loss:  0.4579586118059291
valid loss  0.5017448472740431  and accuracy  0.8744838976052849
training loss:  0.4605472745438517
valid loss  0.5086379893843936  and accuracy  0.8728323699421965
training loss:  0.45972588449013574
valid loss  0.4965632633174578  and accuracy  0.8744838976052849
training loss:  0.4566480273750716
valid loss  0.49827618632407156  and accuracy  0.8769611890999174
training loss:  0.45494946692329624
valid loss  0.5087184237606953  and accuracy  0.8736581337737407
training loss:  0.45701274462192343
valid loss  0.5036846151241677  and accuracy  0.8720066061106524
training loss:  0.45664937813560685
valid loss  0.5107584053282891  and accuracy  0.8720066061106524
training loss:  0.4569264279251281
valid loss  0.5012399427512773  and accuracy  0.8736581337737407
training loss:  0.4568423333341853
valid loss  0.4967463953892718  and accuracy  0.8753096614368291
training loss:  0.4547890122616001
valid loss  0.5066428101082462  and accuracy  0.8736581337737407
training loss:  0.45461968596092084
valid loss  0.48992088675794276  and accuracy  0.8786127167630058
training loss:  0.45392726615881535
valid loss  0.4937628044952153  and accuracy  0.8720066061106524
training loss:  0.4532243489293129
valid loss  0.507696911557266  and accuracy  0.870355078447564
training loss:  0.4544135318959014
valid loss  0.5040235514241147  and accuracy  0.8720066061106524
training loss:  0.4539932497358387
valid loss  0.49301876222467145  and accuracy  0.8786127167630058
training loss:  0.4517600639584271
valid loss  0.5048413368712567  and accuracy  0.8753096614368291
training loss:  0.45063357228080675
valid loss  0.5014198600606028  and accuracy  0.87943848059455
training loss:  0.4522507422425924
valid loss  0.49444871097433973  and accuracy  0.8736581337737407
training loss:  0.4510921410723037
valid loss  0.5030141827688052  and accuracy  0.8711808422791082
training loss:  0.45237046958727
valid loss  0.5007960143953586  and accuracy  0.8736581337737407
training loss:  0.4505318156156159
valid loss  0.49502378780914674  and accuracy  0.8720066061106524
training loss:  0.4511276993083843
valid loss  0.5009474870983379  and accuracy  0.8744838976052849
training loss:  0.4495728443725123
valid loss  0.4954804517236453  and accuracy  0.8736581337737407
training loss:  0.45007324330913523
valid loss  0.5063896206269867  and accuracy  0.8736581337737407
training loss:  0.44906103940070247
valid loss  0.5010876497092275  and accuracy  0.8736581337737407
training loss:  0.4480014355404447
valid loss  0.48797168869995855  and accuracy  0.8761354252683733
training loss:  0.4492709548093631
valid loss  0.5069316798108949  and accuracy  0.8720066061106524
training loss:  0.45193960591180043
valid loss  0.5101785690306042  and accuracy  0.8744838976052849
training loss:  0.45104984515690166
valid loss  0.503465143882175  and accuracy  0.870355078447564
training loss:  0.45385915175007085
valid loss  0.5003008710560377  and accuracy  0.8761354252683733
training loss:  0.44928283500582433
valid loss  0.5000731339639716  and accuracy  0.8744838976052849
training loss:  0.4521970907437157
valid loss  0.5000669186416488  and accuracy  0.8761354252683733
training loss:  0.44723724775103957
valid loss  0.49722535648590077  and accuracy  0.8753096614368291
training loss:  0.44790986983799763
valid loss  0.5030838487612915  and accuracy  0.8744838976052849
training loss:  0.4502295624457839
valid loss  0.49877522173349176  and accuracy  0.8744838976052849
training loss:  0.45055246195780224
valid loss  0.5034350777194089  and accuracy  0.8753096614368291
training loss:  0.44883659037540624
valid loss  0.4995200936209555  and accuracy  0.8761354252683733
training loss:  0.4472226939431466
valid loss  0.5019909009079023  and accuracy  0.8753096614368291
training loss:  0.4455176314326168
valid loss  0.49143189009706606  and accuracy  0.8769611890999174
training loss:  0.4481515236244928
valid loss  0.5125144413440903  and accuracy  0.8736581337737407
training loss:  0.44982197069898655
valid loss  0.5000529687430226  and accuracy  0.8769611890999174
training loss:  0.4445556592264657
valid loss  0.49839261432603604  and accuracy  0.8769611890999174
training loss:  0.44633905945362506
valid loss  0.4923948871754694  and accuracy  0.8786127167630058
training loss:  0.4460466541343052
valid loss  0.49816348516281533  and accuracy  0.8761354252683733
training loss:  0.44412990033477084
valid loss  0.49283658909364536  and accuracy  0.8777869529314616
training loss:  0.44303932038757615
valid loss  0.5042797501770353  and accuracy  0.8720066061106524
training loss:  0.4447894216758906
valid loss  0.5002167916169942  and accuracy  0.8753096614368291
training loss:  0.4422501517804299
valid loss  0.48750327194061877  and accuracy  0.8769611890999174
training loss:  0.4442305175674156
valid loss  0.4876884459760148  and accuracy  0.8753096614368291
training loss:  0.4447744542616593
valid loss  0.4909520458328537  and accuracy  0.8769611890999174
training loss:  0.44424608512516206
valid loss  0.49218735098838806  and accuracy  0.8753096614368291
training loss:  0.4423911902877241
valid loss  0.5087034084598059  and accuracy  0.8753096614368291
training loss:  0.44272540523523746
valid loss  0.5076637636227218  and accuracy  0.870355078447564
training loss:  0.44176663938991717
valid loss  0.5001513937305164  and accuracy  0.8786127167630058
training loss:  0.4434221926011374
valid loss  0.497042525021327  and accuracy  0.8753096614368291
training loss:  0.44348563285383763
valid loss  0.4932161516438428  and accuracy  0.8761354252683733
training loss:  0.44160492435714715
valid loss  0.49189368704938374  and accuracy  0.8744838976052849
training loss:  0.44211486776625264
valid loss  0.49280374474411265  and accuracy  0.8736581337737407
training loss:  0.44195431841448074
valid loss  0.4912604493705031  and accuracy  0.8761354252683733
training loss:  0.4440563790784674
valid loss  0.4954329135986914  and accuracy  0.8802642444260942
training loss:  0.44557143967789237
valid loss  0.4895980558446574  and accuracy  0.87943848059455
training loss:  0.44202601750870774
valid loss  0.49660971049530067  and accuracy  0.8769611890999174
training loss:  0.44046681212947264
valid loss  0.4913009260923384  and accuracy  0.8761354252683733
training loss:  0.440205928525851
valid loss  0.4954171627636885  and accuracy  0.8744838976052849
training loss:  0.4416730695798732
valid loss  0.5000913437243634  and accuracy  0.8777869529314616
training loss:  0.44127681386874484
valid loss  0.5005933187108311  and accuracy  0.8736581337737407
training loss:  0.4400540846421324
valid loss  0.4904097977204327  and accuracy  0.8753096614368291
training loss:  0.4400293981120716
valid loss  0.5027730809470822  and accuracy  0.8728323699421965
training loss:  0.4410331774023822
valid loss  0.49056815702397405  and accuracy  0.8777869529314616
training loss:  0.4370693283653757
valid loss  0.49931114971096313  and accuracy  0.8786127167630058
training loss:  0.4382573182984991
valid loss  0.49295691957560384  and accuracy  0.8720066061106524
training loss:  0.43929095353399006
valid loss  0.49169180971054277  and accuracy  0.8761354252683733
training loss:  0.43932889176461576
valid loss  0.49710985430009497  and accuracy  0.8736581337737407
training loss:  0.43808621878667614
valid loss  0.49308018956078076  and accuracy  0.8761354252683733
training loss:  0.44178425381957903
valid loss  0.4921449256967061  and accuracy  0.8777869529314616
training loss:  0.4357640360611513
valid loss  0.4913576087805971  and accuracy  0.8761354252683733
training loss:  0.43528071524545237
valid loss  0.4875234816066496  and accuracy  0.8761354252683733
training loss:  0.4363713044822903
valid loss  0.5009358819854053  and accuracy  0.8736581337737407
training loss:  0.4364122513633574
valid loss  0.5068985809480524  and accuracy  0.8695293146160198
training loss:  0.4351869142047734
valid loss  0.4950001031578719  and accuracy  0.8786127167630058
training loss:  0.43555562086028937
valid loss  0.49781942653025996  and accuracy  0.8761354252683733
training loss:  0.4375849087772176
valid loss  0.49539000032166625  and accuracy  0.8769611890999174
training loss:  0.4353114523858032
valid loss  0.5062679795073045  and accuracy  0.8753096614368291
training loss:  0.4353646972481562
valid loss  0.49626197579180675  and accuracy  0.8802642444260942
training loss:  0.43590156203582625
valid loss  0.5031534837209716  and accuracy  0.8753096614368291
training loss:  0.43706314482125824
valid loss  0.49992417486909  and accuracy  0.8753096614368291
training loss:  0.43574123271352694
valid loss  0.4961352851349494  and accuracy  0.8810900082576383
training loss:  0.4371717293978627
valid loss  0.488945744728668  and accuracy  0.87943848059455
training loss:  0.43542777683011163
valid loss  0.49848554178174126  and accuracy  0.8761354252683733
training loss:  0.434325204097609
valid loss  0.4852164931084675  and accuracy  0.8802642444260942
training loss:  0.43478058778709094
valid loss  0.49415322628816627  and accuracy  0.8777869529314616
training loss:  0.4349244523698293
valid loss  0.4865093396578221  and accuracy  0.8786127167630058
training loss:  0.43255566015610813
valid loss  0.5021847082405028  and accuracy  0.8802642444260942
training loss:  0.4342546125373752
valid loss  0.5053862081540704  and accuracy  0.8744838976052849
training loss:  0.43509780242818874
valid loss  0.5011818426685034  and accuracy  0.8728323699421965
training loss:  0.4346802707267621
valid loss  0.4916747723799122  and accuracy  0.8761354252683733

[[  164    31    58     1  1576]
 [   13   215    25     8  1857]
 [   24    26   108     0  1276]
 [    7    35     4    23  1076]
 [   58   120    54    15 43487]]

              precision    recall  f1-score   support

           0       0.62      0.09      0.16      1830
           1       0.50      0.10      0.17      2118
           2       0.43      0.08      0.13      1434
           3       0.49      0.02      0.04      1145
           4       0.88      0.99      0.94     43734

    accuracy                           0.88     50261
   macro avg       0.59      0.26      0.29     50261
weighted avg       0.84      0.88      0.83     50261

Accuracy:  0.875370565647321
Precision_weighted:  0.8351648340047811
Recall_weighted:  0.875370565647321
mcc:  0.22488323115451256
f2:  0.8670226850888165

- train_loop(model, epochs=500, lr=0.001, wd=0.00001)

training loss:  0.9274221899147347
valid loss  0.874366201072759  and accuracy  0.8687035507844756
training loss:  0.5761514774445313
valid loss  0.5695885885460725  and accuracy  0.8687035507844756
training loss:  0.5677289857094231
valid loss  0.5594022664326858  and accuracy  0.8687035507844756
training loss:  0.5634824542202675
valid loss  0.5575581916987059  and accuracy  0.8678777869529315
training loss:  0.5620807541962702
valid loss  0.5499148544548564  and accuracy  0.8678777869529315
training loss:  0.5590161027479406
valid loss  0.5524493148511939  and accuracy  0.8678777869529315
training loss:  0.5563248737732976
valid loss  0.5538451618443433  and accuracy  0.8678777869529315
training loss:  0.554454723810036
valid loss  0.5514487167658834  and accuracy  0.8678777869529315
training loss:  0.5536240171993901
valid loss  0.5479146272657727  and accuracy  0.8687035507844756
training loss:  0.5513022196978496
valid loss  0.5578762083433563  and accuracy  0.8687035507844756
training loss:  0.5504273919935836
valid loss  0.5578897113055852  and accuracy  0.8654004954582989
training loss:  0.5484394616384521
valid loss  0.5476834140071979  and accuracy  0.8687035507844756
training loss:  0.5461557748150069
valid loss  0.5457149620985414  and accuracy  0.8678777869529315
training loss:  0.5452431213437956
valid loss  0.545866678043597  and accuracy  0.8687035507844756
training loss:  0.5452627097852597
valid loss  0.5449388915600607  and accuracy  0.8670520231213873
training loss:  0.5399066221361853
valid loss  0.5411765814615025  and accuracy  0.8662262592898431
training loss:  0.5407648643367446
valid loss  0.5403535488860652  and accuracy  0.8687035507844756
training loss:  0.5386490886499496
valid loss  0.538137218166638  and accuracy  0.8687035507844756
training loss:  0.53792151357958
valid loss  0.537304821435722  and accuracy  0.8687035507844756
training loss:  0.5369453391003612
valid loss  0.5375134058475101  and accuracy  0.8678777869529315
training loss:  0.5410408369493367
valid loss  0.5405339687250549  and accuracy  0.8695293146160198
training loss:  0.5393279615985832
valid loss  0.5469471785669776  and accuracy  0.8687035507844756
training loss:  0.5376976594049687
valid loss  0.5374642471013041  and accuracy  0.8687035507844756
training loss:  0.5371118644797553
valid loss  0.5360855014239531  and accuracy  0.8687035507844756
training loss:  0.5375405118386144
valid loss  0.5481206222962585  and accuracy  0.8678777869529315
training loss:  0.5367418021384738
valid loss  0.5376928537764104  and accuracy  0.8695293146160198
training loss:  0.5331270680672732
valid loss  0.535148915675531  and accuracy  0.8695293146160198
training loss:  0.5322883883483374
valid loss  0.534595759765458  and accuracy  0.8695293146160198
training loss:  0.5326482915847187
valid loss  0.5397851453843341  and accuracy  0.8678777869529315
training loss:  0.5305171036423036
valid loss  0.5320025614526624  and accuracy  0.8687035507844756
training loss:  0.5290675346825927
valid loss  0.5389460664362479  and accuracy  0.8687035507844756
training loss:  0.5286093871076624
valid loss  0.5390437788258496  and accuracy  0.8695293146160198
training loss:  0.5284493564891024
valid loss  0.542181451688809  and accuracy  0.8662262592898431
training loss:  0.532539840288883
valid loss  0.5361973126253542  and accuracy  0.8695293146160198
training loss:  0.5316193430735413
valid loss  0.5387337799656992  and accuracy  0.8695293146160198
training loss:  0.5278726919421279
valid loss  0.532141324264958  and accuracy  0.8695293146160198
training loss:  0.5264058105029176
valid loss  0.5310156148982186  and accuracy  0.8695293146160198
training loss:  0.5238429366395112
valid loss  0.5386388060182309  and accuracy  0.8687035507844756
training loss:  0.5261624125863763
valid loss  0.5281717837299816  and accuracy  0.870355078447564
training loss:  0.524577556979744
valid loss  0.5278905443078127  and accuracy  0.8695293146160198
training loss:  0.5225561558373947
valid loss  0.5303021023629816  and accuracy  0.8687035507844756
training loss:  0.5237258342567036
valid loss  0.5311880831281377  and accuracy  0.8687035507844756
training loss:  0.5218783058960549
valid loss  0.5355606693359174  and accuracy  0.8687035507844756
training loss:  0.522054274355994
valid loss  0.5411003461364475  and accuracy  0.8678777869529315
training loss:  0.5216495093675867
valid loss  0.5346737324354769  and accuracy  0.8687035507844756
training loss:  0.52033014971706
valid loss  0.5304620347172637  and accuracy  0.8695293146160198
training loss:  0.5191537461520656
valid loss  0.5360425188952886  and accuracy  0.8695293146160198
training loss:  0.5206959900567966
valid loss  0.5377416500958601  and accuracy  0.8687035507844756
training loss:  0.5210174816644273
valid loss  0.5398215439376705  and accuracy  0.8695293146160198
training loss:  0.51899822424106
valid loss  0.5303431104570258  and accuracy  0.8687035507844756
training loss:  0.5198172340881774
valid loss  0.5344140134647205  and accuracy  0.8695293146160198
training loss:  0.521086354390953
valid loss  0.5254685902871148  and accuracy  0.8695293146160198
training loss:  0.5188816941200785
valid loss  0.5298855230396784  and accuracy  0.8695293146160198
training loss:  0.5194822200472492
valid loss  0.5292568482415525  and accuracy  0.870355078447564
training loss:  0.5158525938146712
valid loss  0.531510847152313  and accuracy  0.870355078447564
training loss:  0.5183015306374259
valid loss  0.5315851349411278  and accuracy  0.8695293146160198
training loss:  0.5189068526380174
valid loss  0.5371818954446511  and accuracy  0.8695293146160198
training loss:  0.5180095849607054
valid loss  0.5261161949494769  and accuracy  0.8695293146160198
training loss:  0.5197219587339194
valid loss  0.5289781147492612  and accuracy  0.8695293146160198
training loss:  0.5193208507005174
valid loss  0.5336571064101675  and accuracy  0.870355078447564
training loss:  0.5192828069523294
valid loss  0.5301877016080105  and accuracy  0.8695293146160198
training loss:  0.5197304763675633
valid loss  0.5295475856220299  and accuracy  0.8695293146160198
training loss:  0.5170656544160531
valid loss  0.531441233037032  and accuracy  0.8695293146160198
training loss:  0.5156006602332576
valid loss  0.5315587679085704  and accuracy  0.8687035507844756
training loss:  0.515500146121181
valid loss  0.5393012748780475  and accuracy  0.8687035507844756
training loss:  0.5154839063357154
valid loss  0.5388674721257142  and accuracy  0.8695293146160198
training loss:  0.5155817218754192
valid loss  0.5252493628286115  and accuracy  0.8695293146160198
training loss:  0.5120201661516571
valid loss  0.5345989591222868  and accuracy  0.8695293146160198
training loss:  0.5122693828803523
valid loss  0.5353702094119802  and accuracy  0.8711808422791082
training loss:  0.5113222967522567
valid loss  0.5274121129591538  and accuracy  0.8687035507844756
training loss:  0.5125798336744877
valid loss  0.542367907273287  and accuracy  0.870355078447564
training loss:  0.5114143180008432
valid loss  0.5308897007752016  and accuracy  0.8687035507844756
training loss:  0.5136731994166894
valid loss  0.5441092280294165  and accuracy  0.8695293146160198
training loss:  0.5145173564791138
valid loss  0.5336608738568477  and accuracy  0.8695293146160198
training loss:  0.512666647697424
valid loss  0.5322049741504805  and accuracy  0.8678777869529315
training loss:  0.5111354670006142
valid loss  0.5341119042710761  and accuracy  0.8687035507844756
training loss:  0.51033817140302
valid loss  0.538746414501543  and accuracy  0.8687035507844756
training loss:  0.5104895634172053
valid loss  0.5310987745504749  and accuracy  0.8687035507844756
training loss:  0.507121278598925
valid loss  0.5282715520860733  and accuracy  0.870355078447564
training loss:  0.5071187628445906
valid loss  0.5221826002727747  and accuracy  0.8711808422791082
training loss:  0.5069988030346286
valid loss  0.5281091584930924  and accuracy  0.8687035507844756
training loss:  0.5077753764204654
valid loss  0.5194440275110753  and accuracy  0.8695293146160198
training loss:  0.5066528698776293
valid loss  0.531680356708073  and accuracy  0.870355078447564
training loss:  0.5033232146808867
valid loss  0.5411323334538769  and accuracy  0.8687035507844756
training loss:  0.5060916000526735
valid loss  0.5236088194992796  and accuracy  0.870355078447564
training loss:  0.5045774482694464
valid loss  0.5335936046503085  and accuracy  0.870355078447564
training loss:  0.5036327267428801
valid loss  0.5334820820220141  and accuracy  0.8711808422791082
training loss:  0.5048311975103685
valid loss  0.5166379350162951  and accuracy  0.8695293146160198
training loss:  0.502656395941977
valid loss  0.5258064643102279  and accuracy  0.8720066061106524
training loss:  0.5047821033977729
valid loss  0.5328438894301776  and accuracy  0.870355078447564
training loss:  0.5033058435555401
valid loss  0.5268978153645353  and accuracy  0.870355078447564
training loss:  0.5044789176939576
valid loss  0.5232199866057032  and accuracy  0.8720066061106524
training loss:  0.5072849132204914
valid loss  0.5232837774062531  and accuracy  0.8728323699421965
training loss:  0.5062707047712407
valid loss  0.5139138855587638  and accuracy  0.8720066061106524
training loss:  0.504910771230759
valid loss  0.5327434803610691  and accuracy  0.870355078447564
training loss:  0.5039186869394976
valid loss  0.5249592315353312  and accuracy  0.8720066061106524
training loss:  0.5045302646267404
valid loss  0.5264949831068761  and accuracy  0.8678777869529315
training loss:  0.5029375194475171
valid loss  0.5262119947239744  and accuracy  0.8720066061106524
training loss:  0.5011802655301324
valid loss  0.5223931287262285  and accuracy  0.8695293146160198
training loss:  0.5009974296925306
valid loss  0.5234010295863982  and accuracy  0.8711808422791082
training loss:  0.5024101932956946
valid loss  0.5232905857234234  and accuracy  0.8720066061106524
training loss:  0.500040988873122
valid loss  0.520372013874432  and accuracy  0.8687035507844756
training loss:  0.5009687541530009
valid loss  0.5266802598876859  and accuracy  0.8720066061106524
training loss:  0.5007335274197008
valid loss  0.5298796802586903  and accuracy  0.8695293146160198
training loss:  0.5054990384429915
valid loss  0.5319942261540722  and accuracy  0.870355078447564
training loss:  0.5013319273048853
valid loss  0.5203337871171934  and accuracy  0.8711808422791082
training loss:  0.4988446057811323
valid loss  0.525065729949811  and accuracy  0.8720066061106524
training loss:  0.49863371756412744
valid loss  0.5251404091801947  and accuracy  0.8711808422791082
training loss:  0.49885023469363404
valid loss  0.5271867947279176  and accuracy  0.8711808422791082
training loss:  0.49894846589702013
valid loss  0.5228969809144711  and accuracy  0.870355078447564
training loss:  0.49902383928116767
valid loss  0.5328547540132317  and accuracy  0.8670520231213873
training loss:  0.5020784101159753
valid loss  0.5251536017405307  and accuracy  0.870355078447564
training loss:  0.5017421235421904
valid loss  0.526345535214532  and accuracy  0.8687035507844756
training loss:  0.5014219910630042
valid loss  0.524121735678735  and accuracy  0.870355078447564
training loss:  0.5003082700314178
valid loss  0.5274665284806495  and accuracy  0.8695293146160198
training loss:  0.4984893917031224
valid loss  0.5208596358803261  and accuracy  0.8711808422791082
training loss:  0.4972680342236548
valid loss  0.5334071171716852  and accuracy  0.8711808422791082
training loss:  0.4974211289402945
valid loss  0.5316393094846378  and accuracy  0.8678777869529315
training loss:  0.4967186019238051
valid loss  0.5242702975733825  and accuracy  0.870355078447564
training loss:  0.4951162200975001
valid loss  0.5341601702519235  and accuracy  0.8711808422791082
training loss:  0.49913911057256427
valid loss  0.5257401544382314  and accuracy  0.8670520231213873
training loss:  0.4975610937583011
valid loss  0.5233037347000163  and accuracy  0.870355078447564
training loss:  0.4974449178692947
valid loss  0.5274965035629903  and accuracy  0.8711808422791082
training loss:  0.4985647280635852
valid loss  0.5433949914773961  and accuracy  0.8662262592898431
training loss:  0.49836508243168387
valid loss  0.5224404556016898  and accuracy  0.870355078447564
training loss:  0.4969991148374878
valid loss  0.5266755541231296  and accuracy  0.870355078447564
training loss:  0.4962325733530283
valid loss  0.520223234577872  and accuracy  0.870355078447564
training loss:  0.4970242390592722
valid loss  0.5239625492438514  and accuracy  0.870355078447564
training loss:  0.4967057060987816
valid loss  0.5240628950168632  and accuracy  0.8720066061106524
training loss:  0.49399600836326035
valid loss  0.527965095395593  and accuracy  0.8687035507844756
training loss:  0.49707057786698994
valid loss  0.5270577730667877  and accuracy  0.8695293146160198
training loss:  0.4949353481933887
valid loss  0.5238380826129881  and accuracy  0.870355078447564
training loss:  0.4934981720490413
valid loss  0.5207898082022426  and accuracy  0.8695293146160198
training loss:  0.4932831449827926
valid loss  0.5196777161835247  and accuracy  0.8711808422791082
training loss:  0.492825957934726
valid loss  0.5181978776571083  and accuracy  0.8720066061106524
training loss:  0.4912410242620376
valid loss  0.5151724074125094  and accuracy  0.870355078447564
training loss:  0.491702624202302
valid loss  0.5227195693873651  and accuracy  0.8711808422791082
training loss:  0.49327762576876977
valid loss  0.5218759315452922  and accuracy  0.8678777869529315
training loss:  0.49242138420335596
valid loss  0.5212165137443732  and accuracy  0.8687035507844756
training loss:  0.49063993728453237
valid loss  0.5155313725711687  and accuracy  0.8711808422791082
training loss:  0.49166155841210124
valid loss  0.5238978942990008  and accuracy  0.8687035507844756
training loss:  0.49252201525775513
valid loss  0.518709540022591  and accuracy  0.8687035507844756
training loss:  0.49051279118064517
valid loss  0.5162668869477672  and accuracy  0.8711808422791082
training loss:  0.48927767349579065
valid loss  0.5126444089028778  and accuracy  0.8728323699421965
training loss:  0.49009869535845885
valid loss  0.5187419593875611  and accuracy  0.8678777869529315
training loss:  0.4899080837541415
valid loss  0.517299839502322  and accuracy  0.8720066061106524
training loss:  0.4887172276389113
valid loss  0.5390775736750502  and accuracy  0.8695293146160198
training loss:  0.49245756434243243
valid loss  0.5244830413163939  and accuracy  0.870355078447564
training loss:  0.4901490038787799
valid loss  0.5213337763344129  and accuracy  0.870355078447564
training loss:  0.4917962330897291
valid loss  0.5145857984324707  and accuracy  0.8711808422791082
training loss:  0.48676936225513523
valid loss  0.5198816227873527  and accuracy  0.8711808422791082
training loss:  0.48600882077917545
valid loss  0.5157835290611135  and accuracy  0.870355078447564
training loss:  0.4847258324190397
valid loss  0.5177493390714109  and accuracy  0.8695293146160198
training loss:  0.48618687483053497
valid loss  0.5187980006983416  and accuracy  0.8695293146160198
training loss:  0.4844117201092304
valid loss  0.522434131799898  and accuracy  0.8687035507844756
training loss:  0.48615065225099247
valid loss  0.5223654175375632  and accuracy  0.8695293146160198
training loss:  0.48481755775582297
valid loss  0.5117943848488647  and accuracy  0.870355078447564
training loss:  0.48537703592159104
valid loss  0.5091778524004655  and accuracy  0.8720066061106524
training loss:  0.4848458853763733
valid loss  0.5147239858212301  and accuracy  0.8711808422791082
training loss:  0.4852549566275026
valid loss  0.5154515278427981  and accuracy  0.8720066061106524
training loss:  0.48809128139647956
valid loss  0.5175603695342995  and accuracy  0.8695293146160198
training loss:  0.4875265158409518
valid loss  0.5217172974500648  and accuracy  0.8720066061106524
training loss:  0.4865833317548843
valid loss  0.5201773124229406  and accuracy  0.8695293146160198
training loss:  0.48346809659247936
valid loss  0.5141218652368872  and accuracy  0.8711808422791082
training loss:  0.48101829885998987
valid loss  0.5165122320197221  and accuracy  0.870355078447564
training loss:  0.48292255981750726
valid loss  0.51418231967852  and accuracy  0.8720066061106524
training loss:  0.4819227485172454
valid loss  0.5176844854772239  and accuracy  0.8720066061106524
training loss:  0.48299735870871036
valid loss  0.5128473408157229  and accuracy  0.8711808422791082
training loss:  0.48390533817091735
valid loss  0.5198066980261925  and accuracy  0.8736581337737407
training loss:  0.48298495238086236
valid loss  0.5161722503744011  and accuracy  0.8736581337737407
training loss:  0.48288257651286365
valid loss  0.520790487700115  and accuracy  0.870355078447564
training loss:  0.481819107639731
valid loss  0.523693016203448  and accuracy  0.870355078447564
training loss:  0.48202529933117744
valid loss  0.5192914582991974  and accuracy  0.8711808422791082
training loss:  0.48044697655353363
valid loss  0.513730615008281  and accuracy  0.8728323699421965
training loss:  0.4809949819923879
valid loss  0.5149395138049894  and accuracy  0.8720066061106524
training loss:  0.4839008539698171
valid loss  0.5195282141498846  and accuracy  0.870355078447564
training loss:  0.4837176945860173
valid loss  0.5158302104246705  and accuracy  0.8720066061106524
training loss:  0.47985023233625085
valid loss  0.515230146299011  and accuracy  0.8728323699421965
training loss:  0.4798172700915264
valid loss  0.5192147198046267  and accuracy  0.8728323699421965
training loss:  0.48216966567495145
valid loss  0.5144731152855001  and accuracy  0.8720066061106524
training loss:  0.47888776451375387
valid loss  0.5217181108492057  and accuracy  0.8728323699421965
training loss:  0.4785786744642871
valid loss  0.5172940718054673  and accuracy  0.8711808422791082
training loss:  0.47923087049797797
valid loss  0.5196857266835592  and accuracy  0.8720066061106524
training loss:  0.4783738397291043
valid loss  0.514132988787603  and accuracy  0.8736581337737407
training loss:  0.479668802754002
valid loss  0.5213446868637394  and accuracy  0.8728323699421965
training loss:  0.4768971995182432
valid loss  0.520017720174435  and accuracy  0.8695293146160198
training loss:  0.47749404113854943
valid loss  0.5150240022323035  and accuracy  0.8711808422791082
training loss:  0.4775236952866022
valid loss  0.5206400297772874  and accuracy  0.8720066061106524
training loss:  0.47635709413677846
valid loss  0.5174256997055302  and accuracy  0.870355078447564
training loss:  0.47666268469825596
valid loss  0.5144446026579985  and accuracy  0.8711808422791082
training loss:  0.4766036227742535
valid loss  0.5303097705423684  and accuracy  0.8695293146160198
training loss:  0.47841024078110816
valid loss  0.5181569883785988  and accuracy  0.8678777869529315
training loss:  0.47988831045411334
valid loss  0.5229457055213135  and accuracy  0.8711808422791082
training loss:  0.47580104225437564
valid loss  0.519544389895621  and accuracy  0.8695293146160198
training loss:  0.47841815971793983
valid loss  0.5169810245688743  and accuracy  0.870355078447564
training loss:  0.47742219335170216
valid loss  0.5240960019270711  and accuracy  0.870355078447564
training loss:  0.4757676965951088
valid loss  0.5208866301397959  and accuracy  0.8720066061106524
training loss:  0.47738072697062817
valid loss  0.5263262988367324  and accuracy  0.8662262592898431
training loss:  0.4760343385811146
valid loss  0.5152914726123842  and accuracy  0.8711808422791082
training loss:  0.47586655704255093
valid loss  0.5110335372677527  and accuracy  0.870355078447564
training loss:  0.4750594209865434
valid loss  0.5118396253402719  and accuracy  0.8744838976052849
training loss:  0.47445439193082944
valid loss  0.5152850616677156  and accuracy  0.8695293146160198
training loss:  0.4724570598832504
valid loss  0.5131119300421361  and accuracy  0.8687035507844756
training loss:  0.4746656146965084
valid loss  0.5147002012546549  and accuracy  0.8695293146160198
training loss:  0.4733590317729071
valid loss  0.507846711146546  and accuracy  0.8753096614368291
training loss:  0.47419614150812434
valid loss  0.512464439864005  and accuracy  0.870355078447564
training loss:  0.4725400029236117
valid loss  0.5145999296552184  and accuracy  0.8728323699421965
training loss:  0.4718528145741132
valid loss  0.5043994648755827  and accuracy  0.8736581337737407
training loss:  0.4734886923539471
valid loss  0.5165966848368294  and accuracy  0.8720066061106524
training loss:  0.46999789457765545
valid loss  0.5114690949855808  and accuracy  0.8736581337737407
training loss:  0.47118479979167294
valid loss  0.5142292426599145  and accuracy  0.870355078447564
training loss:  0.47217160190883917
valid loss  0.516385534742861  and accuracy  0.8695293146160198
training loss:  0.47099204645705844
valid loss  0.5072226936565166  and accuracy  0.8744838976052849
training loss:  0.4712935628881318
valid loss  0.5067394514107487  and accuracy  0.8728323699421965
training loss:  0.4724468061387054
valid loss  0.5152111643114727  and accuracy  0.8711808422791082
training loss:  0.471437620328334
valid loss  0.5078688497685087  and accuracy  0.8711808422791082
training loss:  0.47231302773391787
valid loss  0.5140428253927081  and accuracy  0.870355078447564
training loss:  0.46996066425806127
valid loss  0.500911313329818  and accuracy  0.8728323699421965
training loss:  0.4703778713390478
valid loss  0.5088511514427443  and accuracy  0.8744838976052849
training loss:  0.47054204580499437
valid loss  0.514006861766245  and accuracy  0.870355078447564
training loss:  0.46859844990755
valid loss  0.5089062981424403  and accuracy  0.8711808422791082
training loss:  0.4703586691411259
valid loss  0.5095065674390014  and accuracy  0.8711808422791082
training loss:  0.4677880277166833
valid loss  0.5093002531471379  and accuracy  0.8711808422791082
training loss:  0.46850075793608015
valid loss  0.5045881720488963  and accuracy  0.8744838976052849
training loss:  0.46855612973359584
valid loss  0.5035835816090408  and accuracy  0.8720066061106524
training loss:  0.46704156882935616
valid loss  0.5042457458502394  and accuracy  0.8736581337737407
training loss:  0.4686209473744278
valid loss  0.5130413214496893  and accuracy  0.8711808422791082
training loss:  0.46909888825877566
valid loss  0.5081587972766993  and accuracy  0.8728323699421965
training loss:  0.47103711769056794
valid loss  0.5152924760720831  and accuracy  0.8695293146160198
training loss:  0.467364830472797
valid loss  0.5122828882750551  and accuracy  0.8736581337737407
training loss:  0.46741456601492803
valid loss  0.5138564244171099  and accuracy  0.8695293146160198
training loss:  0.4700543185340444
valid loss  0.503387924637507  and accuracy  0.8711808422791082
training loss:  0.46840357592524917
valid loss  0.5025402103250033  and accuracy  0.8728323699421965
training loss:  0.46652816700926614
valid loss  0.5117633166783708  and accuracy  0.870355078447564
training loss:  0.46782625475866296
valid loss  0.5088969112722269  and accuracy  0.8728323699421965
training loss:  0.46639554349201606
valid loss  0.504573554169728  and accuracy  0.8736581337737407
training loss:  0.466973443887205
valid loss  0.504920708453921  and accuracy  0.8753096614368291
training loss:  0.4648578799257716
valid loss  0.5084771498434806  and accuracy  0.8728323699421965
training loss:  0.46556037438081316
valid loss  0.5104651223717397  and accuracy  0.8720066061106524
training loss:  0.4643939419939539
valid loss  0.510671916380094  and accuracy  0.8720066061106524
training loss:  0.4656458572608941
valid loss  0.5070915737002473  and accuracy  0.8728323699421965
training loss:  0.4657463506093514
valid loss  0.5057523542942831  and accuracy  0.8744838976052849
training loss:  0.4655833422132689
valid loss  0.5119585662120438  and accuracy  0.8695293146160198
training loss:  0.4655539101562859
valid loss  0.4999410174633234  and accuracy  0.8769611890999174
training loss:  0.46518713751345536
valid loss  0.5085369294384705  and accuracy  0.8720066061106524
training loss:  0.4650691979289767
valid loss  0.49641216140852595  and accuracy  0.8761354252683733
training loss:  0.4633793413606258
valid loss  0.5056855893154758  and accuracy  0.8687035507844756
training loss:  0.46493652666445556
valid loss  0.49794494963007263  and accuracy  0.8753096614368291
training loss:  0.4640250900872813
valid loss  0.5022788296348295  and accuracy  0.8720066061106524
training loss:  0.4701653474290917
valid loss  0.5080794628546516  and accuracy  0.8736581337737407
training loss:  0.4705982104078409
valid loss  0.5041143350321631  and accuracy  0.8728323699421965
training loss:  0.469664640069511
valid loss  0.502121393514211  and accuracy  0.8753096614368291
training loss:  0.46555267285711527
valid loss  0.506934535690421  and accuracy  0.8736581337737407
training loss:  0.46964618025254967
valid loss  0.5044682431723045  and accuracy  0.8736581337737407
training loss:  0.4653920177192206
valid loss  0.500446807122447  and accuracy  0.8728323699421965
training loss:  0.4634641101327316
valid loss  0.5072376723135925  and accuracy  0.8728323699421965
training loss:  0.4632076736384739
valid loss  0.5015796921550489  and accuracy  0.8736581337737407
training loss:  0.46614417337190583
valid loss  0.5020494966542396  and accuracy  0.8753096614368291
training loss:  0.4637618989949951
valid loss  0.5073415703462432  and accuracy  0.8744838976052849
training loss:  0.4651775472799561
valid loss  0.513337553605671  and accuracy  0.8769611890999174
training loss:  0.46198390754392676
valid loss  0.5020927049767173  and accuracy  0.8744838976052849
training loss:  0.4614329615584275
valid loss  0.49825213769563065  and accuracy  0.8753096614368291
training loss:  0.4615325649701292
valid loss  0.5021246578159853  and accuracy  0.8761354252683733
training loss:  0.4616537015814405
valid loss  0.5186812428774468  and accuracy  0.8753096614368291
training loss:  0.46365989075904934
valid loss  0.504054677230479  and accuracy  0.8769611890999174
training loss:  0.4640186282251148
valid loss  0.5031356548199862  and accuracy  0.8761354252683733
training loss:  0.46339621321914726
valid loss  0.5136898102394123  and accuracy  0.8736581337737407
training loss:  0.4611595931890487
valid loss  0.5071688919988769  and accuracy  0.8761354252683733
training loss:  0.4620343525367555
valid loss  0.5061386868443792  and accuracy  0.8753096614368291
training loss:  0.45972934967285634
valid loss  0.504727951757776  and accuracy  0.8728323699421965
training loss:  0.46025936610752316
valid loss  0.5055706030223903  and accuracy  0.8761354252683733
training loss:  0.46215251573070393
valid loss  0.5049350718689989  and accuracy  0.8736581337737407
training loss:  0.46302373824592236
valid loss  0.5106462729951746  and accuracy  0.8744838976052849
training loss:  0.46241976888761277
valid loss  0.5087807807028539  and accuracy  0.8736581337737407
training loss:  0.46348671268820385
valid loss  0.499616880932689  and accuracy  0.8744838976052849
training loss:  0.46132890436162705
valid loss  0.4917266142949105  and accuracy  0.8761354252683733
training loss:  0.4615253140448591
valid loss  0.49993279992205164  and accuracy  0.8736581337737407
training loss:  0.4588078086039675
valid loss  0.5051873885187406  and accuracy  0.8744838976052849
training loss:  0.4678576617516126
valid loss  0.5089862830475477  and accuracy  0.8720066061106524
training loss:  0.46447918296477614
valid loss  0.49917329470054067  and accuracy  0.8736581337737407
training loss:  0.4584277110857206
valid loss  0.5048968990561787  and accuracy  0.8736581337737407
training loss:  0.4589715383549067
valid loss  0.49650999778169524  and accuracy  0.8777869529314616
training loss:  0.459772990585071
valid loss  0.5032912083247001  and accuracy  0.8761354252683733
training loss:  0.46025464521161363
valid loss  0.5069115313983574  and accuracy  0.8761354252683733
training loss:  0.45923946286944717
valid loss  0.5073537181764471  and accuracy  0.8777869529314616
training loss:  0.46180136848457015
valid loss  0.5072298900437296  and accuracy  0.8728323699421965
training loss:  0.45737449697000726
valid loss  0.5006949949323589  and accuracy  0.8753096614368291
training loss:  0.45964932164036826
valid loss  0.5097441484276467  and accuracy  0.8720066061106524
training loss:  0.45747188332868344
valid loss  0.5010216232176953  and accuracy  0.8753096614368291
training loss:  0.45704557119982187
valid loss  0.5101226226639689  and accuracy  0.8720066061106524
training loss:  0.45902477574738776
valid loss  0.5030242691937801  and accuracy  0.8753096614368291
training loss:  0.4564456243779103
valid loss  0.5071541524494512  and accuracy  0.8720066061106524
training loss:  0.45614095633954804
valid loss  0.4971105135934201  and accuracy  0.8753096614368291
training loss:  0.4538710913609971
valid loss  0.5005184668435428  and accuracy  0.8761354252683733
training loss:  0.45576309173337404
valid loss  0.497781734861292  and accuracy  0.8761354252683733
training loss:  0.4556012524153985
valid loss  0.5029262112842721  and accuracy  0.8761354252683733
training loss:  0.4553929815579342
valid loss  0.5162670250080716  and accuracy  0.8728323699421965
training loss:  0.45677962782852083
valid loss  0.5010488167417433  and accuracy  0.8777869529314616
training loss:  0.4554671194796951
valid loss  0.49893218684944607  and accuracy  0.8777869529314616
training loss:  0.4543237397946792
valid loss  0.50002954534811  and accuracy  0.8761354252683733
training loss:  0.45542503429986925
valid loss  0.49856190854709664  and accuracy  0.8777869529314616
training loss:  0.45421324832939747
valid loss  0.49731187161879536  and accuracy  0.8769611890999174
training loss:  0.45420889231917516
valid loss  0.49101161388303505  and accuracy  0.8786127167630058
training loss:  0.4594219066469226
valid loss  0.5040697596665752  and accuracy  0.8753096614368291
training loss:  0.4566429635267301
valid loss  0.49932134402477474  and accuracy  0.8777869529314616
training loss:  0.4538995474579988
valid loss  0.5011393780751626  and accuracy  0.8769611890999174
training loss:  0.45593994505901375
valid loss  0.4954344613014618  and accuracy  0.8802642444260942
training loss:  0.45429432822667887
valid loss  0.5017848985191222  and accuracy  0.8786127167630058
training loss:  0.4554631185247501
valid loss  0.5039488793797575  and accuracy  0.8761354252683733
training loss:  0.4538986999082401
valid loss  0.4983882560255506  and accuracy  0.8777869529314616
training loss:  0.4517955373755512
valid loss  0.49539834639929625  and accuracy  0.8769611890999174
training loss:  0.4520053467283132
valid loss  0.499185225374929  and accuracy  0.8769611890999174
training loss:  0.45438809706014255
valid loss  0.49753688259817763  and accuracy  0.8769611890999174
training loss:  0.45485001833553734
valid loss  0.5070223738397083  and accuracy  0.8736581337737407
training loss:  0.4548809076670869
valid loss  0.5085923849896494  and accuracy  0.8777869529314616
training loss:  0.45443399284563507
valid loss  0.5047153476148868  and accuracy  0.8761354252683733
training loss:  0.45704065324791143
valid loss  0.5057556235918224  and accuracy  0.8769611890999174
training loss:  0.45516984643050323
valid loss  0.5030641656193627  and accuracy  0.8777869529314616
training loss:  0.45610367998490825
valid loss  0.5098531462258292  and accuracy  0.8744838976052849
training loss:  0.45764121856477324
valid loss  0.5104525855609349  and accuracy  0.8744838976052849
training loss:  0.4550778763153596
valid loss  0.5025282208088703  and accuracy  0.8761354252683733
training loss:  0.4538842515528959
valid loss  0.5046533399333056  and accuracy  0.8769611890999174
training loss:  0.4552372161907125
valid loss  0.508130690585868  and accuracy  0.8744838976052849
training loss:  0.4533857113189898
valid loss  0.5093029473164572  and accuracy  0.8744838976052849
training loss:  0.45459060600261864
valid loss  0.5052370376374287  and accuracy  0.8777869529314616
training loss:  0.45264801588145287
valid loss  0.5002543884894751  and accuracy  0.8786127167630058
training loss:  0.4522549997755491
valid loss  0.5067596025057414  and accuracy  0.8736581337737407
training loss:  0.45277926975629507
valid loss  0.5040111385722676  and accuracy  0.8728323699421965
training loss:  0.45167523783588387
valid loss  0.4959236310593458  and accuracy  0.8761354252683733
training loss:  0.4510036518215081
valid loss  0.5046215029810205  and accuracy  0.8786127167630058
training loss:  0.4513616443783766
valid loss  0.5048742494634318  and accuracy  0.8753096614368291
training loss:  0.45061804423366436
valid loss  0.503286450264574  and accuracy  0.8744838976052849
training loss:  0.4498734242575995
valid loss  0.5012323832974563  and accuracy  0.8769611890999174
training loss:  0.4489100433116066
valid loss  0.4965453824162188  and accuracy  0.8786127167630058
training loss:  0.44883294465255097
valid loss  0.49929142800566584  and accuracy  0.8761354252683733
training loss:  0.45055425473923283
valid loss  0.4946080872975136  and accuracy  0.87943848059455
training loss:  0.45028562018158175
valid loss  0.4977541413315065  and accuracy  0.8761354252683733
training loss:  0.45019574499003384
valid loss  0.5026901645861216  and accuracy  0.8728323699421965
training loss:  0.45005754354790023
valid loss  0.5029764483774328  and accuracy  0.8753096614368291
training loss:  0.4482919100983631
valid loss  0.4950011390531289  and accuracy  0.8810900082576383
training loss:  0.45208310359525505
valid loss  0.5009067829584505  and accuracy  0.8761354252683733
training loss:  0.4522273930407861
valid loss  0.5125070594245004  and accuracy  0.870355078447564
training loss:  0.4527884075673422
valid loss  0.5013019347318828  and accuracy  0.8810900082576383
training loss:  0.4505097986527808
valid loss  0.4919175579467162  and accuracy  0.8786127167630058
training loss:  0.44847337695219874
valid loss  0.493156789921218  and accuracy  0.8786127167630058
training loss:  0.44936777120356036
valid loss  0.49763547173223643  and accuracy  0.8761354252683733
training loss:  0.44927226220283767
valid loss  0.5077921420852572  and accuracy  0.8761354252683733
training loss:  0.4509956559420784
valid loss  0.4956796570418789  and accuracy  0.8786127167630058
training loss:  0.44909471014158014
valid loss  0.495403041286374  and accuracy  0.8786127167630058
training loss:  0.4498084724282399
valid loss  0.509206913553123  and accuracy  0.8777869529314616
training loss:  0.4480720234678198
valid loss  0.4936035815395667  and accuracy  0.8786127167630058
training loss:  0.44937895014224355
valid loss  0.500153881276963  and accuracy  0.8769611890999174
training loss:  0.4495266790385057
valid loss  0.4920646244095535  and accuracy  0.8769611890999174
training loss:  0.4482269443858812
valid loss  0.4977948821890364  and accuracy  0.8777869529314616
training loss:  0.4498203560473292
valid loss  0.4907352247334629  and accuracy  0.8802642444260942
training loss:  0.44721896566769437
valid loss  0.5031424367309699  and accuracy  0.8786127167630058
training loss:  0.44552249319704207
valid loss  0.4983055596012994  and accuracy  0.8777869529314616
training loss:  0.4448239736745218
valid loss  0.49984186014097454  and accuracy  0.87943848059455
training loss:  0.44823916348084686
valid loss  0.49530157680653225  and accuracy  0.87943848059455
training loss:  0.44764234933635094
valid loss  0.49918699840392877  and accuracy  0.8769611890999174
training loss:  0.44688027091083565
valid loss  0.4965412758727589  and accuracy  0.8769611890999174
training loss:  0.44567141400148835
valid loss  0.49568177504937183  and accuracy  0.87943848059455
training loss:  0.44592034289646293
valid loss  0.49467434743334693  and accuracy  0.8761354252683733
training loss:  0.44561733456413233
valid loss  0.4874026201954565  and accuracy  0.8802642444260942
training loss:  0.44605770074311846
valid loss  0.494651527615444  and accuracy  0.87943848059455
training loss:  0.4459120313289657
valid loss  0.49803557585921787  and accuracy  0.8777869529314616
training loss:  0.4441301976920618
valid loss  0.4942053100178795  and accuracy  0.87943848059455
training loss:  0.44464213981324907
valid loss  0.49323581268627914  and accuracy  0.8786127167630058
training loss:  0.4478015861117941
valid loss  0.49816237561078625  and accuracy  0.8761354252683733
training loss:  0.4440950343669811
valid loss  0.5001653691739902  and accuracy  0.8769611890999174
training loss:  0.4474072488082819
valid loss  0.4914784219814272  and accuracy  0.87943848059455
training loss:  0.4466807949010496
valid loss  0.4929905787762485  and accuracy  0.87943848059455
training loss:  0.44725582433351085
valid loss  0.49905335371598636  and accuracy  0.8761354252683733
training loss:  0.44656396143181093
valid loss  0.4882188527964049  and accuracy  0.8802642444260942
training loss:  0.44440612374463917
valid loss  0.49206120747854454  and accuracy  0.8769611890999174
training loss:  0.4430553519019036
valid loss  0.49277873579674175  and accuracy  0.8761354252683733
training loss:  0.4449884589361161
valid loss  0.48987168696279076  and accuracy  0.8777869529314616
training loss:  0.4435554932919955
valid loss  0.48586696041801763  and accuracy  0.8777869529314616
training loss:  0.44384665325046735
valid loss  0.485131676781778  and accuracy  0.87943848059455
training loss:  0.44464267970198423
valid loss  0.4946720070527862  and accuracy  0.8761354252683733
training loss:  0.4421453978950264
valid loss  0.48933304498059804  and accuracy  0.87943848059455
training loss:  0.4417197377941203
valid loss  0.4887147848513577  and accuracy  0.8786127167630058
training loss:  0.44157151839813763
valid loss  0.5024586775694673  and accuracy  0.8761354252683733
training loss:  0.44315107417919847
valid loss  0.4945087282250087  and accuracy  0.8786127167630058
training loss:  0.44071810369379877
valid loss  0.4993776338933225  and accuracy  0.87943848059455
training loss:  0.44152640222022993
valid loss  0.5064311281006558  and accuracy  0.8753096614368291
training loss:  0.44224541499327
valid loss  0.4992460964338333  and accuracy  0.8736581337737407
training loss:  0.44189860468348335
valid loss  0.5072271623560262  and accuracy  0.8744838976052849
training loss:  0.441545725143411
valid loss  0.49838620532553224  and accuracy  0.8753096614368291
training loss:  0.44169944532724614
valid loss  0.5005297002764796  and accuracy  0.8761354252683733
training loss:  0.44315468323928514
valid loss  0.5083544941897435  and accuracy  0.8720066061106524
training loss:  0.44137842901042385
valid loss  0.5012895584008007  and accuracy  0.8744838976052849
training loss:  0.4452248234705482
valid loss  0.500900876069246  and accuracy  0.8761354252683733
training loss:  0.44376023382804586
valid loss  0.4965735086471359  and accuracy  0.8786127167630058
training loss:  0.4445388762517652
valid loss  0.4968119073170064  and accuracy  0.8769611890999174
training loss:  0.4424244091043842
valid loss  0.4899217243730087  and accuracy  0.8786127167630058
training loss:  0.4407509666166893
valid loss  0.4976323787384443  and accuracy  0.8769611890999174
training loss:  0.4439262647200205
valid loss  0.5183124282358108  and accuracy  0.8720066061106524
training loss:  0.4461488244908989
valid loss  0.515488642783917  and accuracy  0.8761354252683733
training loss:  0.44511668022703244
valid loss  0.5023702461300164  and accuracy  0.8769611890999174
training loss:  0.4427852529370337
valid loss  0.49604857947391284  and accuracy  0.8769611890999174
training loss:  0.44184763933514265
valid loss  0.5013223296103647  and accuracy  0.8753096614368291
training loss:  0.44222097532078725
valid loss  0.5051146759955377  and accuracy  0.8753096614368291
training loss:  0.4421885135325747
valid loss  0.498758936597728  and accuracy  0.8769611890999174
training loss:  0.44390959442591466
valid loss  0.5064086928828307  and accuracy  0.8753096614368291
training loss:  0.4412310359837649
valid loss  0.5079336804756933  and accuracy  0.8786127167630058
training loss:  0.4400950225373579
valid loss  0.4943071031846063  and accuracy  0.8769611890999174
training loss:  0.43991870893214724
valid loss  0.4971116913142626  and accuracy  0.8777869529314616
training loss:  0.43780305545437814
valid loss  0.49097434756777486  and accuracy  0.8753096614368291
training loss:  0.4410565962288153
valid loss  0.4912798594188139  and accuracy  0.8769611890999174
training loss:  0.4379951806951292
valid loss  0.49745499698905093  and accuracy  0.8769611890999174
training loss:  0.4418456140562247
valid loss  0.48667929189250453  and accuracy  0.8777869529314616
training loss:  0.4384143763637078
valid loss  0.506959358096418  and accuracy  0.8777869529314616
training loss:  0.44260807343974323
valid loss  0.4944546212104605  and accuracy  0.8769611890999174
training loss:  0.440980186041139
valid loss  0.4937955724316919  and accuracy  0.8769611890999174
training loss:  0.4376608193183169
valid loss  0.48491047124642167  and accuracy  0.8761354252683733
training loss:  0.43891549147415626
valid loss  0.49917687383395004  and accuracy  0.8777869529314616
training loss:  0.43825127940644165
valid loss  0.4910778753231814  and accuracy  0.8786127167630058
training loss:  0.4372873831207
valid loss  0.4953742318661325  and accuracy  0.8802642444260942
training loss:  0.43755653637571434
valid loss  0.497505968336622  and accuracy  0.8786127167630058
training loss:  0.43798074593490344
valid loss  0.49423124522438333  and accuracy  0.8802642444260942
training loss:  0.44036115075305354
valid loss  0.4994559677350039  and accuracy  0.8777869529314616
training loss:  0.43846216251775805
valid loss  0.4983613434948279  and accuracy  0.8753096614368291
training loss:  0.4392207001923528
valid loss  0.5036217531421576  and accuracy  0.8769611890999174
training loss:  0.4399571372936784
valid loss  0.4976895264512148  and accuracy  0.8777869529314616
training loss:  0.43948344982456133
valid loss  0.4899667109663481  and accuracy  0.87943848059455
training loss:  0.4372727309394416
valid loss  0.4948360695512112  and accuracy  0.87943848059455
training loss:  0.4371912777897789
valid loss  0.49148003657330763  and accuracy  0.8769611890999174
training loss:  0.4384157529801836
valid loss  0.4981230106804609  and accuracy  0.87943848059455
training loss:  0.4363798061706072
valid loss  0.4925520126727078  and accuracy  0.87943848059455
training loss:  0.434337897271215
valid loss  0.4949952557646077  and accuracy  0.87943848059455
training loss:  0.4370508185281003
valid loss  0.4921620308516146  and accuracy  0.87943848059455
training loss:  0.4385271543438042
valid loss  0.49939710344783933  and accuracy  0.8728323699421965
training loss:  0.4369349949443597
valid loss  0.5054714596291202  and accuracy  0.8769611890999174
training loss:  0.43676681151465224
valid loss  0.49180578528407387  and accuracy  0.87943848059455
training loss:  0.4351567474463248
valid loss  0.49355561188387537  and accuracy  0.8802642444260942
training loss:  0.44315969896181884
valid loss  0.49969367194431663  and accuracy  0.87943848059455
training loss:  0.4405751049806589
valid loss  0.5110981401385206  and accuracy  0.8802642444260942
training loss:  0.43949671493978654
valid loss  0.4980700751851158  and accuracy  0.8786127167630058
training loss:  0.43778490313924845
valid loss  0.5010751164422165  and accuracy  0.8769611890999174
training loss:  0.43814979388897624
valid loss  0.5004162296689708  and accuracy  0.8777869529314616
training loss:  0.4369271419148394
valid loss  0.5043382259511829  and accuracy  0.8769611890999174
training loss:  0.4397553379896494
valid loss  0.508454343831608  and accuracy  0.8769611890999174
training loss:  0.4475082648922324
valid loss  0.5236534276941806  and accuracy  0.8736581337737407
training loss:  0.4450088016043701
valid loss  0.5164856721457127  and accuracy  0.8753096614368291
training loss:  0.44166258806256226
valid loss  0.5103705090299113  and accuracy  0.87943848059455
training loss:  0.44087733202781265
valid loss  0.5083785276586216  and accuracy  0.8753096614368291
training loss:  0.4390555467715494
valid loss  0.5073104456473145  and accuracy  0.8761354252683733
training loss:  0.43733670668243846
valid loss  0.5098551973934315  and accuracy  0.8744838976052849
training loss:  0.4350171416931681
valid loss  0.5126871796406368  and accuracy  0.8744838976052849
training loss:  0.437527002526991
valid loss  0.5109535106738474  and accuracy  0.8769611890999174
training loss:  0.4373188076557575
valid loss  0.5015787718022391  and accuracy  0.8777869529314616
training loss:  0.43459772996884094
valid loss  0.5103645331105549  and accuracy  0.8769611890999174
training loss:  0.4413818862128859
valid loss  0.5112463753642769  and accuracy  0.8736581337737407
training loss:  0.4358193450264348
valid loss  0.5075364521374691  and accuracy  0.8761354252683733
training loss:  0.43461949444769377
valid loss  0.5064085088270347  and accuracy  0.8753096614368291
training loss:  0.43744680994951884
valid loss  0.4971276555151904  and accuracy  0.8769611890999174
training loss:  0.433480277565457
valid loss  0.497747222467654  and accuracy  0.87943848059455
training loss:  0.43588129761287125
valid loss  0.5001483218272199  and accuracy  0.8769611890999174
training loss:  0.4348841078002051
valid loss  0.5081177247299034  and accuracy  0.8777869529314616
training loss:  0.4317709249489781
valid loss  0.49566340894368344  and accuracy  0.87943848059455
training loss:  0.43499824355925965
valid loss  0.49990484182446776  and accuracy  0.8761354252683733
training loss:  0.4333766673330104
valid loss  0.49962462062780566  and accuracy  0.8744838976052849
training loss:  0.4390800695143071
valid loss  0.4985947386476247  and accuracy  0.87943848059455
training loss:  0.4363327561872067
valid loss  0.4963936443470609  and accuracy  0.8786127167630058
training loss:  0.43739937692707465
valid loss  0.49658887931180334  and accuracy  0.8753096614368291
training loss:  0.43522252226612756
valid loss  0.513423633147231  and accuracy  0.8761354252683733
training loss:  0.43630579924138185
valid loss  0.5117545583639924  and accuracy  0.8720066061106524
training loss:  0.43510902698084875
valid loss  0.5115691163784014  and accuracy  0.8744838976052849
training loss:  0.4354919254805335
valid loss  0.5007022912102628  and accuracy  0.8810900082576383
training loss:  0.43192684271216075
valid loss  0.5020551602275385  and accuracy  0.8761354252683733
training loss:  0.43443012701393
valid loss  0.5069698239042186  and accuracy  0.87943848059455
training loss:  0.43078507344441647
valid loss  0.504124872011158  and accuracy  0.8786127167630058
training loss:  0.4304830235445419
valid loss  0.49252275914618243  and accuracy  0.8827415359207267
training loss:  0.4305758615295132
valid loss  0.5014128993602944  and accuracy  0.87943848059455
training loss:  0.4334440693046882
valid loss  0.5115465125350496  and accuracy  0.8810900082576383
training loss:  0.43251730997547
valid loss  0.5006354574227904  and accuracy  0.8802642444260942
training loss:  0.43124751414143736
valid loss  0.5063716090483472  and accuracy  0.8761354252683733
training loss:  0.4324341043697764
valid loss  0.5019612593851633  and accuracy  0.8769611890999174
training loss:  0.43197892968906276
valid loss  0.5048085125293936  and accuracy  0.8777869529314616
training loss:  0.4306055275829403
valid loss  0.5088797275878298  and accuracy  0.8777869529314616
training loss:  0.4315166720312479
valid loss  0.5041364901918306  and accuracy  0.8761354252683733
training loss:  0.4335668353026386
valid loss  0.509284965392482  and accuracy  0.8711808422791082
training loss:  0.4356401989993661
valid loss  0.5029538737703807  and accuracy  0.8786127167630058
training loss:  0.4327718603057831
valid loss  0.5048795696578667  and accuracy  0.87943848059455
training loss:  0.43296017877180476
valid loss  0.49147282209797993  and accuracy  0.8769611890999174
training loss:  0.4320206573655786
valid loss  0.4940591581245772  and accuracy  0.8777869529314616
training loss:  0.4294077692330752
valid loss  0.49999378689254836  and accuracy  0.8753096614368291
training loss:  0.43043703389409854
valid loss  0.5008916683328929  and accuracy  0.8777869529314616
training loss:  0.4312237952222843
valid loss  0.49599903265766426  and accuracy  0.87943848059455
training loss:  0.4310972115230788
valid loss  0.5057866714347207  and accuracy  0.8744838976052849
training loss:  0.43130429956817673
valid loss  0.49303452940050185  and accuracy  0.87943848059455
training loss:  0.4300314520368489
valid loss  0.5117272053938282  and accuracy  0.8761354252683733
training loss:  0.4332832993073343
valid loss  0.508417547438186  and accuracy  0.8728323699421965
training loss:  0.4327265089800235
valid loss  0.509579600281798  and accuracy  0.8720066061106524
training loss:  0.4350797374323021
valid loss  0.5276212027307191  and accuracy  0.8744838976052849
training loss:  0.43334423346476597
valid loss  0.5047266260835378  and accuracy  0.8786127167630058
training loss:  0.43299323883211505
valid loss  0.5012997524685154  and accuracy  0.8777869529314616
training loss:  0.4304649740206595
valid loss  0.50168963270085  and accuracy  0.8786127167630058
training loss:  0.429713597527148
valid loss  0.49668685992230666  and accuracy  0.8802642444260942


print(cm)
[[  224    19    49     5  1533]
 [   47   170     8    12  1881]
 [   42    23   107     4  1258]
 [   16    26     3    42  1058]
 [   43    70    19    30 43572]]

              precision    recall  f1-score   support

           0       0.60      0.12      0.20      1830
           1       0.55      0.08      0.14      2118
           2       0.58      0.07      0.13      1434
           3       0.45      0.04      0.07      1145
           4       0.88      1.00      0.94     43734

    accuracy                           0.88     50261
   macro avg       0.61      0.26      0.30     50261
weighted avg       0.84      0.88      0.83     50261

Accuracy:  0.8777183104196097
Precision_weighted:  0.8408929469078197
Recall_weighted:  0.8777183104196097
mcc:  0.24798965272058834
f2:  0.8700974462261866

2) cambio qualche parametro in TabNet
n_d=64, n_a=64, n_shared=3, n_ind=2, n_steps=5, relax=1.2, vbs=128

- train_loop(model, epochs=100, lr=0.001, wd=0.00001)

training loss:  0.7290554948471775
valid loss  0.6601522652006268  and accuracy  0.8827415359207267
training loss:  0.5798778925432425
valid loss  0.540050164517246  and accuracy  0.8819157720891825
training loss:  0.5789326242225323
valid loss  0.5316091255645532  and accuracy  0.8819157720891825
training loss:  0.5728417782477859
valid loss  0.5240583850961397  and accuracy  0.8827415359207267
training loss:  0.5725595630751991
valid loss  0.5253614981394972  and accuracy  0.8827415359207267
training loss:  0.5722810530545148
valid loss  0.514794106945137  and accuracy  0.8835672997522709
training loss:  0.5687666931413176
valid loss  0.519515649407093  and accuracy  0.8835672997522709
training loss:  0.5696959243024603
valid loss  0.5163968728407664  and accuracy  0.8827415359207267
training loss:  0.5677125871724685
valid loss  0.5153503576455088  and accuracy  0.8819157720891825
training loss:  0.5639625587201672
valid loss  0.5203714357438312  and accuracy  0.8835672997522709
training loss:  0.561052246543036
valid loss  0.510056999426652  and accuracy  0.8827415359207267
training loss:  0.5577880336741584
valid loss  0.5108740464111284  and accuracy  0.8827415359207267
training loss:  0.5555946627021257
valid loss  0.5090157350757514  and accuracy  0.8835672997522709
training loss:  0.5566426238609445
valid loss  0.5078890453578025  and accuracy  0.8827415359207267
training loss:  0.556394943911676
valid loss  0.5108680586052965  and accuracy  0.8827415359207267
training loss:  0.5548072674312894
valid loss  0.5076725742818894  and accuracy  0.8827415359207267
training loss:  0.5555514662090156
valid loss  0.5116625076035248  and accuracy  0.8827415359207267
training loss:  0.5554098134138146
valid loss  0.5025666525647425  and accuracy  0.8827415359207267
training loss:  0.5542832802441966
valid loss  0.5080857816360688  and accuracy  0.8827415359207267
training loss:  0.5541013486767066
valid loss  0.5161402095063081  and accuracy  0.8827415359207267
training loss:  0.5514145120767213
valid loss  0.5111535232388018  and accuracy  0.8827415359207267
training loss:  0.5504071498398883
valid loss  0.5055068771421761  and accuracy  0.8827415359207267
training loss:  0.5486155672161734
valid loss  0.5061399529631919  and accuracy  0.8827415359207267
training loss:  0.5496765284166999
valid loss  0.5096082093300256  and accuracy  0.8819157720891825
training loss:  0.5473557019729994
valid loss  0.5045060916049336  and accuracy  0.8827415359207267
training loss:  0.5467600176031989
valid loss  0.5021954551400674  and accuracy  0.8827415359207267
training loss:  0.544968860790745
valid loss  0.5022027178503677  and accuracy  0.8802642444260942
training loss:  0.5441188603811554
valid loss  0.49758333255691434  and accuracy  0.8827415359207267
training loss:  0.543413878907151
valid loss  0.4965788741726013  and accuracy  0.8827415359207267
training loss:  0.5443722218807938
valid loss  0.5039290540923175  and accuracy  0.8819157720891825
training loss:  0.5445193165111606
valid loss  0.5088403469073093  and accuracy  0.8835672997522709
training loss:  0.5469092150687608
valid loss  0.509170124690263  and accuracy  0.8835672997522709
training loss:  0.5433147054943106
valid loss  0.5001809877811829  and accuracy  0.884393063583815
training loss:  0.5422984463033206
valid loss  0.5035681816342445  and accuracy  0.8827415359207267
training loss:  0.5438816639384901
valid loss  0.5003636849999527  and accuracy  0.884393063583815
training loss:  0.5444117737714687
valid loss  0.49954262816344874  and accuracy  0.8835672997522709
training loss:  0.5440022849354161
valid loss  0.4990738691326806  and accuracy  0.8802642444260942
training loss:  0.5431970723173704
valid loss  0.49745669313741064  and accuracy  0.8835672997522709
training loss:  0.5410071271000881
valid loss  0.5000598649167503  and accuracy  0.884393063583815
training loss:  0.5408578668569785
valid loss  0.49576482405650724  and accuracy  0.884393063583815
training loss:  0.5430800289808203
valid loss  0.504046116408782  and accuracy  0.8819157720891825
training loss:  0.5411413181080614
valid loss  0.49203339536558194  and accuracy  0.8827415359207267
training loss:  0.5413095060710953
valid loss  0.4959482491311704  and accuracy  0.8852188274153592
training loss:  0.5385138066542297
valid loss  0.49370050154669437  and accuracy  0.8852188274153592
training loss:  0.5371218978922414
valid loss  0.4895541305929841  and accuracy  0.884393063583815
training loss:  0.5357704312086237
valid loss  0.4913666142007716  and accuracy  0.884393063583815
training loss:  0.5380622044637975
valid loss  0.49951552985425235  and accuracy  0.884393063583815
training loss:  0.5365192125632461
valid loss  0.48961338048922337  and accuracy  0.8852188274153592
training loss:  0.5359303837544439
valid loss  0.49632719107445955  and accuracy  0.8819157720891825
training loss:  0.5356501317850085
valid loss  0.4931163412790275  and accuracy  0.8852188274153592
training loss:  0.5368166073426726
valid loss  0.49390876263650757  and accuracy  0.884393063583815
training loss:  0.5384616127167081
valid loss  0.4953239294588812  and accuracy  0.8827415359207267
training loss:  0.5362673870780621
valid loss  0.49150205431252253  and accuracy  0.884393063583815
training loss:  0.5350945161287908
valid loss  0.4984857335895866  and accuracy  0.8835672997522709
training loss:  0.5337935261105263
valid loss  0.49553954217376833  and accuracy  0.8852188274153592
training loss:  0.5345822693049992
valid loss  0.4920126869351681  and accuracy  0.884393063583815
training loss:  0.5373215600387938
valid loss  0.49399562380611156  and accuracy  0.8827415359207267
training loss:  0.5385152159311927
valid loss  0.4992555078645071  and accuracy  0.8835672997522709
training loss:  0.5376040065039912
valid loss  0.5063870903581357  and accuracy  0.8827415359207267
training loss:  0.5373209994982384
valid loss  0.5061306254269169  and accuracy  0.8835672997522709
training loss:  0.5366952013007221
valid loss  0.4966642214777448  and accuracy  0.8827415359207267
training loss:  0.537757642762654
valid loss  0.5131046777417303  and accuracy  0.8819157720891825
training loss:  0.5463831746596931
valid loss  0.49146300170364504  and accuracy  0.884393063583815
training loss:  0.5386131347622011
valid loss  0.5007462279546566  and accuracy  0.8835672997522709
training loss:  0.5364171252617372
valid loss  0.49360055361376826  and accuracy  0.884393063583815
training loss:  0.5346885605986985
valid loss  0.4966393162355651  and accuracy  0.8827415359207267
training loss:  0.5329201270511783
valid loss  0.4920775229826533  and accuracy  0.8852188274153592
training loss:  0.5323383286593826
valid loss  0.48795110484374443  and accuracy  0.884393063583815
training loss:  0.5303010267364026
valid loss  0.4909708562909228  and accuracy  0.884393063583815
training loss:  0.5323345620460002
valid loss  0.4864542646707335  and accuracy  0.8852188274153592
training loss:  0.5292267512284996
valid loss  0.4871874374855067  and accuracy  0.8860445912469034
training loss:  0.5289158747865484
valid loss  0.49306921940219983  and accuracy  0.884393063583815
training loss:  0.5277354495382306
valid loss  0.48615919699558635  and accuracy  0.8852188274153592
training loss:  0.5261103138152913
valid loss  0.48876374856585025  and accuracy  0.884393063583815
training loss:  0.5263804684378165
valid loss  0.4932473892027637  and accuracy  0.884393063583815
training loss:  0.5277358799229198
valid loss  0.49274468938739313  and accuracy  0.8827415359207267
training loss:  0.524607807233182
valid loss  0.4954265492893663  and accuracy  0.8835672997522709
training loss:  0.5254912844938678
valid loss  0.49527715238827896  and accuracy  0.884393063583815
training loss:  0.5252827819695096
valid loss  0.4878903987681344  and accuracy  0.8852188274153592
training loss:  0.5247113588094522
valid loss  0.4825930924183273  and accuracy  0.8852188274153592
training loss:  0.5226835702490706
valid loss  0.4959831947536138  and accuracy  0.8827415359207267
training loss:  0.5216685636840384
valid loss  0.4836479763321203  and accuracy  0.8852188274153592
training loss:  0.5214093575460782
valid loss  0.49397949139802144  and accuracy  0.8835672997522709
training loss:  0.522723749308837
valid loss  0.49787201830220557  and accuracy  0.8835672997522709
training loss:  0.5218185884108851
valid loss  0.48083881791419575  and accuracy  0.8860445912469034
training loss:  0.5222761923062449
valid loss  0.4868185554871768  and accuracy  0.884393063583815
training loss:  0.5205257653572739
valid loss  0.49562584118445385  and accuracy  0.8835672997522709
training loss:  0.5213238433750473
valid loss  0.4845847257103042  and accuracy  0.8852188274153592
training loss:  0.5205937855292145
valid loss  0.48780696909846993  and accuracy  0.8835672997522709
training loss:  0.5225623645156202
valid loss  0.4916990977934232  and accuracy  0.8835672997522709
training loss:  0.5234758734396832
valid loss  0.5036062598228455  and accuracy  0.8835672997522709
training loss:  0.5215850702865488
valid loss  0.4882658737489352  and accuracy  0.8852188274153592
training loss:  0.5198990955233984
valid loss  0.4868064324143108  and accuracy  0.884393063583815
training loss:  0.5239646975672178
valid loss  0.49735962350342905  and accuracy  0.8827415359207267
training loss:  0.5245343769222967
valid loss  0.48964179851711537  and accuracy  0.8835672997522709
training loss:  0.5251362147118626
valid loss  0.4905487002172714  and accuracy  0.8827415359207267
training loss:  0.5236585353934419
valid loss  0.49435051844988254  and accuracy  0.8819157720891825
training loss:  0.5219156708866136
valid loss  0.49143220431345935  and accuracy  0.8819157720891825
training loss:  0.5215469340490234
valid loss  0.5002715919354452  and accuracy  0.8827415359207267
training loss:  0.5195790543906484
valid loss  0.48973120703468825  and accuracy  0.884393063583815

[[   91     1     5     0  1723]
 [   11    30     1     1  2090]
 [    5     2    12     1  1413]
 [    0     0     1     0  1105]
 [   44    11     3     3 43708]]

              precision    recall  f1-score   support

           0       0.60      0.05      0.09      1820
           1       0.68      0.01      0.03      2133
           2       0.55      0.01      0.02      1433
           3       0.00      0.00      0.00      1106
           4       0.87      1.00      0.93     43769

    accuracy                           0.87     50261
   macro avg       0.54      0.21      0.21     50261
weighted avg       0.83      0.87      0.82     50261


# Per TabNet ogni singolo batch deve essere di lunghezza >= a 128 (che sarebbe il vbs)

- train_loop(model, epochs=100, lr=0.001, wd=0.00001)

training loss:  0.7290554948471775
valid loss  0.6601522652006268  and accuracy  0.8827415359207267
training loss:  0.5798778925432425
valid loss  0.540050164517246  and accuracy  0.8819157720891825
training loss:  0.5789326242225323
valid loss  0.5316091255645532  and accuracy  0.8819157720891825
training loss:  0.5728417782477859
valid loss  0.5240583850961397  and accuracy  0.8827415359207267
training loss:  0.5725595630751991
valid loss  0.5253614981394972  and accuracy  0.8827415359207267
training loss:  0.5722810530545148
valid loss  0.514794106945137  and accuracy  0.8835672997522709
training loss:  0.5687666931413176
valid loss  0.519515649407093  and accuracy  0.8835672997522709
training loss:  0.5696959243024603
valid loss  0.5163968728407664  and accuracy  0.8827415359207267
training loss:  0.5677125871724685
valid loss  0.5153503576455088  and accuracy  0.8819157720891825
training loss:  0.5639625587201672
valid loss  0.5203714357438312  and accuracy  0.8835672997522709
training loss:  0.561052246543036
valid loss  0.510056999426652  and accuracy  0.8827415359207267
training loss:  0.5577880336741584
valid loss  0.5108740464111284  and accuracy  0.8827415359207267
training loss:  0.5555946627021257
valid loss  0.5090157350757514  and accuracy  0.8835672997522709
training loss:  0.5566426238609445
valid loss  0.5078890453578025  and accuracy  0.8827415359207267
training loss:  0.556394943911676
valid loss  0.5108680586052965  and accuracy  0.8827415359207267
training loss:  0.5548072674312894
valid loss  0.5076725742818894  and accuracy  0.8827415359207267
training loss:  0.5555514662090156
valid loss  0.5116625076035248  and accuracy  0.8827415359207267
training loss:  0.5554098134138146
valid loss  0.5025666525647425  and accuracy  0.8827415359207267
training loss:  0.5542832802441966
valid loss  0.5080857816360688  and accuracy  0.8827415359207267
training loss:  0.5541013486767066
valid loss  0.5161402095063081  and accuracy  0.8827415359207267
training loss:  0.5514145120767213
valid loss  0.5111535232388018  and accuracy  0.8827415359207267
training loss:  0.5504071498398883
valid loss  0.5055068771421761  and accuracy  0.8827415359207267
training loss:  0.5486155672161734
valid loss  0.5061399529631919  and accuracy  0.8827415359207267
training loss:  0.5496765284166999
valid loss  0.5096082093300256  and accuracy  0.8819157720891825
training loss:  0.5473557019729994
valid loss  0.5045060916049336  and accuracy  0.8827415359207267
training loss:  0.5467600176031989
valid loss  0.5021954551400674  and accuracy  0.8827415359207267
training loss:  0.544968860790745
valid loss  0.5022027178503677  and accuracy  0.8802642444260942
training loss:  0.5441188603811554
valid loss  0.49758333255691434  and accuracy  0.8827415359207267
training loss:  0.543413878907151
valid loss  0.4965788741726013  and accuracy  0.8827415359207267
training loss:  0.5443722218807938
valid loss  0.5039290540923175  and accuracy  0.8819157720891825
training loss:  0.5445193165111606
valid loss  0.5088403469073093  and accuracy  0.8835672997522709
training loss:  0.5469092150687608
valid loss  0.509170124690263  and accuracy  0.8835672997522709
training loss:  0.5433147054943106
valid loss  0.5001809877811829  and accuracy  0.884393063583815
training loss:  0.5422984463033206
valid loss  0.5035681816342445  and accuracy  0.8827415359207267
training loss:  0.5438816639384901
valid loss  0.5003636849999527  and accuracy  0.884393063583815
training loss:  0.5444117737714687
valid loss  0.49954262816344874  and accuracy  0.8835672997522709
training loss:  0.5440022849354161
valid loss  0.4990738691326806  and accuracy  0.8802642444260942
training loss:  0.5431970723173704
valid loss  0.49745669313741064  and accuracy  0.8835672997522709
training loss:  0.5410071271000881
valid loss  0.5000598649167503  and accuracy  0.884393063583815
training loss:  0.5408578668569785
valid loss  0.49576482405650724  and accuracy  0.884393063583815
training loss:  0.5430800289808203
valid loss  0.504046116408782  and accuracy  0.8819157720891825
training loss:  0.5411413181080614
valid loss  0.49203339536558194  and accuracy  0.8827415359207267
training loss:  0.5413095060710953
valid loss  0.4959482491311704  and accuracy  0.8852188274153592
training loss:  0.5385138066542297
valid loss  0.49370050154669437  and accuracy  0.8852188274153592
training loss:  0.5371218978922414
valid loss  0.4895541305929841  and accuracy  0.884393063583815
training loss:  0.5357704312086237
valid loss  0.4913666142007716  and accuracy  0.884393063583815
training loss:  0.5380622044637975
valid loss  0.49951552985425235  and accuracy  0.884393063583815
training loss:  0.5365192125632461
valid loss  0.48961338048922337  and accuracy  0.8852188274153592
training loss:  0.5359303837544439
valid loss  0.49632719107445955  and accuracy  0.8819157720891825
training loss:  0.5356501317850085
valid loss  0.4931163412790275  and accuracy  0.8852188274153592
training loss:  0.5368166073426726
valid loss  0.49390876263650757  and accuracy  0.884393063583815
training loss:  0.5384616127167081
valid loss  0.4953239294588812  and accuracy  0.8827415359207267
training loss:  0.5362673870780621
valid loss  0.49150205431252253  and accuracy  0.884393063583815
training loss:  0.5350945161287908
valid loss  0.4984857335895866  and accuracy  0.8835672997522709
training loss:  0.5337935261105263
valid loss  0.49553954217376833  and accuracy  0.8852188274153592
training loss:  0.5345822693049992
valid loss  0.4920126869351681  and accuracy  0.884393063583815
training loss:  0.5373215600387938
valid loss  0.49399562380611156  and accuracy  0.8827415359207267
training loss:  0.5385152159311927
valid loss  0.4992555078645071  and accuracy  0.8835672997522709
training loss:  0.5376040065039912
valid loss  0.5063870903581357  and accuracy  0.8827415359207267
training loss:  0.5373209994982384
valid loss  0.5061306254269169  and accuracy  0.8835672997522709
training loss:  0.5366952013007221
valid loss  0.4966642214777448  and accuracy  0.8827415359207267
training loss:  0.537757642762654
valid loss  0.5131046777417303  and accuracy  0.8819157720891825
training loss:  0.5463831746596931
valid loss  0.49146300170364504  and accuracy  0.884393063583815
training loss:  0.5386131347622011
valid loss  0.5007462279546566  and accuracy  0.8835672997522709
training loss:  0.5364171252617372
valid loss  0.49360055361376826  and accuracy  0.884393063583815
training loss:  0.5346885605986985
valid loss  0.4966393162355651  and accuracy  0.8827415359207267
training loss:  0.5329201270511783
valid loss  0.4920775229826533  and accuracy  0.8852188274153592
training loss:  0.5323383286593826
valid loss  0.48795110484374443  and accuracy  0.884393063583815
training loss:  0.5303010267364026
valid loss  0.4909708562909228  and accuracy  0.884393063583815
training loss:  0.5323345620460002
valid loss  0.4864542646707335  and accuracy  0.8852188274153592
training loss:  0.5292267512284996
valid loss  0.4871874374855067  and accuracy  0.8860445912469034
training loss:  0.5289158747865484
valid loss  0.49306921940219983  and accuracy  0.884393063583815
training loss:  0.5277354495382306
valid loss  0.48615919699558635  and accuracy  0.8852188274153592
training loss:  0.5261103138152913
valid loss  0.48876374856585025  and accuracy  0.884393063583815
training loss:  0.5263804684378165
valid loss  0.4932473892027637  and accuracy  0.884393063583815
training loss:  0.5277358799229198
valid loss  0.49274468938739313  and accuracy  0.8827415359207267
training loss:  0.524607807233182
valid loss  0.4954265492893663  and accuracy  0.8835672997522709
training loss:  0.5254912844938678
valid loss  0.49527715238827896  and accuracy  0.884393063583815
training loss:  0.5252827819695096
valid loss  0.4878903987681344  and accuracy  0.8852188274153592
training loss:  0.5247113588094522
valid loss  0.4825930924183273  and accuracy  0.8852188274153592
training loss:  0.5226835702490706
valid loss  0.4959831947536138  and accuracy  0.8827415359207267
training loss:  0.5216685636840384
valid loss  0.4836479763321203  and accuracy  0.8852188274153592
training loss:  0.5214093575460782
valid loss  0.49397949139802144  and accuracy  0.8835672997522709
training loss:  0.522723749308837
valid loss  0.49787201830220557  and accuracy  0.8835672997522709
training loss:  0.5218185884108851
valid loss  0.48083881791419575  and accuracy  0.8860445912469034
training loss:  0.5222761923062449
valid loss  0.4868185554871768  and accuracy  0.884393063583815
training loss:  0.5205257653572739
valid loss  0.49562584118445385  and accuracy  0.8835672997522709
training loss:  0.5213238433750473
valid loss  0.4845847257103042  and accuracy  0.8852188274153592
training loss:  0.5205937855292145
valid loss  0.48780696909846993  and accuracy  0.8835672997522709
training loss:  0.5225623645156202
valid loss  0.4916990977934232  and accuracy  0.8835672997522709
training loss:  0.5234758734396832
valid loss  0.5036062598228455  and accuracy  0.8835672997522709
training loss:  0.5215850702865488
valid loss  0.4882658737489352  and accuracy  0.8852188274153592
training loss:  0.5198990955233984
valid loss  0.4868064324143108  and accuracy  0.884393063583815
training loss:  0.5239646975672178
valid loss  0.49735962350342905  and accuracy  0.8827415359207267
training loss:  0.5245343769222967
valid loss  0.48964179851711537  and accuracy  0.8835672997522709
training loss:  0.5251362147118626
valid loss  0.4905487002172714  and accuracy  0.8827415359207267
training loss:  0.5236585353934419
valid loss  0.49435051844988254  and accuracy  0.8819157720891825
training loss:  0.5219156708866136
valid loss  0.49143220431345935  and accuracy  0.8819157720891825
training loss:  0.5215469340490234
valid loss  0.5002715919354452  and accuracy  0.8827415359207267
training loss:  0.5195790543906484
valid loss  0.48973120703468825  and accuracy  0.884393063583815

[[   91     1     5     0  1723]
 [   11    30     1     1  2090]
 [    5     2    12     1  1413]
 [    0     0     1     0  1105]
 [   44    11     3     3 43708]]


              precision    recall  f1-score   support

           0       0.60      0.05      0.09      1820
           1       0.68      0.01      0.03      2133
           2       0.55      0.01      0.02      1433
           3       0.00      0.00      0.00      1106
           4       0.87      1.00      0.93     43769

    accuracy                           0.87     50261
   macro avg       0.54      0.21      0.21     50261
weighted avg       0.83      0.87      0.82     50261

Accuracy:  0.8722667674737868
Precision_weighted:  0.8269645325873406
Recall_weighted:  0.8722667674737868
mcc:  0.11199200572595706
f2:  0.8628135489816033

TabNet parameters: n_d=64, n_a=64, n_shared=3, n_ind=2, n_steps=5, relax=1.2, vbs=128

- train_loop(model, epochs=200, lr=0.001, wd=0.00001)

training loss:  0.7091630936867028
valid loss  0.7281246777534485  and accuracy  0.8768
training loss:  0.5805442551572119
valid loss  0.5563987298965454  and accuracy  0.8768
training loss:  0.5717036783795061
valid loss  0.5358224237442016  and accuracy  0.8768
training loss:  0.5655733412234627
valid loss  0.5338233475685119  and accuracy  0.8768
training loss:  0.5642667242031002
valid loss  0.5348356225967407  and accuracy  0.8768
training loss:  0.5618046391649262
valid loss  0.5357481456756592  and accuracy  0.876
training loss:  0.5614612017244949
valid loss  0.5342283501625061  and accuracy  0.8768
training loss:  0.5604244594597936
valid loss  0.5272233136177062  and accuracy  0.8772
training loss:  0.5584688604576903
valid loss  0.5254485139846802  and accuracy  0.8768
training loss:  0.5572404945116347
valid loss  0.5279965173721314  and accuracy  0.8756
training loss:  0.5562690206907902
valid loss  0.5275026658058166  and accuracy  0.8764
training loss:  0.5539760974583714
valid loss  0.5263953808784485  and accuracy  0.8768
training loss:  0.5534673618611379
valid loss  0.5263261241912842  and accuracy  0.8768
training loss:  0.5513848329508924
valid loss  0.5251443299293518  and accuracy  0.8768
training loss:  0.5509939033781464
valid loss  0.5265565053939819  and accuracy  0.8768
training loss:  0.549407290873216
valid loss  0.5279276026725769  and accuracy  0.8768
training loss:  0.5486557487367186
valid loss  0.5224381047248841  and accuracy  0.8768
training loss:  0.5481886999091911
valid loss  0.5248788283348084  and accuracy  0.8772
training loss:  0.5483485978951605
valid loss  0.530475874042511  and accuracy  0.8752
training loss:  0.5464759803297532
valid loss  0.518820130443573  and accuracy  0.8768
training loss:  0.5450093041911995
valid loss  0.5239876098632813  and accuracy  0.8768
training loss:  0.5432855960412241
valid loss  0.5180026070594788  and accuracy  0.8776
training loss:  0.5439775530617041
valid loss  0.523455678653717  and accuracy  0.8768
training loss:  0.5446652607103089
valid loss  0.5226432282447815  and accuracy  0.8764
training loss:  0.5435963214941361
valid loss  0.5269813973426819  and accuracy  0.8764
training loss:  0.5449937300466413
valid loss  0.5245224573135376  and accuracy  0.876
training loss:  0.5460957828079236
valid loss  0.5208369039058686  and accuracy  0.8772
training loss:  0.5433756403787252
valid loss  0.5186082482337951  and accuracy  0.8756
training loss:  0.5415574420037581
valid loss  0.5200558449745178  and accuracy  0.8764
training loss:  0.5385915398398037
valid loss  0.5154013954162597  and accuracy  0.8764
training loss:  0.5372878111287377
valid loss  0.5122653797149658  and accuracy  0.8756
training loss:  0.5351907376468281
valid loss  0.5144293305397034  and accuracy  0.8764
training loss:  0.5355464965654179
valid loss  0.5120264162540435  and accuracy  0.8772
training loss:  0.5353663625828984
valid loss  0.515628204202652  and accuracy  0.8764
training loss:  0.5334991804618932
valid loss  0.5186134468078614  and accuracy  0.8768
training loss:  0.5321833545438808
valid loss  0.5083138122558594  and accuracy  0.8768
training loss:  0.5333347624869802
valid loss  0.5148530222892761  and accuracy  0.8764
training loss:  0.5308813974485924
valid loss  0.5137150213718414  and accuracy  0.8764
training loss:  0.5319338250998875
valid loss  0.5111657361507416  and accuracy  0.8776
training loss:  0.5298067136425868
valid loss  0.5141663015842438  and accuracy  0.8764
training loss:  0.5304820710989698
valid loss  0.5073598628044128  and accuracy  0.8764
training loss:  0.5280306036548199
valid loss  0.5086547597885132  and accuracy  0.8776
training loss:  0.5293746699839581
valid loss  0.5145975828170777  and accuracy  0.876
training loss:  0.5280766150260491
valid loss  0.5155208601474762  and accuracy  0.8776
training loss:  0.5269879846317285
valid loss  0.5089991262435913  and accuracy  0.8768
training loss:  0.5280521708317538
valid loss  0.5170209646224976  and accuracy  0.876
training loss:  0.5259003604277095
valid loss  0.5097921854496003  and accuracy  0.8764
training loss:  0.526868821029088
valid loss  0.5099483134269714  and accuracy  0.8756
training loss:  0.5267422463426638
valid loss  0.5170551282405853  and accuracy  0.8752
training loss:  0.5273148375340243
valid loss  0.5109165768623352  and accuracy  0.8764
training loss:  0.5283231298128764
valid loss  0.512102632522583  and accuracy  0.8768
training loss:  0.5280874667055843
valid loss  0.5061675701141357  and accuracy  0.8768
training loss:  0.5256317323076865
valid loss  0.5085337018966675  and accuracy  0.8744
training loss:  0.5268918660118352
valid loss  0.5074796890258789  and accuracy  0.8768
training loss:  0.5235523126133322
valid loss  0.5140394412040711  and accuracy  0.8772
training loss:  0.5219078802583206
valid loss  0.5065100811958313  and accuracy  0.876
training loss:  0.5208530082574842
valid loss  0.502386607837677  and accuracy  0.8772
training loss:  0.5211277324450475
valid loss  0.5009463187217712  and accuracy  0.8776
training loss:  0.5195343429419264
valid loss  0.5010157249450684  and accuracy  0.8776
training loss:  0.5206426074556769
valid loss  0.506191014623642  and accuracy  0.8764
training loss:  0.5180367354571919
valid loss  0.5019299283027648  and accuracy  0.8756
training loss:  0.5174519967992901
valid loss  0.5009253443717957  and accuracy  0.8772
training loss:  0.5173124496062197
valid loss  0.4974969279289246  and accuracy  0.8776
training loss:  0.5165786463012249
valid loss  0.5016980543136597  and accuracy  0.8764
training loss:  0.5218571152519341
valid loss  0.5171210517883301  and accuracy  0.8764
training loss:  0.521667745245162
valid loss  0.5079061967849732  and accuracy  0.876
training loss:  0.5248831260743453
valid loss  0.5320247075080872  and accuracy  0.8724
training loss:  0.5290227627035361
valid loss  0.5115477992534637  and accuracy  0.8768
training loss:  0.5249386592726013
valid loss  0.5040919563293457  and accuracy  0.8776
training loss:  0.5244282551147231
valid loss  0.508847045135498  and accuracy  0.8768
training loss:  0.519659043656322
valid loss  0.5039409524440766  and accuracy  0.878
training loss:  0.5183870823838603
valid loss  0.5069574627876282  and accuracy  0.8772
training loss:  0.5191968946500839
valid loss  0.5016423261642456  and accuracy  0.8776
training loss:  0.519559985688783
valid loss  0.49718166909217837  and accuracy  0.8764
training loss:  0.5185690373531737
valid loss  0.503538072013855  and accuracy  0.8756
training loss:  0.5174001390011467
valid loss  0.5008618690490723  and accuracy  0.8764
training loss:  0.5161715181528823
valid loss  0.4985991988182068  and accuracy  0.8776
training loss:  0.5166907536524225
valid loss  0.4985479384422302  and accuracy  0.8776
training loss:  0.513991741858535
valid loss  0.5040548748016357  and accuracy  0.8772
training loss:  0.5139878442039841
valid loss  0.49613592491149905  and accuracy  0.878
training loss:  0.5137353074410852
valid loss  0.4971705276012421  and accuracy  0.8768
training loss:  0.5129306004075349
valid loss  0.49741121034622193  and accuracy  0.8776
training loss:  0.512310265755933
valid loss  0.4961445170879364  and accuracy  0.8764
training loss:  0.510108089247341
valid loss  0.49296861147880555  and accuracy  0.8772
training loss:  0.5089489150286919
valid loss  0.4936155781269074  and accuracy  0.8776
training loss:  0.5099303945624449
valid loss  0.4974112308502197  and accuracy  0.8772
training loss:  0.5124494713954191
valid loss  0.4938316002845764  and accuracy  0.878
training loss:  0.5122137487633543
valid loss  0.5041436947822571  and accuracy  0.8768
training loss:  0.5150270439871591
valid loss  0.4945907797813415  and accuracy  0.8772
training loss:  0.513598130356327
valid loss  0.49369770011901853  and accuracy  0.878
training loss:  0.5124518784446332
valid loss  0.4979247387886047  and accuracy  0.8764
training loss:  0.5133827331876835
valid loss  0.5011259846687317  and accuracy  0.878
training loss:  0.5155581015937493
valid loss  0.5008165314674378  and accuracy  0.8772
training loss:  0.5176076957129154
valid loss  0.49593510007858277  and accuracy  0.878
training loss:  0.5153894264993556
valid loss  0.4939660590648651  and accuracy  0.8776
training loss:  0.511545441707774
valid loss  0.494850914478302  and accuracy  0.8776
training loss:  0.5128419612120944
valid loss  0.4960332239627838  and accuracy  0.878
training loss:  0.5092761468947233
valid loss  0.49598146009445193  and accuracy  0.8768
training loss:  0.5077658341158574
valid loss  0.49294309587478635  and accuracy  0.8792
training loss:  0.5114162306889417
valid loss  0.4992432902336121  and accuracy  0.878
training loss:  0.5098656651562382
valid loss  0.4950596719741821  and accuracy  0.8776
training loss:  0.5096353119541852
valid loss  0.49176122016906737  and accuracy  0.8784
training loss:  0.5083646027886088
valid loss  0.49462558188438416  and accuracy  0.8784
training loss:  0.5107391089450575
valid loss  0.49765988426208496  and accuracy  0.8768
training loss:  0.5093405588188363
valid loss  0.4989993412971497  and accuracy  0.876
training loss:  0.5085550177436777
valid loss  0.49883259963989257  and accuracy  0.8784
training loss:  0.5048166709129934
valid loss  0.4960729777336121  and accuracy  0.8788
training loss:  0.5049206995884178
valid loss  0.4940338126659393  and accuracy  0.8772
training loss:  0.5075942079425857
valid loss  0.48998322834968566  and accuracy  0.8776
training loss:  0.5066670153009233
valid loss  0.49033843612670897  and accuracy  0.878
training loss:  0.5059221065623477
valid loss  0.48941764369010926  and accuracy  0.8808
training loss:  0.5042985288741401
valid loss  0.5048720965385437  and accuracy  0.8756
training loss:  0.5075464193146033
valid loss  0.49661192569732665  and accuracy  0.878
training loss:  0.51359758091532
valid loss  0.49095340132713317  and accuracy  0.8784
training loss:  0.5147743155808505
valid loss  0.4995260270118713  and accuracy  0.8772
training loss:  0.5136661510671203
valid loss  0.49531082410812377  and accuracy  0.878
training loss:  0.5113383520588004
valid loss  0.49572544450759887  and accuracy  0.8768
training loss:  0.5104189782086729
valid loss  0.4955660125732422  and accuracy  0.8764
training loss:  0.5106869584551609
valid loss  0.4957354308605194  and accuracy  0.8768
training loss:  0.5073878420657248
valid loss  0.4902191105365753  and accuracy  0.8776
training loss:  0.5060355105392098
valid loss  0.4860592176437378  and accuracy  0.8788
training loss:  0.5054614667317376
valid loss  0.48763446884155276  and accuracy  0.8784
training loss:  0.5058873131946103
valid loss  0.4929125545501709  and accuracy  0.8764
training loss:  0.5044490371716881
valid loss  0.4911182071685791  and accuracy  0.8792
training loss:  0.5032783043444456
valid loss  0.4913582457542419  and accuracy  0.878
training loss:  0.5023153268512169
valid loss  0.4888773245334625  and accuracy  0.8788
training loss:  0.5024344707453869
valid loss  0.48790911917686464  and accuracy  0.8796
training loss:  0.5016583435000287
valid loss  0.48783404903411864  and accuracy  0.8788
training loss:  0.5016885520425474
valid loss  0.4906751428604126  and accuracy  0.8796
training loss:  0.5012451734774476
valid loss  0.49329491567611694  and accuracy  0.88
training loss:  0.5052806234439613
valid loss  0.4892789942741394  and accuracy  0.878
training loss:  0.5018704092282945
valid loss  0.4834414562702179  and accuracy  0.8804
training loss:  0.5006319383380801
valid loss  0.48243620471954346  and accuracy  0.8784
training loss:  0.5009688113402682
valid loss  0.4853198244571686  and accuracy  0.8796
training loss:  0.49993898291483996
valid loss  0.48433293390274046  and accuracy  0.8796
training loss:  0.4996005551499538
valid loss  0.48399098424911496  and accuracy  0.8804
training loss:  0.49806707261794775
valid loss  0.48240990715026855  and accuracy  0.88
training loss:  0.5015954671193606
valid loss  0.5274487375259399  and accuracy  0.88
training loss:  0.5046679987979295
valid loss  0.4899048873901367  and accuracy  0.8788
training loss:  0.5020674447917459
valid loss  0.48609114985466  and accuracy  0.8788
training loss:  0.5066022736742668
valid loss  0.5020107468605042  and accuracy  0.8792
training loss:  0.504292280260642
valid loss  0.4867476432800293  and accuracy  0.8804
training loss:  0.4998258672367588
valid loss  0.4878107250213623  and accuracy  0.8804
training loss:  0.49987369584078767
valid loss  0.48865730786323547  and accuracy  0.88
training loss:  0.49592527624350696
valid loss  0.48356972227096556  and accuracy  0.8812
training loss:  0.4955233476070104
valid loss  0.48469800691604614  and accuracy  0.88
training loss:  0.49557558809293173
valid loss  0.4823813763618469  and accuracy  0.8804
training loss:  0.4937909797607754
valid loss  0.4862421938419342  and accuracy  0.8796
training loss:  0.4927606704846099
valid loss  0.484378662109375  and accuracy  0.88
training loss:  0.49127699733778857
valid loss  0.4848350781440735  and accuracy  0.8796
training loss:  0.4932748995234619
valid loss  0.4860525722026825  and accuracy  0.8792
training loss:  0.4927951991657915
valid loss  0.4871712718009949  and accuracy  0.8796
training loss:  0.49502985217862794
valid loss  0.48714311323165893  and accuracy  0.8784
training loss:  0.4938946014472987
valid loss  0.491696657371521  and accuracy  0.8788
training loss:  0.49397065268090024
valid loss  0.48434777235984805  and accuracy  0.8808
training loss:  0.490921263399236
valid loss  0.48932909021377563  and accuracy  0.88
training loss:  0.49904784271665154
valid loss  0.48549642086029055  and accuracy  0.8792
training loss:  0.5041585308523034
valid loss  0.4758465706825256  and accuracy  0.8808
training loss:  0.49708503117313735
valid loss  0.4817275375366211  and accuracy  0.8808
training loss:  0.4952291295955728
valid loss  0.48328557472229006  and accuracy  0.8804
training loss:  0.4962278448756616
valid loss  0.48603152084350587  and accuracy  0.8792
training loss:  0.4951932348498148
valid loss  0.48175843276977537  and accuracy  0.88
training loss:  0.4961815241094011
valid loss  0.48058159294128416  and accuracy  0.8788
training loss:  0.49353358783514256
valid loss  0.4832665391921997  and accuracy  0.8808
training loss:  0.49129120550762667
valid loss  0.482489032125473  and accuracy  0.8796
training loss:  0.4923289340145424
valid loss  0.47901384687423704  and accuracy  0.8808
training loss:  0.4921391836562548
valid loss  0.48063066968917845  and accuracy  0.8816
training loss:  0.48956867432474493
valid loss  0.48195404176712037  and accuracy  0.8808
training loss:  0.48984518059134685
valid loss  0.47289514369964597  and accuracy  0.8816
training loss:  0.4878380995499628
valid loss  0.475621382522583  and accuracy  0.8816
training loss:  0.4900387942691145
valid loss  0.47413776760101317  and accuracy  0.8836
training loss:  0.48788036715045846
valid loss  0.471765328502655  and accuracy  0.8808
training loss:  0.48616343224467945
valid loss  0.47181915040016176  and accuracy  0.8828
training loss:  0.4876045643684253
valid loss  0.47784356536865236  and accuracy  0.8804
training loss:  0.4845797434424236
valid loss  0.47993760533332824  and accuracy  0.8832
training loss:  0.4826374128136403
valid loss  0.46934470472335815  and accuracy  0.8828
training loss:  0.4843166429393455
valid loss  0.4771185919761658  and accuracy  0.8824
training loss:  0.48282720925819933
valid loss  0.476989967250824  and accuracy  0.8836
training loss:  0.48519443375381394
valid loss  0.47208037123680113  and accuracy  0.8824
training loss:  0.4849374457020656
valid loss  0.4765704789161682  and accuracy  0.8804
training loss:  0.48248212872637775
valid loss  0.4757618657588959  and accuracy  0.8824
training loss:  0.48475734886012883
valid loss  0.4698714748382568  and accuracy  0.8804
training loss:  0.4826353948919018
valid loss  0.47580697560310364  and accuracy  0.8816
training loss:  0.48175794059906774
valid loss  0.47345242495536805  and accuracy  0.8836
training loss:  0.48195743271254216
valid loss  0.4731601366996765  and accuracy  0.8808
training loss:  0.4804514837025398
valid loss  0.4725623188972473  and accuracy  0.8808
training loss:  0.4803611713737699
valid loss  0.4676733726024628  and accuracy  0.8828
training loss:  0.4791654766504489
valid loss  0.46820023612976075  and accuracy  0.8812
training loss:  0.4834691431055117
valid loss  0.47231437215805055  and accuracy  0.8824
training loss:  0.4784556438475598
valid loss  0.473996119594574  and accuracy  0.8844
training loss:  0.4802106914408443
valid loss  0.46965644879341123  and accuracy  0.8852
training loss:  0.48005198853299447
valid loss  0.4713958115577698  and accuracy  0.884
training loss:  0.4788792665879331
valid loss  0.4774754937171936  and accuracy  0.8808
training loss:  0.47880775491196903
valid loss  0.48417719984054564  and accuracy  0.8772
training loss:  0.47824518840316954
valid loss  0.4709386477470398  and accuracy  0.8828
training loss:  0.4766660636793229
valid loss  0.47073191380500795  and accuracy  0.8828
training loss:  0.47630740807483746
valid loss  0.4680954514026642  and accuracy  0.8828
training loss:  0.47784587843733617
valid loss  0.4665796670913696  and accuracy  0.8836
training loss:  0.47651949578793207
valid loss  0.4689795522689819  and accuracy  0.8852
training loss:  0.4785541714136325
valid loss  0.4714711133003235  and accuracy  0.8816

[[  147    10    25     6  1639]
 [   12    84    16     5  2029]
 [   33     8    92     4  1336]
 [    2    10     9    20  1078]
 [   91    21    35     8 43541]]

[49]
print(report)
              precision    recall  f1-score   support

           0       0.52      0.08      0.14      1827
           1       0.63      0.04      0.07      2146
           2       0.52      0.06      0.11      1473
           3       0.47      0.02      0.03      1119
           4       0.88      1.00      0.93     43696

    accuracy                           0.87     50261
   macro avg       0.60      0.24      0.26     50261
weighted avg       0.83      0.87      0.82     50261

Accuracy:  0.8731223015857226
Precision_weighted:  0.8341307503640795
Recall_weighted:  0.8731223015857226
mcc:  0.18454535170731245
f2:  0.8650350660838533

- train_loop(model, epochs=500, lr=0.001, wd=0.00001)

training loss:  0.7274337553498733
valid loss  0.706412621307373  and accuracy  0.8768
training loss:  0.5763000804375564
valid loss  0.5691065954208374  and accuracy  0.8744
training loss:  0.5721484072643708
valid loss  0.5436962375640869  and accuracy  0.8748
training loss:  0.5655578630553618
valid loss  0.546797696352005  and accuracy  0.8744
training loss:  0.5612982373940485
valid loss  0.5298771892547608  and accuracy  0.8764
training loss:  0.5603899757466724
valid loss  0.5270706160545349  and accuracy  0.8768
training loss:  0.5568478457093039
valid loss  0.5244071332931518  and accuracy  0.8768
training loss:  0.5559581288840104
valid loss  0.5241614784240722  and accuracy  0.876
training loss:  0.5528499621242734
valid loss  0.527201143169403  and accuracy  0.8764
training loss:  0.5514968743675718
valid loss  0.5293421347141266  and accuracy  0.8768
training loss:  0.5530443666967715
valid loss  0.527559608745575  and accuracy  0.8764
training loss:  0.5511931054656031
valid loss  0.5315995934486389  and accuracy  0.876
training loss:  0.5505567618550568
valid loss  0.5281880190849304  and accuracy  0.876
training loss:  0.5483958014092054
valid loss  0.5252432545661926  and accuracy  0.8768
training loss:  0.5478196961396503
valid loss  0.5199489584922791  and accuracy  0.8768
training loss:  0.5452725811020613
valid loss  0.5184444501876831  and accuracy  0.8756
training loss:  0.5430011049986285
valid loss  0.5175313396453858  and accuracy  0.8768
training loss:  0.5417280267431109
valid loss  0.5171531854152679  and accuracy  0.8768
training loss:  0.5429113087342612
valid loss  0.5177483182907104  and accuracy  0.876
training loss:  0.5429660843744549
valid loss  0.5135990409851074  and accuracy  0.8768
training loss:  0.5422023184734772
valid loss  0.5181005926132202  and accuracy  0.876
training loss:  0.5410336098279586
valid loss  0.5216448694229125  and accuracy  0.8756
training loss:  0.539865921310444
valid loss  0.5195419250965119  and accuracy  0.876
training loss:  0.5411940107892705
valid loss  0.5245218676567077  and accuracy  0.876
training loss:  0.5412219681332459
valid loss  0.5190893533706665  and accuracy  0.876
training loss:  0.5415782146917116
valid loss  0.5192025998115539  and accuracy  0.876
training loss:  0.5455837205027416
valid loss  0.5217889055728913  and accuracy  0.8764
training loss:  0.5442447903367942
valid loss  0.523168617773056  and accuracy  0.8752
training loss:  0.5416086558920254
valid loss  0.5196479894638062  and accuracy  0.876
training loss:  0.5387689496604242
valid loss  0.521046052312851  and accuracy  0.8764
training loss:  0.5401952338378433
valid loss  0.5220934540748596  and accuracy  0.8764
training loss:  0.5404724267459594
valid loss  0.5206057127952576  and accuracy  0.8748
training loss:  0.5386124365693161
valid loss  0.5217853920459747  and accuracy  0.8752
training loss:  0.5369699153069475
valid loss  0.5142520148277283  and accuracy  0.8748
training loss:  0.5352519548119014
valid loss  0.5189371743202209  and accuracy  0.8756
training loss:  0.5381440479551728
valid loss  0.5108167371273041  and accuracy  0.8756
training loss:  0.5359212292099319
valid loss  0.5159706803321839  and accuracy  0.8744
training loss:  0.5335997958878177
valid loss  0.5144514359474183  and accuracy  0.8748
training loss:  0.5325510568554876
valid loss  0.5126073319435119  and accuracy  0.8764
training loss:  0.5335504879903554
valid loss  0.5159911031723022  and accuracy  0.8768
training loss:  0.5317968494828222
valid loss  0.5108507012844086  and accuracy  0.876
training loss:  0.5318517428147334
valid loss  0.5086772414207459  and accuracy  0.8768
training loss:  0.535320663651829
valid loss  0.5100189286231994  and accuracy  0.8768
training loss:  0.5316151826425614
valid loss  0.5133411706924439  and accuracy  0.8748
training loss:  0.5375105460684503
valid loss  0.5212041764259339  and accuracy  0.876
training loss:  0.5378800394147685
valid loss  0.5186961377143859  and accuracy  0.8752
training loss:  0.53903438972069
valid loss  0.5220568649291992  and accuracy  0.8744
training loss:  0.5465672262948961
valid loss  0.5224318183898926  and accuracy  0.876
training loss:  0.5463670427875104
valid loss  0.5165003776550293  and accuracy  0.8768
training loss:  0.5422292116299346
valid loss  0.5178071908950805  and accuracy  0.8756
training loss:  0.5380594359770092
valid loss  0.5155747315883636  and accuracy  0.8768
training loss:  0.538793460248503
valid loss  0.5199402660369873  and accuracy  0.8764
training loss:  0.5384608493578095
valid loss  0.5124869040489197  and accuracy  0.876
training loss:  0.537071117084829
valid loss  0.5196137622356415  and accuracy  0.8764
training loss:  0.5338766897743071
valid loss  0.5207579845428467  and accuracy  0.8756
training loss:  0.5334143040567586
valid loss  0.5172665971755982  and accuracy  0.874
training loss:  0.5315535431331526
valid loss  0.5130780161857605  and accuracy  0.8768
training loss:  0.5329845213610523
valid loss  0.5132442552566528  and accuracy  0.8756
training loss:  0.5332249940939285
valid loss  0.5206853355407715  and accuracy  0.8756
training loss:  0.5419392005682391
valid loss  0.5177619801044464  and accuracy  0.8756
training loss:  0.5388704233632815
valid loss  0.521089147233963  and accuracy  0.8768
training loss:  0.538032951956019
valid loss  0.5197572410106659  and accuracy  0.8768
training loss:  0.5364312429324267
valid loss  0.5141736193656922  and accuracy  0.8768
training loss:  0.5347604943479924
valid loss  0.5169715114593506  and accuracy  0.8764
training loss:  0.531269252140518
valid loss  0.5095786518096924  and accuracy  0.8768
training loss:  0.5302683710353059
valid loss  0.5120440176010131  and accuracy  0.8768
training loss:  0.5282649688784601
valid loss  0.5107556674003602  and accuracy  0.8768
training loss:  0.5287610210365985
valid loss  0.5100364570617676  and accuracy  0.876
training loss:  0.5288927675691282
valid loss  0.5157908581733703  and accuracy  0.876
training loss:  0.5288701187625802
valid loss  0.5205135769844055  and accuracy  0.8756
training loss:  0.5283348677645576
valid loss  0.516879017829895  and accuracy  0.8756
training loss:  0.5274607284883758
valid loss  0.5080548291206359  and accuracy  0.8764
training loss:  0.5333405287521369
valid loss  0.5165797966003418  and accuracy  0.8768
training loss:  0.5326988950047261
valid loss  0.5124958710670471  and accuracy  0.8768
training loss:  0.5331433703152578
valid loss  0.5391299251556396  and accuracy  0.8756
training loss:  0.5303180775151181
valid loss  0.5092525561332703  and accuracy  0.8768
training loss:  0.5280228078465167
valid loss  0.5096109184265136  and accuracy  0.8772
training loss:  0.5287803549862388
valid loss  0.5052091201782226  and accuracy  0.8768
training loss:  0.5276067048860354
valid loss  0.5086099639892578  and accuracy  0.8756
training loss:  0.5263819314626793
valid loss  0.503606201839447  and accuracy  0.8768
training loss:  0.5287608124103578
valid loss  0.5153038449287415  and accuracy  0.8768
training loss:  0.5267314967198587
valid loss  0.5046070468902588  and accuracy  0.8772
training loss:  0.5256829255190327
valid loss  0.5000929779052734  and accuracy  0.8776
training loss:  0.5234747417009057
valid loss  0.5087014025688171  and accuracy  0.8768
training loss:  0.522231163810845
valid loss  0.5044188057899475  and accuracy  0.8768
training loss:  0.5233347732018386
valid loss  0.5064386222362518  and accuracy  0.8764
training loss:  0.5225531922512917
valid loss  0.5012087412834167  and accuracy  0.8772
training loss:  0.5240010723396762
valid loss  0.5093691039085388  and accuracy  0.8764
training loss:  0.523721127713745
valid loss  0.505529128074646  and accuracy  0.8772
training loss:  0.5209537409258448
valid loss  0.500316139125824  and accuracy  0.8768
training loss:  0.5200157659237509
valid loss  0.502759317111969  and accuracy  0.876
training loss:  0.522476063341751
valid loss  0.5060382699489594  and accuracy  0.8748
training loss:  0.5238244227328692
valid loss  0.5085103165626526  and accuracy  0.8756
training loss:  0.5225142824789587
valid loss  0.5099577233314514  and accuracy  0.8764
training loss:  0.5204621158253208
valid loss  0.5037658489227295  and accuracy  0.8776
training loss:  0.5200323947110966
valid loss  0.5087955574512482  and accuracy  0.8772
training loss:  0.5189213277906229
valid loss  0.5001042447090149  and accuracy  0.8768
training loss:  0.5180610342441092
valid loss  0.5046640068054199  and accuracy  0.876
training loss:  0.5204844293482539
valid loss  0.5052210989952087  and accuracy  0.8772
training loss:  0.5221842538172277
valid loss  0.5015423915863038  and accuracy  0.876
training loss:  0.5223428000657003
valid loss  0.5052503335952758  and accuracy  0.8784
training loss:  0.5253528884307823
valid loss  0.5131359747886658  and accuracy  0.876
training loss:  0.5253630907890985
valid loss  0.5097060868740082  and accuracy  0.876
training loss:  0.5243661040536922
valid loss  0.5095065033912659  and accuracy  0.8764
training loss:  0.522522544920744
valid loss  0.5089886518478394  and accuracy  0.876
training loss:  0.5241910397706918
valid loss  0.5090024255752563  and accuracy  0.8776
training loss:  0.521521000077377
valid loss  0.5173886832237243  and accuracy  0.8768
training loss:  0.5244418815951452
valid loss  0.5033382909774781  and accuracy  0.8788
training loss:  0.5206434101515279
valid loss  0.504355501127243  and accuracy  0.8764
training loss:  0.5189151861459965
valid loss  0.4992017023086548  and accuracy  0.878
training loss:  0.5185766087504887
valid loss  0.5004441635131835  and accuracy  0.878
training loss:  0.5179743685714364
valid loss  0.5099625319957733  and accuracy  0.8772
training loss:  0.5196642560575475
valid loss  0.5002177855014801  and accuracy  0.8788
training loss:  0.5158925241561392
valid loss  0.5016849529266357  and accuracy  0.8784
training loss:  0.5151403108234182
valid loss  0.5037089515209198  and accuracy  0.876
training loss:  0.5159876137722277
valid loss  0.4975590173244476  and accuracy  0.8788
training loss:  0.5229993710166445
valid loss  0.5084509597778321  and accuracy  0.876
training loss:  0.52143919098517
valid loss  0.5066418354511261  and accuracy  0.876
training loss:  0.5196955719085994
valid loss  0.5092485524177551  and accuracy  0.8764
training loss:  0.5186257883132602
valid loss  0.503590547323227  and accuracy  0.8768
training loss:  0.5183098871504243
valid loss  0.5040562133789063  and accuracy  0.8772
training loss:  0.5170798959045154
valid loss  0.5055063190460205  and accuracy  0.8756
training loss:  0.5145656370837085
valid loss  0.5007131517887116  and accuracy  0.8788
training loss:  0.5145688347081643
valid loss  0.49947983837127685  and accuracy  0.878
training loss:  0.5119913503352921
valid loss  0.5045578518867493  and accuracy  0.8788
training loss:  0.5113297168134245
valid loss  0.5048239342689514  and accuracy  0.8768
training loss:  0.5116993557268651
valid loss  0.4957453369617462  and accuracy  0.8792
training loss:  0.5110258504673464
valid loss  0.49847744817733763  and accuracy  0.8788
training loss:  0.5103570017383326
valid loss  0.5038566705703735  and accuracy  0.8784
training loss:  0.5130822523954126
valid loss  0.5085378129959106  and accuracy  0.878
training loss:  0.5141548719438077
valid loss  0.5014221596717835  and accuracy  0.88
training loss:  0.5135737585162797
valid loss  0.496582612323761  and accuracy  0.8796
training loss:  0.5122681647289538
valid loss  0.49911165223121645  and accuracy  0.8784
training loss:  0.5135027170580636
valid loss  0.4954904238700867  and accuracy  0.8788
training loss:  0.5125127489642681
valid loss  0.4950649952888489  and accuracy  0.8792
training loss:  0.5102382040103676
valid loss  0.4954404459953308  and accuracy  0.8792
training loss:  0.5097275427837468
valid loss  0.4937186379432678  and accuracy  0.8776
training loss:  0.5107796952552731
valid loss  0.4947600459098816  and accuracy  0.8792
training loss:  0.509473079113505
valid loss  0.49548418741226197  and accuracy  0.88
training loss:  0.5086361837247309
valid loss  0.4927843740463257  and accuracy  0.8792
training loss:  0.506440326716233
valid loss  0.4953964539527893  and accuracy  0.876
training loss:  0.5084725729284973
valid loss  0.5090918254852295  and accuracy  0.8776
training loss:  0.5080451092128977
valid loss  0.5105127467155457  and accuracy  0.8768
training loss:  0.5079844043282807
valid loss  0.4963917901039124  and accuracy  0.8772
training loss:  0.5078065167619555
valid loss  0.49828554515838624  and accuracy  0.8776
training loss:  0.5060127053528375
valid loss  0.5009839595794677  and accuracy  0.8768
training loss:  0.5051343137495082
valid loss  0.5112849102973938  and accuracy  0.8776
training loss:  0.5045626110367639
valid loss  0.5136363665103912  and accuracy  0.8784
training loss:  0.5064681928561161
valid loss  0.4983479934692383  and accuracy  0.8788
training loss:  0.505543767207831
valid loss  0.5025663035392761  and accuracy  0.8776
training loss:  0.5087288252971879
valid loss  0.49767569336891176  and accuracy  0.8796
training loss:  0.5044768532017367
valid loss  0.501823304271698  and accuracy  0.8792
training loss:  0.5022007041640417
valid loss  0.5016830505371094  and accuracy  0.8788
training loss:  0.5015140299421659
valid loss  0.4980386335372925  and accuracy  0.8788
training loss:  0.5019514055108305
valid loss  0.5030327036380767  and accuracy  0.8784
training loss:  0.5086143297566041
valid loss  0.49884940547943113  and accuracy  0.8768
training loss:  0.5147975465560478
valid loss  0.5002839256286621  and accuracy  0.8772
training loss:  0.509541555814807
valid loss  0.4966429794311523  and accuracy  0.8792
training loss:  0.5105049494323219
valid loss  0.496446111536026  and accuracy  0.8792
training loss:  0.5106659464300977
valid loss  0.49798133163452146  and accuracy  0.8796
training loss:  0.5143948372983853
valid loss  0.5010137250900268  and accuracy  0.8772
training loss:  0.5154667208741858
valid loss  0.5048234348297119  and accuracy  0.8788
training loss:  0.5073343909166167
valid loss  0.4973746322154999  and accuracy  0.8792
training loss:  0.5059413130159554
valid loss  0.5023162922859192  and accuracy  0.8796
training loss:  0.5030123850408711
valid loss  0.49898796353340147  and accuracy  0.8788
training loss:  0.5018676022688547
valid loss  0.49755835733413695  and accuracy  0.88
training loss:  0.5010216525152101
valid loss  0.4941206350326538  and accuracy  0.88
training loss:  0.4993342563734582
valid loss  0.4929227409362793  and accuracy  0.8804
training loss:  0.5027593567343414
valid loss  0.4954672485351562  and accuracy  0.88
training loss:  0.4986473685133597
valid loss  0.5004997703552246  and accuracy  0.88
training loss:  0.5014858797367294
valid loss  0.4944409712314606  and accuracy  0.88
training loss:  0.501683353459216
valid loss  0.49160021166801454  and accuracy  0.8792
training loss:  0.5003068837288835
valid loss  0.49084897718429565  and accuracy  0.8804
training loss:  0.4968599444258353
valid loss  0.4898190635681152  and accuracy  0.8796
training loss:  0.4973006128466109
valid loss  0.5017860970497131  and accuracy  0.8788
training loss:  0.4969571406318115
valid loss  0.503068211221695  and accuracy  0.8792
training loss:  0.49677552455034685
valid loss  0.4967592776298523  and accuracy  0.8788
training loss:  0.4965437658667764
valid loss  0.502378307723999  and accuracy  0.8784
training loss:  0.5012427455216796
valid loss  0.5120579990386963  and accuracy  0.8796
training loss:  0.5023244948842418
valid loss  0.4963850085258484  and accuracy  0.8788
training loss:  0.4999164363746867
valid loss  0.5088592246532441  and accuracy  0.8788
training loss:  0.4995191488833084
valid loss  0.5004146293640137  and accuracy  0.8796
training loss:  0.4975452737492732
valid loss  0.5030547812461853  and accuracy  0.8768
training loss:  0.4970561603804929
valid loss  0.5182195311546326  and accuracy  0.8796
training loss:  0.49754723026924197
valid loss  0.5077683828830719  and accuracy  0.8808
training loss:  0.4967646493983628
valid loss  0.4968437570571899  and accuracy  0.8772
training loss:  0.4948681940984486
valid loss  0.5026551517486573  and accuracy  0.8804
training loss:  0.4970215269868298
valid loss  0.5114282814979553  and accuracy  0.8792
training loss:  0.4973133069785995
valid loss  0.49070579624176025  and accuracy  0.8812
training loss:  0.500023794793204
valid loss  0.5062141896247864  and accuracy  0.88
training loss:  0.4981652667374667
valid loss  0.49361985249519347  and accuracy  0.8804
training loss:  0.49529757825174126
valid loss  0.49366883573532105  and accuracy  0.8784
training loss:  0.49988810315204024
valid loss  0.4943745035171509  and accuracy  0.8804
training loss:  0.5039296782495988
valid loss  0.5001528354644775  and accuracy  0.8792
training loss:  0.5015170083772796
valid loss  0.49139682874679563  and accuracy  0.8792
training loss:  0.4991623302999653
valid loss  0.4958864650249481  and accuracy  0.8804
training loss:  0.4970814605653785
valid loss  0.49005380606651305  and accuracy  0.8796
training loss:  0.49572301815502606
valid loss  0.48814424619674684  and accuracy  0.8792
training loss:  0.493997793031897
valid loss  0.48838097443580625  and accuracy  0.88
training loss:  0.49611370337868055
valid loss  0.49485118684768675  and accuracy  0.8788
training loss:  0.4942416513685006
valid loss  0.49687714438438413  and accuracy  0.8784
training loss:  0.49535361991655485
valid loss  0.504987663936615  and accuracy  0.8796
training loss:  0.4949086081642202
valid loss  0.48621197748184203  and accuracy  0.88
training loss:  0.49182942017438624
valid loss  0.49041263875961305  and accuracy  0.8788
training loss:  0.4925135825846463
valid loss  0.49422620639801024  and accuracy  0.8792
training loss:  0.4918358608106872
valid loss  0.493207066822052  and accuracy  0.8812
training loss:  0.4915419744486785
valid loss  0.49964202032089233  and accuracy  0.8808
training loss:  0.4906157239797327
valid loss  0.4874257963180542  and accuracy  0.88
training loss:  0.5028362239425506
valid loss  0.49395636777877805  and accuracy  0.8784
training loss:  0.4952118192286148
valid loss  0.4933975643157959  and accuracy  0.878
training loss:  0.4929437565444103
valid loss  0.48091721801757814  and accuracy  0.88
training loss:  0.4907638974325541
valid loss  0.4860667185783386  and accuracy  0.8788
training loss:  0.4908273262294693
valid loss  0.4861002055644989  and accuracy  0.8804
training loss:  0.48882041323923786
valid loss  0.48042804155349733  and accuracy  0.8812
training loss:  0.49293953369011234
valid loss  0.48805493669509886  and accuracy  0.8792
training loss:  0.4894109530464888
valid loss  0.4883845446586609  and accuracy  0.8788
training loss:  0.4877585334594126
valid loss  0.48885758171081545  and accuracy  0.878
training loss:  0.48762003924179714
valid loss  0.4894933388710022  and accuracy  0.8808
training loss:  0.4871270295363575
valid loss  0.48760952320098877  and accuracy  0.8808
training loss:  0.48768068847544427
valid loss  0.4857576629638672  and accuracy  0.8804
training loss:  0.48726076598542817
valid loss  0.4929401566028595  and accuracy  0.8808
training loss:  0.4869473451346009
valid loss  0.49242804684638974  and accuracy  0.88
training loss:  0.4865827466175584
valid loss  0.5055472415447235  and accuracy  0.8808
training loss:  0.4853500373998479
valid loss  0.4855651354789734  and accuracy  0.88
training loss:  0.4835104688328115
valid loss  0.49647813711166383  and accuracy  0.8808
training loss:  0.49094903346282154
valid loss  0.492995459985733  and accuracy  0.88
training loss:  0.4874611779872696
valid loss  0.4985455966949463  and accuracy  0.8804
training loss:  0.48846539101209274
valid loss  0.4906857021331787  and accuracy  0.8784
training loss:  0.4905382454694815
valid loss  0.5054891561508179  and accuracy  0.8808
training loss:  0.48855132914867433
valid loss  0.4886509356975555  and accuracy  0.8796
training loss:  0.48644221547859995
valid loss  0.4865148078918457  and accuracy  0.8804
training loss:  0.48414748326018825
valid loss  0.49508404712677  and accuracy  0.88
training loss:  0.484594174925007
valid loss  0.4937419587135315  and accuracy  0.8808
training loss:  0.4862671453050233
valid loss  0.4788758739948273  and accuracy  0.8816
training loss:  0.48641997741494747
valid loss  0.4980323783874512  and accuracy  0.88
training loss:  0.4856267783909387
valid loss  0.4904369836807251  and accuracy  0.8784
training loss:  0.4824763155862115
valid loss  0.5015347655296326  and accuracy  0.88
training loss:  0.4806597226828187
valid loss  0.4884161150932312  and accuracy  0.8812
training loss:  0.4802984869859526
valid loss  0.48786287026405334  and accuracy  0.8804
training loss:  0.47967900342278347
valid loss  0.49846494822502135  and accuracy  0.88
training loss:  0.4822810147675238
valid loss  0.4961957089424133  and accuracy  0.8792
training loss:  0.48581331055767374
valid loss  0.482538729095459  and accuracy  0.88
training loss:  0.48208903827060207
valid loss  0.48913851137161257  and accuracy  0.8816
training loss:  0.4809191823205357
valid loss  0.49627833547592165  and accuracy  0.8804
training loss:  0.4890720707587461
valid loss  0.49687427549362184  and accuracy  0.8804
training loss:  0.48612927044656007
valid loss  0.4871429860591888  and accuracy  0.8796
training loss:  0.4822267901158612
valid loss  0.4942645345687866  and accuracy  0.8804
training loss:  0.4831288196134008
valid loss  0.5022179111480712  and accuracy  0.8812
training loss:  0.48300001126038566
valid loss  0.4854573896408081  and accuracy  0.88
training loss:  0.4820153151624766
valid loss  0.4912811330318451  and accuracy  0.8804
training loss:  0.4810878750866582
valid loss  0.4974524302482605  and accuracy  0.8816
training loss:  0.48236400168545085
valid loss  0.49324712114334107  and accuracy  0.8796
training loss:  0.4848780134614788
valid loss  0.49014533133506777  and accuracy  0.8792
training loss:  0.48245725347169083
valid loss  0.48911439304351806  and accuracy  0.8788
training loss:  0.48205454426394834
valid loss  0.5122193431854248  and accuracy  0.8828
training loss:  0.48091633391739735
valid loss  0.48994818778038024  and accuracy  0.8808
training loss:  0.4788053224933407
valid loss  0.4821915267944336  and accuracy  0.88
training loss:  0.47783359942124715
valid loss  0.48131574649810793  and accuracy  0.8796
training loss:  0.47794557154877504
valid loss  0.47049624719619754  and accuracy  0.8812
training loss:  0.4802070489880228
valid loss  0.4814027040481567  and accuracy  0.8816
training loss:  0.47607745137845653
valid loss  0.48321051759719846  and accuracy  0.8808
training loss:  0.47738737863312214
valid loss  0.49433005843162536  and accuracy  0.8796
training loss:  0.4821481802355704
valid loss  0.49141452980041506  and accuracy  0.8804
training loss:  0.4786704345863668
valid loss  0.4924229863166809  and accuracy  0.8788
training loss:  0.4793319009256922
valid loss  0.4920522854804993  and accuracy  0.88
training loss:  0.4864456161981452
valid loss  0.4933683141231537  and accuracy  0.8816
training loss:  0.4804276245522938
valid loss  0.49127024421691895  and accuracy  0.8812
training loss:  0.4810726759222085
valid loss  0.5054124915122986  and accuracy  0.8796
training loss:  0.48110984155480785
valid loss  0.4893302535057068  and accuracy  0.8808
training loss:  0.4781227177710988
valid loss  0.48069057273864746  and accuracy  0.8808
training loss:  0.4773165195032181
valid loss  0.4923494752883911  and accuracy  0.8808
training loss:  0.4771121111547128
valid loss  0.483357323551178  and accuracy  0.8808
training loss:  0.4759060690550748
valid loss  0.49823089714050295  and accuracy  0.8808
training loss:  0.4780329129104838
valid loss  0.48186087255477905  and accuracy  0.8808
training loss:  0.4767346501550083
valid loss  0.4954075610637665  and accuracy  0.8776
training loss:  0.48064291924487806
valid loss  0.4836073838710785  and accuracy  0.8812
training loss:  0.47647895238906696
valid loss  0.488443816280365  and accuracy  0.8816
training loss:  0.47723388141723133
valid loss  0.48384380860328674  and accuracy  0.8804
training loss:  0.4748073507593305
valid loss  0.49628753080368043  and accuracy  0.88
training loss:  0.47491163816084614
valid loss  0.4915100546360016  and accuracy  0.8812
training loss:  0.4751092094572345
valid loss  0.49800718169212344  and accuracy  0.8804
training loss:  0.47331236862457376
valid loss  0.4884825708389282  and accuracy  0.8812
training loss:  0.47424779161935676
valid loss  0.5000276505470276  and accuracy  0.8816
training loss:  0.4741787333883832
valid loss  0.479120774269104  and accuracy  0.8828
training loss:  0.4737660022237193
valid loss  0.4868070472240448  and accuracy  0.8808
training loss:  0.4734667192651599
valid loss  0.4904868172645569  and accuracy  0.882
training loss:  0.47458543645676654
valid loss  0.4883049096107483  and accuracy  0.8812
training loss:  0.47618245595264275
valid loss  0.48608975706100466  and accuracy  0.8804
training loss:  0.4745096176912637
valid loss  0.48086726112365724  and accuracy  0.8804
training loss:  0.471544443243113
valid loss  0.4816322939872742  and accuracy  0.8824
training loss:  0.4734426227446577
valid loss  0.49295073676109313  and accuracy  0.8808
training loss:  0.4695905883308232
valid loss  0.4811965063095093  and accuracy  0.882
training loss:  0.47024313910322973
valid loss  0.4875297034263611  and accuracy  0.8816
training loss:  0.47092547985776584
valid loss  0.488565976524353  and accuracy  0.8828
training loss:  0.46820075298274183
valid loss  0.4876915503025055  and accuracy  0.8808
training loss:  0.4702802809938115
valid loss  0.48822047810554503  and accuracy  0.882
training loss:  0.47011878424553416
valid loss  0.4894212912559509  and accuracy  0.882
training loss:  0.47203768535674717
valid loss  0.4917316451072693  and accuracy  0.8824
training loss:  0.47019121087775556
valid loss  0.49536721868515016  and accuracy  0.8824
training loss:  0.4696004840796517
valid loss  0.4886901649475098  and accuracy  0.8824
training loss:  0.46925085498459973
valid loss  0.48952534408569337  and accuracy  0.8816
training loss:  0.46878138550961235
valid loss  0.479887606048584  and accuracy  0.8808
training loss:  0.4723118266667952
valid loss  0.4893722858428955  and accuracy  0.88
training loss:  0.46858785396045577
valid loss  0.49428293948173524  and accuracy  0.8804
training loss:  0.4693372400761449
valid loss  0.48348241486549376  and accuracy  0.8808
training loss:  0.4681738324600648
valid loss  0.48894806485176084  and accuracy  0.8824
training loss:  0.46617304865439335
valid loss  0.48982649002075196  and accuracy  0.8804
training loss:  0.46799258121493675
valid loss  0.49293960018157956  and accuracy  0.8804
training loss:  0.46625159559537416
valid loss  0.5011188401222229  and accuracy  0.8828
training loss:  0.4692659233982239
valid loss  0.47941104679107666  and accuracy  0.8832
training loss:  0.46793535282863447
valid loss  0.49000689940452574  and accuracy  0.882
training loss:  0.4653579697936424
valid loss  0.4972286853313446  and accuracy  0.8824
training loss:  0.4628177078025824
valid loss  0.46881995611190797  and accuracy  0.8844
training loss:  0.4646414451263658
valid loss  0.48779106845855713  and accuracy  0.8804
training loss:  0.46525615717298424
valid loss  0.4968654417991638  and accuracy  0.8824
training loss:  0.46381677538905314
valid loss  0.48054357385635377  and accuracy  0.882
training loss:  0.4653605830928988
valid loss  0.4914274832725525  and accuracy  0.8792
training loss:  0.46475509325263886
valid loss  0.47665035004615786  and accuracy  0.8812
training loss:  0.4615846394893512
valid loss  0.4783791109085083  and accuracy  0.882
training loss:  0.46384265860121454
valid loss  0.5102380599021912  and accuracy  0.8848
training loss:  0.4635079698247127
valid loss  0.47817183327674867  and accuracy  0.8816
training loss:  0.46140102818982687
valid loss  0.48562428331375124  and accuracy  0.8792
training loss:  0.4607146306353398
valid loss  0.4817016860961914  and accuracy  0.8824
training loss:  0.4629884473442832
valid loss  0.48486916108131406  and accuracy  0.8824
training loss:  0.463538590987124
valid loss  0.4832197605133057  and accuracy  0.8824
training loss:  0.4633517097987522
valid loss  0.4825653455734253  and accuracy  0.88
training loss:  0.46047327685196393
valid loss  0.48697086997032163  and accuracy  0.8812
training loss:  0.45838173819546724
valid loss  0.5156452578544617  and accuracy  0.8808
training loss:  0.4615140956849908
valid loss  0.4910787760257721  and accuracy  0.8824
training loss:  0.46052802581483593
valid loss  0.4790043098926544  and accuracy  0.8836
training loss:  0.46303461448631095
valid loss  0.48085438890457155  and accuracy  0.8796
training loss:  0.461770638748629
valid loss  0.47610087037086485  and accuracy  0.8836
training loss:  0.4616042251762633
valid loss  0.47225162119865416  and accuracy  0.8816
training loss:  0.4605495206075697
valid loss  0.4722652855396271  and accuracy  0.8828
training loss:  0.4622627445601139
valid loss  0.48163336038589477  and accuracy  0.884
training loss:  0.46338299786425513
valid loss  0.4818479874134064  and accuracy  0.8832
training loss:  0.45968736784542025
valid loss  0.48030507860183713  and accuracy  0.882
training loss:  0.460394075847351
valid loss  0.4758689287662506  and accuracy  0.8844
training loss:  0.4578873358779217
valid loss  0.4792928753852844  and accuracy  0.8836
training loss:  0.4577067829256681
valid loss  0.4830038297653198  and accuracy  0.8828
training loss:  0.45758500566434623
valid loss  0.5155401599884033  and accuracy  0.882
training loss:  0.45982749950346635
valid loss  0.4807262553215027  and accuracy  0.8812
training loss:  0.45943241549696356
valid loss  0.47649244437217714  and accuracy  0.8836
training loss:  0.4555050858201294
valid loss  0.5001952652931213  and accuracy  0.8816
training loss:  0.45818763012862085
valid loss  0.5046652851104736  and accuracy  0.8816
training loss:  0.4575477177972969
valid loss  0.4718010986804962  and accuracy  0.8832
training loss:  0.45893840556168675
valid loss  0.4804285382270813  and accuracy  0.8836
training loss:  0.4546104039478941
valid loss  0.47828936977386477  and accuracy  0.882
training loss:  0.4571712712546689
valid loss  0.4958257669448852  and accuracy  0.8836
training loss:  0.4562015145068592
valid loss  0.4669287548065186  and accuracy  0.8828
training loss:  0.4553639120192983
valid loss  0.4747947406768799  and accuracy  0.8812
training loss:  0.45625465417427435
valid loss  0.4832294337272644  and accuracy  0.882
training loss:  0.45460130425553824
valid loss  0.46436290736198427  and accuracy  0.8832
training loss:  0.4555584649244944
valid loss  0.4867019117355347  and accuracy  0.884
training loss:  0.4536911132147963
valid loss  0.4748018015861511  and accuracy  0.8816
training loss:  0.4526893919836137
valid loss  0.47736359968185427  and accuracy  0.8824
training loss:  0.45905415577505104
valid loss  0.4824675312995911  and accuracy  0.8808
training loss:  0.4558661284758218
valid loss  0.4892907125473022  and accuracy  0.884
training loss:  0.4549957861552885
valid loss  0.4783736253738403  and accuracy  0.8844
training loss:  0.4542345424193633
valid loss  0.47042886486053465  and accuracy  0.8844
training loss:  0.45268688377623184
valid loss  0.48195500383377077  and accuracy  0.8848
training loss:  0.45117098623184704
valid loss  0.4727968151092529  and accuracy  0.8856
training loss:  0.4520845979102692
valid loss  0.4758057067871094  and accuracy  0.8856
training loss:  0.45057696452492246
valid loss  0.4822101729393005  and accuracy  0.8832
training loss:  0.45194449748226145
valid loss  0.4767719382286072  and accuracy  0.8848
training loss:  0.4559461121982466
valid loss  0.5027809838294983  and accuracy  0.8844
training loss:  0.45525247821057063
valid loss  0.5039057613372803  and accuracy  0.8844
training loss:  0.45101260100577145
valid loss  0.49356898279190065  and accuracy  0.8824
training loss:  0.45306336835800504
valid loss  0.5196511286735535  and accuracy  0.856
training loss:  0.45160678701983825
valid loss  0.47949785928726196  and accuracy  0.8848
training loss:  0.4497613299432112
valid loss  0.4898034568786621  and accuracy  0.8824
training loss:  0.4526703693758902
valid loss  0.481504447221756  and accuracy  0.8808
training loss:  0.4519795280804786
valid loss  0.4772947669506073  and accuracy  0.8844
training loss:  0.45088276679392036
valid loss  0.4825531060218811  and accuracy  0.8812
training loss:  0.45328397379448665
valid loss  0.4875813608169556  and accuracy  0.8844
training loss:  0.45158324633012065
valid loss  0.47055946106910707  and accuracy  0.8856
training loss:  0.4482645672569722
valid loss  0.4770035940170288  and accuracy  0.884
training loss:  0.45426307930618876
valid loss  0.48172767815589906  and accuracy  0.8836
training loss:  0.45421225515442276
valid loss  0.48911461815834045  and accuracy  0.8816
training loss:  0.44916124552538245
valid loss  0.47636366052627566  and accuracy  0.8848
training loss:  0.4494187522174126
valid loss  0.4863136584758759  and accuracy  0.8844
training loss:  0.44789796617362565
valid loss  0.47412376890182495  and accuracy  0.8856
training loss:  0.44979828799789273
valid loss  0.5029969163894653  and accuracy  0.8832
training loss:  0.4469095638249587
valid loss  0.4739195188045502  and accuracy  0.8844
training loss:  0.4489990924470988
valid loss  0.4751524178504944  and accuracy  0.886
training loss:  0.449177274092957
valid loss  0.48909741477966306  and accuracy  0.88
training loss:  0.44681945536004836
valid loss  0.4715226982116699  and accuracy  0.8852
training loss:  0.4490851019295416
valid loss  0.4702240349292755  and accuracy  0.8836
training loss:  0.44745098665930716
valid loss  0.48630565552711486  and accuracy  0.884
training loss:  0.4496733086792069
valid loss  0.47121606092453006  and accuracy  0.8856
training loss:  0.4613448043464616
valid loss  0.5114054959297181  and accuracy  0.8712
training loss:  0.45484943703391045
valid loss  0.4799101117134094  and accuracy  0.884
training loss:  0.44948251407150447
valid loss  0.47382747440338135  and accuracy  0.8832
training loss:  0.45257587293084944
valid loss  0.4912065272331238  and accuracy  0.88
training loss:  0.44970355231558257
valid loss  0.4709557898521423  and accuracy  0.8828
training loss:  0.4480694208312873
valid loss  0.47829588532447814  and accuracy  0.8844
training loss:  0.44792583096566513
valid loss  0.47075507049560544  and accuracy  0.8848
training loss:  0.44919176910390807
valid loss  0.4770875041007996  and accuracy  0.8856
training loss:  0.44882829224244636
valid loss  0.5034775447368622  and accuracy  0.8676
training loss:  0.44792021027761486
valid loss  0.48255200605392456  and accuracy  0.8852
training loss:  0.4499733756535017
valid loss  0.47690926361083985  and accuracy  0.8852
training loss:  0.44747086350043214
valid loss  0.4732750597000122  and accuracy  0.8848
training loss:  0.44544513329389307
valid loss  0.4739516861438751  and accuracy  0.8848
training loss:  0.44776160489374667
valid loss  0.47214248757362365  and accuracy  0.8856
training loss:  0.44846887027398624
valid loss  0.49498154973983766  and accuracy  0.8828
training loss:  0.4493916341407814
valid loss  0.4762705223083496  and accuracy  0.8856
training loss:  0.44734991954598996
valid loss  0.4736207884311676  and accuracy  0.8836
training loss:  0.44522848904033
valid loss  0.5066777538299561  and accuracy  0.8756
training loss:  0.4448783304823104
valid loss  0.4713399411201477  and accuracy  0.8832
training loss:  0.4448387200608725
valid loss  0.47601106352806094  and accuracy  0.8836
training loss:  0.4446619048190476
valid loss  0.4706359018802643  and accuracy  0.886
training loss:  0.451273292552686
valid loss  0.5219040283203125  and accuracy  0.842
training loss:  0.4489849308028293
valid loss  0.49106054043769837  and accuracy  0.8844
training loss:  0.44557335421667627
valid loss  0.48062274618148804  and accuracy  0.8856
training loss:  0.44586749815661303
valid loss  0.48225991144180297  and accuracy  0.8828
training loss:  0.4471371149397775
valid loss  0.4762275257110596  and accuracy  0.884
training loss:  0.4500509404656875
valid loss  0.47964623794555666  and accuracy  0.8864
training loss:  0.4471365905886319
valid loss  0.47834694204330447  and accuracy  0.8832
training loss:  0.4468768395770535
valid loss  0.4817792695999146  and accuracy  0.8848
training loss:  0.4461592490349583
valid loss  0.5072346409797669  and accuracy  0.8732
training loss:  0.4453472960534407
valid loss  0.4749904704093933  and accuracy  0.8848
training loss:  0.4409906616163014
valid loss  0.4737760335445404  and accuracy  0.8852
training loss:  0.44103759874650583
valid loss  0.48864665541648866  and accuracy  0.8816
training loss:  0.44317376542730347
valid loss  0.4837094405651092  and accuracy  0.8848
training loss:  0.441416867933481
valid loss  0.4678014698028564  and accuracy  0.8848
training loss:  0.44100179332784273
valid loss  0.47656599225997925  and accuracy  0.886
training loss:  0.44175314227340606
valid loss  0.47316407709121705  and accuracy  0.8844
training loss:  0.4429421982373824
valid loss  0.48864877252578737  and accuracy  0.8844
training loss:  0.44293644631927337
valid loss  0.48160488104820254  and accuracy  0.8848
training loss:  0.4445459821815267
valid loss  0.4815156422138214  and accuracy  0.8844
training loss:  0.44193141097000094
valid loss  0.4727825490951538  and accuracy  0.8856
training loss:  0.43834049490029287
valid loss  0.4866110110282898  and accuracy  0.8852
training loss:  0.45020293031305925
valid loss  0.4939026933193207  and accuracy  0.8792
training loss:  0.4458129884609625
valid loss  0.4794805989742279  and accuracy  0.8836
training loss:  0.4426763086063379
valid loss  0.47691479988098145  and accuracy  0.8836
training loss:  0.44437533079878966
valid loss  0.47335943002700803  and accuracy  0.8836
training loss:  0.4419961579880323
valid loss  0.47355100531578065  and accuracy  0.8844
training loss:  0.44003058962486497
valid loss  0.498221462059021  and accuracy  0.8852
training loss:  0.4416715971588889
valid loss  0.48450902233123777  and accuracy  0.8848
training loss:  0.4427154506271209
valid loss  0.4876217875480652  and accuracy  0.884
training loss:  0.4508110738179991
valid loss  0.48336295385360717  and accuracy  0.884
training loss:  0.44359880845550714
valid loss  0.4733070089817047  and accuracy  0.8832
training loss:  0.44235330884979795
valid loss  0.49448868780136107  and accuracy  0.8816
training loss:  0.4503542251123655
valid loss  0.47624545850753786  and accuracy  0.884
training loss:  0.44421247541405245
valid loss  0.48747031354904174  and accuracy  0.8832
training loss:  0.4422241913413682
valid loss  0.47993985452651977  and accuracy  0.8852
training loss:  0.4412562957860317
valid loss  0.48799843864440917  and accuracy  0.8848
training loss:  0.43980831361096506
valid loss  0.4815761538505554  and accuracy  0.886
training loss:  0.4388910360272406
valid loss  0.46680272679328916  and accuracy  0.8852
training loss:  0.4382888880606672
valid loss  0.4713866723060608  and accuracy  0.8836
training loss:  0.4395443237406924
valid loss  0.4570005903244019  and accuracy  0.8856
training loss:  0.4412102533245406
valid loss  0.4752647989273071  and accuracy  0.8856
training loss:  0.4366175114409608
valid loss  0.46511412324905393  and accuracy  0.8852
training loss:  0.43826935031705566
valid loss  0.46499119472503664  and accuracy  0.8848
training loss:  0.4375992773565615
valid loss  0.4686818447113037  and accuracy  0.8844
training loss:  0.4382916019989218
valid loss  0.4697886300086975  and accuracy  0.8844
training loss:  0.4369580431998874
valid loss  0.47076408524513247  and accuracy  0.8852
training loss:  0.43718449388516806
valid loss  0.47478082327842713  and accuracy  0.8844
training loss:  0.43709260335120326
valid loss  0.47701249122619627  and accuracy  0.886
training loss:  0.4381622261637619
valid loss  0.4731521678447723  and accuracy  0.8836
training loss:  0.43727494180701687
valid loss  0.4696670548915863  and accuracy  0.8848
training loss:  0.440109291468034
valid loss  0.47224710149765015  and accuracy  0.8852
training loss:  0.43966896480052314
valid loss  0.48436615438461306  and accuracy  0.8796
training loss:  0.4381412411854295
valid loss  0.505587373828888  and accuracy  0.8828
training loss:  0.4393328010936079
valid loss  0.47774415316581725  and accuracy  0.8844
training loss:  0.4369683403465616
valid loss  0.4832576182842255  and accuracy  0.8832
training loss:  0.4377511631703656
valid loss  0.47251168947219846  and accuracy  0.8848
training loss:  0.4362833709872548
valid loss  0.46122875719070433  and accuracy  0.8848
training loss:  0.4337478940411029
valid loss  0.479264917755127  and accuracy  0.8844
training loss:  0.43523079759910877
valid loss  0.4629702088356018  and accuracy  0.8868
training loss:  0.43414247171763
valid loss  0.4720441027641296  and accuracy  0.8836
training loss:  0.4388616831558234
valid loss  0.4767632378101349  and accuracy  0.8864
training loss:  0.43753237194152333
valid loss  0.47248001885414126  and accuracy  0.8852
training loss:  0.436059832572937
valid loss  0.49482893114089965  and accuracy  0.8832
training loss:  0.4347197550425378
valid loss  0.47264830808639524  and accuracy  0.8832
training loss:  0.4359531959097589
valid loss  0.47697184810638427  and accuracy  0.882
training loss:  0.4369602704866847
valid loss  0.48566766104698184  and accuracy  0.8772
training loss:  0.4352152876418639
valid loss  0.4691836501121521  and accuracy  0.8852
training loss:  0.4340737416117235
valid loss  0.47252076683044436  and accuracy  0.8852
training loss:  0.4333127019792745
valid loss  0.48956553087234495  and accuracy  0.882
training loss:  0.43392424437668253
valid loss  0.469337273311615  and accuracy  0.8832
training loss:  0.43591087092107267
valid loss  0.482331450843811  and accuracy  0.8844
training loss:  0.4337058717681335
valid loss  0.4744253053188324  and accuracy  0.886
training loss:  0.4332993549018649
valid loss  0.48601452550888063  and accuracy  0.8852
training loss:  0.43364891041463344
valid loss  0.473428982925415  and accuracy  0.8836
training loss:  0.4321952655287444
valid loss  0.485085101890564  and accuracy  0.8848
training loss:  0.4317167544684418
valid loss  0.45972650804519655  and accuracy  0.8848
training loss:  0.430961203105885
valid loss  0.5020091862678527  and accuracy  0.8824
training loss:  0.4322670054395794
valid loss  0.4775276078224182  and accuracy  0.8812
training loss:  0.43351836472100747
valid loss  0.4730193562507629  and accuracy  0.884
training loss:  0.4314659310745035
valid loss  0.4793711117267609  and accuracy  0.8824
training loss:  0.4354480877893055
valid loss  0.48100619134902955  and accuracy  0.8836
training loss:  0.43428477990367703
valid loss  0.4700660848617554  and accuracy  0.8848
training loss:  0.4322861363540343
valid loss  0.4840957643032074  and accuracy  0.8856
training loss:  0.4312232661087509
valid loss  0.5168860631942749  and accuracy  0.8584
training loss:  0.4315120773499136
valid loss  0.46502438735961915  and accuracy  0.8856
training loss:  0.43024392385578636
valid loss  0.4821590332508087  and accuracy  0.886
training loss:  0.4281655906832198
valid loss  0.48858211727142337  and accuracy  0.8848
training loss:  0.43023744178577084
valid loss  0.4713695826530456  and accuracy  0.8848
training loss:  0.4292593015198731
valid loss  0.4815550757408142  and accuracy  0.886


print(cm)
[[  183    15    67     2  1560]
 [   19   169    18     9  1931]
 [   30     8   176     4  1255]
 [    7    21    14    40  1037]
 [   38    46    38    11 43563]]

              precision    recall  f1-score   support

           0       0.66      0.10      0.17      1827
           1       0.65      0.08      0.14      2146
           2       0.56      0.12      0.20      1473
           3       0.61      0.04      0.07      1119
           4       0.88      1.00      0.94     43696

    accuracy                           0.88     50261
   macro avg       0.67      0.27      0.30     50261
weighted avg       0.85      0.88      0.83     50261

Accuracy:  0.8780366486938183
Precision_weighted:  0.8493440821257808
Recall_weighted:  0.8780366486938183
mcc:  0.25682358573609765
f2:  0.8721440892788115

time = 30 min, 6 sec

- train_loop(model, epochs=500, lr=0.001, wd=0.000001)

ep  0  training loss:  0.7254245714725961
valid loss  0.7450845735549927  and accuracy  0.8736
ep  1  training loss:  0.5795362719938384
valid loss  0.5760305516242981  and accuracy  0.8704
ep  2  training loss:  0.5693202984592624
valid loss  0.5604301150321961  and accuracy  0.8728
ep  3  training loss:  0.5675133659611994
valid loss  0.556142356967926  and accuracy  0.8728
ep  4  training loss:  0.5621861119366172
valid loss  0.5507300025939942  and accuracy  0.8712
ep  5  training loss:  0.5592302564400524
valid loss  0.5452161146163941  and accuracy  0.8732
ep  6  training loss:  0.5577016460635954
valid loss  0.5458666650772095  and accuracy  0.8736
ep  7  training loss:  0.5574221412540481
valid loss  0.5437821208000183  and accuracy  0.8736
ep  8  training loss:  0.5582312256047873
valid loss  0.5457792246818542  and accuracy  0.8736
ep  9  training loss:  0.5561187221975982
valid loss  0.5366519411563874  and accuracy  0.8736
ep  10  training loss:  0.5545606930451576
valid loss  0.542651234292984  and accuracy  0.8732
ep  11  training loss:  0.554968233523856
valid loss  0.5390000088691711  and accuracy  0.8728
ep  12  training loss:  0.5530278677916407
valid loss  0.5381158325195312  and accuracy  0.8728
ep  13  training loss:  0.5501519904264454
valid loss  0.5374820658683777  and accuracy  0.8732
ep  14  training loss:  0.5506530217988607
valid loss  0.5394983324050904  and accuracy  0.8724
ep  15  training loss:  0.5518545880189892
valid loss  0.5363125986099243  and accuracy  0.8728
ep  16  training loss:  0.5524971150073973
valid loss  0.5341150127410889  and accuracy  0.8736
ep  17  training loss:  0.5492286033167112
valid loss  0.5333256653785705  and accuracy  0.8724
ep  18  training loss:  0.5479396805291998
valid loss  0.5369590486526489  and accuracy  0.8732
ep  19  training loss:  0.5463895604039357
valid loss  0.5304287943840027  and accuracy  0.8736
ep  20  training loss:  0.5441596867650798
valid loss  0.5350606211662292  and accuracy  0.8736
ep  21  training loss:  0.5440194933059028
valid loss  0.5291220362663269  and accuracy  0.874
ep  22  training loss:  0.5408900710506056
valid loss  0.5298816970348358  and accuracy  0.874
ep  23  training loss:  0.5411413658603751
valid loss  0.5270673024177551  and accuracy  0.8736
ep  24  training loss:  0.53807911521426
valid loss  0.5280467827796936  and accuracy  0.8732
ep  25  training loss:  0.5386261672330661
valid loss  0.5304474199295044  and accuracy  0.874
ep  26  training loss:  0.5406783134094635
valid loss  0.535975360584259  and accuracy  0.8736
ep  27  training loss:  0.5400721275626714
valid loss  0.5338775998115539  and accuracy  0.8724
ep  28  training loss:  0.5391067146855583
valid loss  0.5292696216583251  and accuracy  0.8732
ep  29  training loss:  0.5382231580552145
valid loss  0.5293678830623627  and accuracy  0.8736
ep  30  training loss:  0.5412063179403493
valid loss  0.5286898788452148  and accuracy  0.8736
ep  31  training loss:  0.5396138850967689
valid loss  0.5299716973304749  and accuracy  0.8728
ep  32  training loss:  0.53537305014417
valid loss  0.5267131629943848  and accuracy  0.874
ep  33  training loss:  0.5357991194205867
valid loss  0.5197487896919251  and accuracy  0.8744
ep  34  training loss:  0.5349263225168839
valid loss  0.5204758964061738  and accuracy  0.872
ep  35  training loss:  0.5339935804531603
valid loss  0.525044466495514  and accuracy  0.8728
ep  36  training loss:  0.5344874602566213
valid loss  0.5216123064041138  and accuracy  0.8736
ep  37  training loss:  0.5338822808097954
valid loss  0.5198879897117614  and accuracy  0.8724
ep  38  training loss:  0.5337019360844214
valid loss  0.5187206068992615  and accuracy  0.8736
ep  39  training loss:  0.5347253861539129
valid loss  0.5262959412574768  and accuracy  0.8736
ep  40  training loss:  0.5342438674152796
valid loss  0.5376306254386902  and accuracy  0.8732
ep  41  training loss:  0.5341001757924481
valid loss  0.5337252162933349  and accuracy  0.874
ep  42  training loss:  0.533261496697239
valid loss  0.5309871984481811  and accuracy  0.8744
ep  43  training loss:  0.5304875767610381
valid loss  0.5227740475654602  and accuracy  0.874
ep  44  training loss:  0.5313306482792699
valid loss  0.5290001549720764  and accuracy  0.8736
ep  45  training loss:  0.5316266937571354
valid loss  0.5257167911529541  and accuracy  0.8732
ep  46  training loss:  0.5306641457667902
valid loss  0.5290597849845886  and accuracy  0.8756
ep  47  training loss:  0.5329349156500307
valid loss  0.5266984708786011  and accuracy  0.8748
ep  48  training loss:  0.5358522245832025
valid loss  0.5335998104095458  and accuracy  0.8724
ep  49  training loss:  0.5346113225123791
valid loss  0.5291860217094422  and accuracy  0.874
ep  50  training loss:  0.5348808395503154
valid loss  0.5248747631072999  and accuracy  0.874
ep  51  training loss:  0.5330658119527539
valid loss  0.5295674101829528  and accuracy  0.8744
ep  52  training loss:  0.5352852177779679
valid loss  0.5259079721450806  and accuracy  0.8756
ep  53  training loss:  0.5319126829430086
valid loss  0.5273481015205383  and accuracy  0.8768
ep  54  training loss:  0.530330204214882
valid loss  0.5168970862388611  and accuracy  0.8768
ep  55  training loss:  0.5271784612681198
valid loss  0.5151984040260315  and accuracy  0.8764
ep  56  training loss:  0.5266174896877615
valid loss  0.5118461019992828  and accuracy  0.8764
ep  57  training loss:  0.524134372247124
valid loss  0.512936810684204  and accuracy  0.8752
ep  58  training loss:  0.526914045938495
valid loss  0.5156135731697082  and accuracy  0.876
ep  59  training loss:  0.5247959793911907
valid loss  0.5140102705955505  and accuracy  0.8748
ep  60  training loss:  0.5220380948616232
valid loss  0.5139938242912292  and accuracy  0.8768
ep  61  training loss:  0.5211944365621212
valid loss  0.5165055674552917  and accuracy  0.8756
ep  62  training loss:  0.5201734837375493
valid loss  0.5116598786354065  and accuracy  0.876
ep  63  training loss:  0.5210694864966362
valid loss  0.5128082718849182  and accuracy  0.8764
ep  64  training loss:  0.5219899020202995
valid loss  0.5070115862369537  and accuracy  0.8772
ep  65  training loss:  0.5209945554409794
valid loss  0.5112250583648682  and accuracy  0.8772
ep  66  training loss:  0.5195693394247212
valid loss  0.5101323986053466  and accuracy  0.8772
ep  67  training loss:  0.5174979658981463
valid loss  0.5130035898208618  and accuracy  0.8764
ep  68  training loss:  0.5166340926983448
valid loss  0.5120091526985169  and accuracy  0.8764
ep  69  training loss:  0.516388346412074
valid loss  0.5096446547031402  and accuracy  0.8768
ep  70  training loss:  0.5157138689478837
valid loss  0.5131166751861572  and accuracy  0.8772
ep  71  training loss:  0.5154138164504289
valid loss  0.5106469326972961  and accuracy  0.878
ep  72  training loss:  0.5140641530554498
valid loss  0.5074230867385864  and accuracy  0.8772
ep  73  training loss:  0.5118331945222027
valid loss  0.5146377906799317  and accuracy  0.8748
ep  74  training loss:  0.5130941341869795
valid loss  0.5148463524818421  and accuracy  0.8744
ep  75  training loss:  0.5119221478451037
valid loss  0.5108305305004119  and accuracy  0.8764
ep  76  training loss:  0.5108387021563161
valid loss  0.5079542801856994  and accuracy  0.8776
ep  77  training loss:  0.5097934300775704
valid loss  0.5055603541374206  and accuracy  0.8772
ep  78  training loss:  0.5105966337362127
valid loss  0.5152715935230255  and accuracy  0.8756
ep  79  training loss:  0.5089977090382696
valid loss  0.505525123500824  and accuracy  0.8756
ep  80  training loss:  0.5090516516311684
valid loss  0.5047587892532349  and accuracy  0.8776
ep  81  training loss:  0.5105771842713731
valid loss  0.5040624928474426  and accuracy  0.878
ep  82  training loss:  0.5098717654170702
valid loss  0.5060048926830292  and accuracy  0.8776
ep  83  training loss:  0.5075627158235265
valid loss  0.5018182361602783  and accuracy  0.878
ep  84  training loss:  0.5073333328892438
valid loss  0.5040253395080566  and accuracy  0.8784
ep  85  training loss:  0.506111215107405
valid loss  0.5029997437953949  and accuracy  0.878
ep  86  training loss:  0.5048031122041508
valid loss  0.5052651077270508  and accuracy  0.8748
ep  87  training loss:  0.5089823364712286
valid loss  0.509760244178772  and accuracy  0.8756
ep  88  training loss:  0.5069148242174082
valid loss  0.5043036266326905  and accuracy  0.8772
ep  89  training loss:  0.5062852280024108
valid loss  0.5042858745098114  and accuracy  0.8784
ep  90  training loss:  0.5063733773614894
valid loss  0.5047966257095337  and accuracy  0.8768
ep  91  training loss:  0.5090846443495758
valid loss  0.511224545097351  and accuracy  0.8764
ep  92  training loss:  0.5081489949769511
valid loss  0.521454255104065  and accuracy  0.876
ep  93  training loss:  0.5089562975282046
valid loss  0.5034060206413269  and accuracy  0.8776
ep  94  training loss:  0.5061385826050137
valid loss  0.49923352599143983  and accuracy  0.878
ep  95  training loss:  0.5060159992532715
valid loss  0.5106228344917297  and accuracy  0.876
ep  96  training loss:  0.5067107266916502
valid loss  0.5025666721343994  and accuracy  0.878
ep  97  training loss:  0.5052181837946127
valid loss  0.5017748303413391  and accuracy  0.8772
ep  98  training loss:  0.5034335653586204
valid loss  0.5009860892295838  and accuracy  0.8768
ep  99  training loss:  0.5045278186374773
valid loss  0.51084250664711  and accuracy  0.8764
ep  100  training loss:  0.5048969411171061
valid loss  0.5154500383853913  and accuracy  0.8768
ep  101  training loss:  0.5059415981597837
valid loss  0.5001380735397339  and accuracy  0.8776
ep  102  training loss:  0.5033882900698101
valid loss  0.5014608772277832  and accuracy  0.8768
ep  103  training loss:  0.5024164953423504
valid loss  0.5055101203918457  and accuracy  0.8756
ep  104  training loss:  0.5021224460609793
valid loss  0.5000234628677368  and accuracy  0.8788
ep  105  training loss:  0.5012525592617054
valid loss  0.499210924243927  and accuracy  0.8768
ep  106  training loss:  0.5025067680644829
valid loss  0.5017994291305542  and accuracy  0.8768
ep  107  training loss:  0.5022604743439948
valid loss  0.5039672476291657  and accuracy  0.8764
ep  108  training loss:  0.5025491458686752
valid loss  0.49822790603637696  and accuracy  0.8788
ep  109  training loss:  0.5031127288712928
valid loss  0.5148679317474365  and accuracy  0.8752
ep  110  training loss:  0.5022088064021202
valid loss  0.5024111325263977  and accuracy  0.876
ep  111  training loss:  0.4991877547760106
valid loss  0.4940590331554413  and accuracy  0.8768
ep  112  training loss:  0.4989083436765463
valid loss  0.5006449418067932  and accuracy  0.8768
ep  113  training loss:  0.5014370236963882
valid loss  0.5073307518959045  and accuracy  0.8768
ep  114  training loss:  0.5045826093236007
valid loss  0.5051677874565125  and accuracy  0.8768
ep  115  training loss:  0.5060413086134784
valid loss  0.5048092480659485  and accuracy  0.8768
ep  116  training loss:  0.505800613686068
valid loss  0.5049580906867981  and accuracy  0.8772
ep  117  training loss:  0.5026819840547827
valid loss  0.49883622016906737  and accuracy  0.8788
ep  118  training loss:  0.5018317456421141
valid loss  0.5021880604743958  and accuracy  0.878
ep  119  training loss:  0.5008978158984352
valid loss  0.5022151782512665  and accuracy  0.8764
ep  120  training loss:  0.5008412265198514
valid loss  0.4984182201385498  and accuracy  0.8772
ep  121  training loss:  0.4988635114688969
valid loss  0.5005660076141357  and accuracy  0.8772
ep  122  training loss:  0.5011633014559147
valid loss  0.499797824382782  and accuracy  0.8764
ep  123  training loss:  0.5005481258209427
valid loss  0.5064195426464081  and accuracy  0.876
ep  124  training loss:  0.5023999121121226
valid loss  0.5037892826080322  and accuracy  0.8776
ep  125  training loss:  0.5001970506692851
valid loss  0.49657241258621215  and accuracy  0.878
ep  126  training loss:  0.49929101806589504
valid loss  0.5071822370052338  and accuracy  0.8772
ep  127  training loss:  0.49753262162008877
valid loss  0.5022710566520691  and accuracy  0.8772
ep  128  training loss:  0.5000607535467675
valid loss  0.5029999445915222  and accuracy  0.8776
ep  129  training loss:  0.4985544923462061
valid loss  0.5055696158409119  and accuracy  0.876
ep  130  training loss:  0.49752126682543474
valid loss  0.49680577325820924  and accuracy  0.878
ep  131  training loss:  0.4999209595685029
valid loss  0.4960668704032898  and accuracy  0.8796
ep  132  training loss:  0.499257486150093
valid loss  0.5010793251991272  and accuracy  0.8768
ep  133  training loss:  0.49661065640561347
valid loss  0.49746571469306944  and accuracy  0.878
ep  134  training loss:  0.4962120956312272
valid loss  0.49383936557769775  and accuracy  0.8776
ep  135  training loss:  0.49331085251603696
valid loss  0.4936976524353027  and accuracy  0.8776
ep  136  training loss:  0.49538400744273636
valid loss  0.4995003737449646  and accuracy  0.8772
ep  137  training loss:  0.49545738466420963
valid loss  0.5122323945999145  and accuracy  0.8748
ep  138  training loss:  0.4985342216252083
valid loss  0.5034119246482849  and accuracy  0.8764
ep  139  training loss:  0.4951461329132668
valid loss  0.4992111181259155  and accuracy  0.8768
ep  140  training loss:  0.4952354182749737
valid loss  0.5026536427021027  and accuracy  0.8768
ep  141  training loss:  0.4950609165819446
valid loss  0.4944222327232361  and accuracy  0.8772
ep  142  training loss:  0.49451471245668244
valid loss  0.4980988596916199  and accuracy  0.8776
ep  143  training loss:  0.49238238323673333
valid loss  0.49409852929115294  and accuracy  0.878
ep  144  training loss:  0.49140956527623697
valid loss  0.4971557119369507  and accuracy  0.8764
ep  145  training loss:  0.49198543166794767
valid loss  0.4976804208755493  and accuracy  0.8772
ep  146  training loss:  0.4903090664190264
valid loss  0.5119134829521179  and accuracy  0.8764
ep  147  training loss:  0.49266195403071905
valid loss  0.5087595537185668  and accuracy  0.8768
ep  148  training loss:  0.4912461434576779
valid loss  0.4924543565750122  and accuracy  0.878
ep  149  training loss:  0.4916137652980223
valid loss  0.5022446292877197  and accuracy  0.8768
ep  150  training loss:  0.4899792732305862
valid loss  0.49792932686805724  and accuracy  0.878
ep  151  training loss:  0.4912731739444349
valid loss  0.4921902485847473  and accuracy  0.8772
ep  152  training loss:  0.4902821104330833
valid loss  0.49042296204566954  and accuracy  0.878
ep  153  training loss:  0.49047403667041006
valid loss  0.4936703212738037  and accuracy  0.8776
ep  154  training loss:  0.4899016877514633
valid loss  0.49977254829406736  and accuracy  0.8772
ep  155  training loss:  0.49305018839524617
valid loss  0.4976499135017395  and accuracy  0.878
ep  156  training loss:  0.4895653276587251
valid loss  0.5004959094047546  and accuracy  0.8772
ep  157  training loss:  0.48936963121295973
valid loss  0.5024778597831726  and accuracy  0.8768
ep  158  training loss:  0.4887164722535279
valid loss  0.4891741587638855  and accuracy  0.878
ep  159  training loss:  0.48760591823252003
valid loss  0.49209168491363525  and accuracy  0.8776
ep  160  training loss:  0.4861156749365917
valid loss  0.4996861228942871  and accuracy  0.8776
ep  161  training loss:  0.4862170403926217
valid loss  0.4945345846652985  and accuracy  0.8788
ep  162  training loss:  0.48777058507928894
valid loss  0.4898777379989624  and accuracy  0.878
ep  163  training loss:  0.48784845458402903
valid loss  0.49626589832305906  and accuracy  0.8772
ep  164  training loss:  0.48784060502172116
valid loss  0.5157075577735901  and accuracy  0.8768
ep  165  training loss:  0.48842411212985043
valid loss  0.49550946292877196  and accuracy  0.878
ep  166  training loss:  0.4864148192269918
valid loss  0.4974974757194519  and accuracy  0.878
ep  167  training loss:  0.48721448719201976
valid loss  0.49050793051719666  and accuracy  0.8768
ep  168  training loss:  0.4863343498914485
valid loss  0.49791041774749756  and accuracy  0.8776
ep  169  training loss:  0.48521663104269774
valid loss  0.5020055967330933  and accuracy  0.878
ep  170  training loss:  0.4859684681772587
valid loss  0.5044601219654083  and accuracy  0.8772
ep  171  training loss:  0.4848732154373348
valid loss  0.503061394739151  and accuracy  0.876
ep  172  training loss:  0.48525121613363525
valid loss  0.4893292922496796  and accuracy  0.8784
ep  173  training loss:  0.4835926972941138
valid loss  0.4956111887931824  and accuracy  0.8784
ep  174  training loss:  0.4816665849593816
valid loss  0.495316908788681  and accuracy  0.8784
ep  175  training loss:  0.48322914467385064
valid loss  0.49619417114257813  and accuracy  0.8776
ep  176  training loss:  0.48297071365056127
valid loss  0.501818863773346  and accuracy  0.8764
ep  177  training loss:  0.4864140267747531
valid loss  0.49315545902252195  and accuracy  0.878
ep  178  training loss:  0.48913282244848444
valid loss  0.49225690994262694  and accuracy  0.878
ep  179  training loss:  0.48733595649601025
valid loss  0.4898225749015808  and accuracy  0.88
ep  180  training loss:  0.4850716687526735
valid loss  0.4954824112892151  and accuracy  0.8772
ep  181  training loss:  0.4824921782891355
valid loss  0.5054732048988342  and accuracy  0.8784
ep  182  training loss:  0.4854165237353275
valid loss  0.4983562539100647  and accuracy  0.8768
ep  183  training loss:  0.48301952220686717
valid loss  0.49524754009246824  and accuracy  0.878
ep  184  training loss:  0.4830448341928734
valid loss  0.4987400061607361  and accuracy  0.8772
ep  185  training loss:  0.4827866848988749
valid loss  0.4988242255210876  and accuracy  0.8784
ep  186  training loss:  0.4808429195253893
valid loss  0.4909015028953552  and accuracy  0.8788
ep  187  training loss:  0.4840663475207548
valid loss  0.4991682360649109  and accuracy  0.8788
ep  188  training loss:  0.48104270088612733
valid loss  0.49555396165847776  and accuracy  0.8784
ep  189  training loss:  0.47920528131114376
valid loss  0.5035851036548614  and accuracy  0.878
ep  190  training loss:  0.4792328687169444
valid loss  0.499366840839386  and accuracy  0.88
ep  191  training loss:  0.47885263660045924
valid loss  0.48655048117637634  and accuracy  0.8788
ep  192  training loss:  0.4821458668764712
valid loss  0.49905571866035464  and accuracy  0.8776
ep  193  training loss:  0.48104584390793614
valid loss  0.4953652575969696  and accuracy  0.8764
ep  194  training loss:  0.478701214854242
valid loss  0.4985059977531433  and accuracy  0.8792
ep  195  training loss:  0.47958387639654343
valid loss  0.49352966232299805  and accuracy  0.8784
ep  196  training loss:  0.48048534439237073
valid loss  0.48943352789878847  and accuracy  0.8784
ep  197  training loss:  0.4780789605736533
valid loss  0.4973742899894714  and accuracy  0.8784
ep  198  training loss:  0.4816322210046714
valid loss  0.491266374874115  and accuracy  0.88
ep  199  training loss:  0.4780275677017231
valid loss  0.4889717339515686  and accuracy  0.88
ep  200  training loss:  0.48103521606031574
valid loss  0.49899939794540404  and accuracy  0.8792
ep  201  training loss:  0.47863840911056926
valid loss  0.49409543838500974  and accuracy  0.8768
ep  202  training loss:  0.47933085861317876
valid loss  0.4971653754234314  and accuracy  0.8792
ep  203  training loss:  0.47986809216951404
valid loss  0.487364280128479  and accuracy  0.88
ep  204  training loss:  0.4758489768908451
valid loss  0.48519334287643434  and accuracy  0.8796
ep  205  training loss:  0.47513616763966365
valid loss  0.49449111938476564  and accuracy  0.878
ep  206  training loss:  0.4743783694915037
valid loss  0.4844280989646912  and accuracy  0.88
ep  207  training loss:  0.4765240501718505
valid loss  0.48927309246063233  and accuracy  0.8788
ep  208  training loss:  0.47504049350867916
valid loss  0.48188263158798217  and accuracy  0.8792
ep  209  training loss:  0.47490657758672833
valid loss  0.5006618200302124  and accuracy  0.8788
ep  210  training loss:  0.4742026775925603
valid loss  0.487583406829834  and accuracy  0.8772
ep  211  training loss:  0.47362569596499854
valid loss  0.48686135416030885  and accuracy  0.8776
ep  212  training loss:  0.4731715199336335
valid loss  0.48193514885902405  and accuracy  0.8788
ep  213  training loss:  0.4719469776505002
valid loss  0.4861280336380005  and accuracy  0.8788
ep  214  training loss:  0.47244522523640387
valid loss  0.4893948409080505  and accuracy  0.8792
ep  215  training loss:  0.4718992185053514
valid loss  0.49558970742225644  and accuracy  0.8764
ep  216  training loss:  0.4729555778567316
valid loss  0.4895403031349182  and accuracy  0.8788
ep  217  training loss:  0.47128076864846385
valid loss  0.48257326912879944  and accuracy  0.8788
ep  218  training loss:  0.4721163177709883
valid loss  0.4838008852481842  and accuracy  0.8792
ep  219  training loss:  0.47313532484835713
valid loss  0.49112093381881716  and accuracy  0.8788
ep  220  training loss:  0.47204572750096346
valid loss  0.4932456668376923  and accuracy  0.8752
ep  221  training loss:  0.47510266267075213
valid loss  0.4989336221694946  and accuracy  0.878
ep  222  training loss:  0.47328897641731466
valid loss  0.4940978582382202  and accuracy  0.8784
ep  223  training loss:  0.4724493506565765
valid loss  0.49640330200195315  and accuracy  0.8788
ep  224  training loss:  0.47175204644051427
valid loss  0.4808292217254639  and accuracy  0.8792
ep  225  training loss:  0.4722433095102134
valid loss  0.48582568283081057  and accuracy  0.8752
ep  226  training loss:  0.46895736566740065
valid loss  0.48601082134246826  and accuracy  0.8772
ep  227  training loss:  0.47113091647125765
valid loss  0.4915795696258545  and accuracy  0.8784
ep  228  training loss:  0.4691885799718662
valid loss  0.48578656234741213  and accuracy  0.878
ep  229  training loss:  0.4694849404158504
valid loss  0.4847153819561005  and accuracy  0.8804
ep  230  training loss:  0.46838590316037637
valid loss  0.49885866041183474  and accuracy  0.8764
ep  231  training loss:  0.4670118979173689
valid loss  0.49226006870269773  and accuracy  0.8796
ep  232  training loss:  0.46812721501842414
valid loss  0.4825522675514221  and accuracy  0.8812
ep  233  training loss:  0.478768467483808
valid loss  0.4859550309181213  and accuracy  0.878
ep  234  training loss:  0.47531287340662587
valid loss  0.49268451194763185  and accuracy  0.878
ep  235  training loss:  0.47095512728794936
valid loss  0.49159780888557436  and accuracy  0.878
ep  236  training loss:  0.4694736173404521
valid loss  0.4934840563774109  and accuracy  0.878
ep  237  training loss:  0.46842511830417755
valid loss  0.48747767128944397  and accuracy  0.88
ep  238  training loss:  0.46624405100517335
valid loss  0.490062823343277  and accuracy  0.8792
ep  239  training loss:  0.4648209620260913
valid loss  0.4856820637702942  and accuracy  0.8796
ep  240  training loss:  0.46600988504275603
valid loss  0.49364729022979736  and accuracy  0.8796
ep  241  training loss:  0.46734895188005726
valid loss  0.4901759490013123  and accuracy  0.878
ep  242  training loss:  0.46665120834681256
valid loss  0.488466371011734  and accuracy  0.878
ep  243  training loss:  0.47274543289563165
valid loss  0.485554495716095  and accuracy  0.8788
ep  244  training loss:  0.47021745537193976
valid loss  0.4881696741104126  and accuracy  0.8792
ep  245  training loss:  0.4698739045029709
valid loss  0.4852848969459534  and accuracy  0.8788
ep  246  training loss:  0.4682943592518818
valid loss  0.4883293555736542  and accuracy  0.8792
ep  247  training loss:  0.4688395657631221
valid loss  0.49857041969299315  and accuracy  0.8788
ep  248  training loss:  0.46608645644419555
valid loss  0.4921948905467987  and accuracy  0.878
ep  249  training loss:  0.4665547341158242
valid loss  0.5006698837280273  and accuracy  0.8792
ep  250  training loss:  0.46592597036904826
valid loss  0.4924061131954193  and accuracy  0.8796
ep  251  training loss:  0.4680236704984502
valid loss  0.48148758301734923  and accuracy  0.8796
ep  252  training loss:  0.4667470559902127
valid loss  0.4863284824848175  and accuracy  0.8784
ep  253  training loss:  0.46420362490904793
valid loss  0.48894153985977173  and accuracy  0.8792
ep  254  training loss:  0.4649562701707709
valid loss  0.49007441515922545  and accuracy  0.878
ep  255  training loss:  0.46324389247439013
valid loss  0.48409727420806886  and accuracy  0.8804
ep  256  training loss:  0.4640753066320116
valid loss  0.4925007411003113  and accuracy  0.8796
ep  257  training loss:  0.46181031060178873
valid loss  0.48919825501441955  and accuracy  0.8796
ep  258  training loss:  0.4660687357536712
valid loss  0.48299483680725097  and accuracy  0.8792
ep  259  training loss:  0.46366963082022805
valid loss  0.4845315116405487  and accuracy  0.8792
ep  260  training loss:  0.46341911849863765
valid loss  0.48585817642211915  and accuracy  0.8804
ep  261  training loss:  0.4667144877028026
valid loss  0.4928103687286377  and accuracy  0.878
ep  262  training loss:  0.4658506621467808
valid loss  0.5055863472938538  and accuracy  0.878
ep  263  training loss:  0.4628936218855968
valid loss  0.49426227731704714  and accuracy  0.8792
ep  264  training loss:  0.4703727780175169
valid loss  0.4969328860759735  and accuracy  0.8788
ep  265  training loss:  0.4701588165620264
valid loss  0.4959465329647064  and accuracy  0.8788
ep  266  training loss:  0.46902990058638544
valid loss  0.4860150077819824  and accuracy  0.88
ep  267  training loss:  0.4668710959616618
valid loss  0.49408443870544433  and accuracy  0.8792
ep  268  training loss:  0.46270663410774626
valid loss  0.48946615123748777  and accuracy  0.8792
ep  269  training loss:  0.462931858195332
valid loss  0.49401925086975096  and accuracy  0.8796
ep  270  training loss:  0.46404593006849687
valid loss  0.500396085357666  and accuracy  0.8796
ep  271  training loss:  0.46602845571348617
valid loss  0.4959878465175629  and accuracy  0.8788
ep  272  training loss:  0.46170909143173117
valid loss  0.49459380550384524  and accuracy  0.88
ep  273  training loss:  0.46289915533321385
valid loss  0.48128274917602537  and accuracy  0.8812
ep  274  training loss:  0.4610412534357515
valid loss  0.4869446118354797  and accuracy  0.8792
ep  275  training loss:  0.45992706494914426
valid loss  0.47206604533195495  and accuracy  0.8816
ep  276  training loss:  0.4581707308739673
valid loss  0.4797316785812378  and accuracy  0.88
ep  277  training loss:  0.46503013745823896
valid loss  0.4932811901092529  and accuracy  0.8784
ep  278  training loss:  0.46847812458498395
valid loss  0.47993947262763975  and accuracy  0.8792
ep  279  training loss:  0.4660977117779666
valid loss  0.4898569100856781  and accuracy  0.88
ep  280  training loss:  0.4625059327088808
valid loss  0.4783886258125305  and accuracy  0.8804
ep  281  training loss:  0.46242919840804697
valid loss  0.49606734018325804  and accuracy  0.8784
ep  282  training loss:  0.4627127838993392
valid loss  0.48269404106140135  and accuracy  0.8808
ep  283  training loss:  0.46116182032342334
valid loss  0.48893966817855833  and accuracy  0.878
ep  284  training loss:  0.46088226277824224
valid loss  0.4898159649372101  and accuracy  0.8804
ep  285  training loss:  0.46093326503507653
valid loss  0.48891060104370115  and accuracy  0.8808
ep  286  training loss:  0.47055242550832344
valid loss  0.5021863179683685  and accuracy  0.88
ep  287  training loss:  0.46921940660556555
valid loss  0.4837111619472504  and accuracy  0.8796
ep  288  training loss:  0.46423011393203606
valid loss  0.4884259638786316  and accuracy  0.8792
ep  289  training loss:  0.4643455269248841
valid loss  0.4873846978187561  and accuracy  0.878
ep  290  training loss:  0.46028447626423796
valid loss  0.4824530663490295  and accuracy  0.882
ep  291  training loss:  0.46133123215918165
valid loss  0.49883147983551024  and accuracy  0.882
ep  292  training loss:  0.4594977010734117
valid loss  0.48778555068969726  and accuracy  0.8812
ep  293  training loss:  0.45796641271118343
valid loss  0.48720478949546814  and accuracy  0.8816
ep  294  training loss:  0.45875511491917687
valid loss  0.4949987236022949  and accuracy  0.8808
ep  295  training loss:  0.4580720232459568
valid loss  0.48146782207489014  and accuracy  0.8808
ep  296  training loss:  0.45686651450305726
valid loss  0.5070640954971314  and accuracy  0.8792
ep  297  training loss:  0.45695901909465564
valid loss  0.4989298407077789  and accuracy  0.88
ep  298  training loss:  0.4628687362575052
valid loss  0.49384094390869143  and accuracy  0.878
ep  299  training loss:  0.4596565234201837
valid loss  0.4836438185214996  and accuracy  0.88
ep  300  training loss:  0.4594040238777397
valid loss  0.49480704488754274  and accuracy  0.88
ep  301  training loss:  0.45846105933788434
valid loss  0.49528039116859435  and accuracy  0.8804
ep  302  training loss:  0.457704082925116
valid loss  0.4871591980457306  and accuracy  0.8792
ep  303  training loss:  0.457465048151799
valid loss  0.49393518314361573  and accuracy  0.8808
ep  304  training loss:  0.45559269924659024
valid loss  0.5020426815509796  and accuracy  0.8816
ep  305  training loss:  0.4578544338943571
valid loss  0.4884720337867737  and accuracy  0.8796
ep  306  training loss:  0.45688885680994196
valid loss  0.4858037268638611  and accuracy  0.8812
ep  307  training loss:  0.4562920294315172
valid loss  0.49701406774520873  and accuracy  0.8808
ep  308  training loss:  0.45835221770221063
valid loss  0.5022704978466034  and accuracy  0.88
ep  309  training loss:  0.4558027245890555
valid loss  0.4995286433696747  and accuracy  0.8784
ep  310  training loss:  0.459269464115801
valid loss  0.48484740500450135  and accuracy  0.8792
ep  311  training loss:  0.45378550798250006
valid loss  0.4806603560447693  and accuracy  0.882
ep  312  training loss:  0.4531711387274852
valid loss  0.48862380294799806  and accuracy  0.8776
ep  313  training loss:  0.45191142001942775
valid loss  0.47139187803268434  and accuracy  0.8808
ep  314  training loss:  0.45402679711929717
valid loss  0.4796122292518616  and accuracy  0.8804
ep  315  training loss:  0.45339936189715385
valid loss  0.49165296006202697  and accuracy  0.88
ep  316  training loss:  0.45452745920249965
valid loss  0.4953873707294464  and accuracy  0.8796
ep  317  training loss:  0.45329890752357854
valid loss  0.5144283487796784  and accuracy  0.8788
ep  318  training loss:  0.4530034169879191
valid loss  0.48158001160621644  and accuracy  0.8804
ep  319  training loss:  0.45334377381669816
valid loss  0.5094366744041443  and accuracy  0.8808
ep  320  training loss:  0.4525464788853024
valid loss  0.4837904679775238  and accuracy  0.8796
ep  321  training loss:  0.4504262118682989
valid loss  0.4805039759635925  and accuracy  0.8808
ep  322  training loss:  0.4530958697624143
valid loss  0.4966114544868469  and accuracy  0.8804
ep  323  training loss:  0.45111157888144104
valid loss  0.4872298153877258  and accuracy  0.8788
ep  324  training loss:  0.4517141125909048
valid loss  0.4937775843143463  and accuracy  0.8796
ep  325  training loss:  0.45150356701071337
valid loss  0.47999401984214785  and accuracy  0.8804
ep  326  training loss:  0.4564368168113619
valid loss  0.4870397185325623  and accuracy  0.8816
ep  327  training loss:  0.4552118737693608
valid loss  0.49153441967964173  and accuracy  0.8812
ep  328  training loss:  0.4537456166404775
valid loss  0.4833700433731079  and accuracy  0.8828
ep  329  training loss:  0.4533541894438279
valid loss  0.48747317695617676  and accuracy  0.8808
ep  330  training loss:  0.45176425768901757
valid loss  0.4850831964492798  and accuracy  0.8808
ep  331  training loss:  0.44848491772135696
valid loss  0.48653001537323  and accuracy  0.8812
ep  332  training loss:  0.4488099839918977
valid loss  0.4883170030117035  and accuracy  0.8812
ep  333  training loss:  0.4470170562590786
valid loss  0.48724655923843385  and accuracy  0.8828
ep  334  training loss:  0.4480999230539779
valid loss  0.5099525489807128  and accuracy  0.8796
ep  335  training loss:  0.44740378274590126
valid loss  0.49866203165054324  and accuracy  0.8808
ep  336  training loss:  0.4484958795446846
valid loss  0.48413848485946653  and accuracy  0.8792
ep  337  training loss:  0.44666627751323246
valid loss  0.5017282038211822  and accuracy  0.8812
ep  338  training loss:  0.4477912407224901
valid loss  0.492747386598587  and accuracy  0.8804
ep  339  training loss:  0.4459536740128918
valid loss  0.48128719186782837  and accuracy  0.8816
ep  340  training loss:  0.44504122589301426
valid loss  0.49121597881317136  and accuracy  0.8804
ep  341  training loss:  0.44614721591548506
valid loss  0.5057266453742981  and accuracy  0.8816
ep  342  training loss:  0.4454218041557363
valid loss  0.49705176219940184  and accuracy  0.88
ep  343  training loss:  0.44605447883581995
valid loss  0.4959754725933075  and accuracy  0.882
ep  344  training loss:  0.4459520324390737
valid loss  0.5006147438049317  and accuracy  0.8808
ep  345  training loss:  0.44542460245902415
valid loss  0.5004312517166137  and accuracy  0.882
ep  346  training loss:  0.4449715450480955
valid loss  0.5006580576896668  and accuracy  0.8828
ep  347  training loss:  0.4421776686381655
valid loss  0.4904279860496521  and accuracy  0.8812
ep  348  training loss:  0.44241335383012664
valid loss  0.4967274823188782  and accuracy  0.8816
ep  349  training loss:  0.44145445303501596
valid loss  0.4971053958892822  and accuracy  0.8828
ep  350  training loss:  0.4426525830423812
valid loss  0.5017940445899963  and accuracy  0.8812
ep  351  training loss:  0.4425474105368507
valid loss  0.5042000880718231  and accuracy  0.8812
ep  352  training loss:  0.44206575295034567
valid loss  0.48371596546173096  and accuracy  0.8828
ep  353  training loss:  0.44278177230601734
valid loss  0.49379111919403074  and accuracy  0.8816
ep  354  training loss:  0.4437563589869232
valid loss  0.504487149810791  and accuracy  0.8828
ep  355  training loss:  0.44497261190134874
valid loss  0.4922558737754822  and accuracy  0.8808
ep  356  training loss:  0.4409120272252228
valid loss  0.4964481999397278  and accuracy  0.8816
ep  357  training loss:  0.44554521434470834
valid loss  0.4978314528465271  and accuracy  0.884
ep  358  training loss:  0.44082201779188224
valid loss  0.4890983229637146  and accuracy  0.882
ep  359  training loss:  0.44180809260413073
valid loss  0.49548416795730593  and accuracy  0.8816
ep  360  training loss:  0.4409792131315324
valid loss  0.4962061857223511  and accuracy  0.8816
ep  361  training loss:  0.44315022049437414
valid loss  0.508841231250763  and accuracy  0.8804
ep  362  training loss:  0.443177482430859
valid loss  0.5088854175567626  and accuracy  0.8804
ep  363  training loss:  0.44191628751443257
valid loss  0.4964252850532532  and accuracy  0.8816
ep  364  training loss:  0.4393071309605635
valid loss  0.4969123603820801  and accuracy  0.8808
ep  365  training loss:  0.43980535574294816
valid loss  0.5068775217056274  and accuracy  0.88
ep  366  training loss:  0.43729666916768956
valid loss  0.5011164385795593  and accuracy  0.8824
ep  367  training loss:  0.4385782840463584
valid loss  0.5093964850902557  and accuracy  0.8816
ep  368  training loss:  0.43966707198863453
valid loss  0.4944340777873993  and accuracy  0.8804
ep  369  training loss:  0.4377851927500075
valid loss  0.4963487265110016  and accuracy  0.8808
ep  370  training loss:  0.4368287837385532
valid loss  0.5032711616992951  and accuracy  0.8832
ep  371  training loss:  0.4387695896665455
valid loss  0.49903332328796385  and accuracy  0.882
ep  372  training loss:  0.43548900667746465
valid loss  0.507006926727295  and accuracy  0.88
ep  373  training loss:  0.44165636936024805
valid loss  0.5004312569618226  and accuracy  0.8804
ep  374  training loss:  0.4451305720075291
valid loss  0.5019661273956298  and accuracy  0.8808
ep  375  training loss:  0.4415668427145461
valid loss  0.5025564603805542  and accuracy  0.8812
ep  376  training loss:  0.44154776545425556
valid loss  0.4891297577857971  and accuracy  0.8804
ep  377  training loss:  0.4401012471373157
valid loss  0.495445143699646  and accuracy  0.8812
ep  378  training loss:  0.4396369364593097
valid loss  0.508142287158966  and accuracy  0.8796
ep  379  training loss:  0.4422487519693934
valid loss  0.5103549633979797  and accuracy  0.882
ep  380  training loss:  0.4392806051364496
valid loss  0.5046145836830139  and accuracy  0.8844
ep  381  training loss:  0.4416990706269665
valid loss  0.5050418430805206  and accuracy  0.8808
ep  382  training loss:  0.43741704980332646
valid loss  0.49306418170928956  and accuracy  0.8796
ep  383  training loss:  0.43888954391431567
valid loss  0.5089967290878296  and accuracy  0.8804
ep  384  training loss:  0.43826396823728103
valid loss  0.498186071062088  and accuracy  0.8808
ep  385  training loss:  0.4366157266961869
valid loss  0.5053416680335998  and accuracy  0.8812
ep  386  training loss:  0.43533194979630924
valid loss  0.493858217048645  and accuracy  0.8816
ep  387  training loss:  0.43693254449259694
valid loss  0.49521977262496947  and accuracy  0.8824
ep  388  training loss:  0.435576533831943
valid loss  0.4952916395187378  and accuracy  0.8812
ep  389  training loss:  0.4365187732019217
valid loss  0.5161260950088501  and accuracy  0.8816
ep  390  training loss:  0.4364692639865268
valid loss  0.5103501895427703  and accuracy  0.8828
ep  391  training loss:  0.43495461154024007
valid loss  0.506865367603302  and accuracy  0.884
ep  392  training loss:  0.43322986168677685
valid loss  0.5011311549186707  and accuracy  0.8824
ep  393  training loss:  0.43304123830555674
valid loss  0.5159261507987976  and accuracy  0.8812
ep  394  training loss:  0.43191499578293846
valid loss  0.5221476058006287  and accuracy  0.8816
ep  395  training loss:  0.4348683499111802
valid loss  0.49733296642303465  and accuracy  0.8812
ep  396  training loss:  0.43504450654664034
valid loss  0.4904455230712891  and accuracy  0.8816
ep  397  training loss:  0.4329924432776082
valid loss  0.5149415950775147  and accuracy  0.8824
ep  398  training loss:  0.4317639060555591
valid loss  0.5039378277301788  and accuracy  0.8824
ep  399  training loss:  0.43375915051305314
valid loss  0.5073311695575714  and accuracy  0.8808
ep  400  training loss:  0.43585095850866246
valid loss  0.50773385181427  and accuracy  0.8824
ep  401  training loss:  0.43460519555425725
valid loss  0.5013679511070251  and accuracy  0.8836
ep  402  training loss:  0.43945391222060826
valid loss  0.5019288228034973  and accuracy  0.8844
ep  403  training loss:  0.4323859145293883
valid loss  0.508413760471344  and accuracy  0.8816
ep  404  training loss:  0.43467590858788546
valid loss  0.5158673628807068  and accuracy  0.8828
ep  405  training loss:  0.4347474285056643
valid loss  0.5026083399772644  and accuracy  0.8828
ep  406  training loss:  0.43547673644731993
valid loss  0.5068041805267334  and accuracy  0.8836
ep  407  training loss:  0.45169322114294297
valid loss  0.4831100284576416  and accuracy  0.8832
ep  408  training loss:  0.4516742087688478
valid loss  0.5147200835227966  and accuracy  0.8812
ep  409  training loss:  0.4440771759255248
valid loss  0.5029696208000183  and accuracy  0.8792
ep  410  training loss:  0.44145956091345656
valid loss  0.5107259216308594  and accuracy  0.88
ep  411  training loss:  0.44015834529196196
valid loss  0.5137872723579406  and accuracy  0.8804
ep  412  training loss:  0.4380927247517872
valid loss  0.5109517554283142  and accuracy  0.8824
ep  413  training loss:  0.43939236670083537
valid loss  0.5184342907905579  and accuracy  0.8804
ep  414  training loss:  0.4365159280935125
valid loss  0.5221819160938262  and accuracy  0.8832
ep  415  training loss:  0.4372332507541631
valid loss  0.5041539552211761  and accuracy  0.8828
ep  416  training loss:  0.43596369415471703
valid loss  0.5033021894931793  and accuracy  0.8816
ep  417  training loss:  0.4345373245439737
valid loss  0.5054676712036132  and accuracy  0.8788
ep  418  training loss:  0.43497941909323584
valid loss  0.5039897742271423  and accuracy  0.8816
ep  419  training loss:  0.4319243777437226
valid loss  0.5058482737541199  and accuracy  0.8788
ep  420  training loss:  0.4336773801089531
valid loss  0.5082113456726074  and accuracy  0.8816
ep  421  training loss:  0.43329221441518123
valid loss  0.5192655034542084  and accuracy  0.8816
ep  422  training loss:  0.4345506254951758
valid loss  0.504835702085495  and accuracy  0.8804
ep  423  training loss:  0.43883463697417496
valid loss  0.5157839952468872  and accuracy  0.8804
ep  424  training loss:  0.43955579870310263
valid loss  0.5122234347343445  and accuracy  0.8832
ep  425  training loss:  0.4391626989422132
valid loss  0.5028679232597351  and accuracy  0.882
ep  426  training loss:  0.43705020234413083
valid loss  0.5082787322044373  and accuracy  0.882
ep  427  training loss:  0.436625001907748
valid loss  0.5055717517852784  and accuracy  0.8808
ep  428  training loss:  0.43305325304443515
valid loss  0.5151535418510437  and accuracy  0.8804
ep  429  training loss:  0.4356168706612771
valid loss  0.5066874716758728  and accuracy  0.8808
ep  430  training loss:  0.43351723223275673
valid loss  0.5125823935508728  and accuracy  0.8808
ep  431  training loss:  0.433874522201979
valid loss  0.5126677286148071  and accuracy  0.882
ep  432  training loss:  0.4338045071117842
valid loss  0.509539098405838  and accuracy  0.8828
ep  433  training loss:  0.4361587238371672
valid loss  0.5355266834259034  and accuracy  0.882
ep  434  training loss:  0.4337712404017073
valid loss  0.520107899427414  and accuracy  0.8832
ep  435  training loss:  0.4287345381238353
valid loss  0.5233954366683959  and accuracy  0.8788
ep  436  training loss:  0.4316345257475548
valid loss  0.5253155797958374  and accuracy  0.882
ep  437  training loss:  0.4300275763674597
valid loss  0.5116696521759033  and accuracy  0.8816
ep  438  training loss:  0.431805008280417
valid loss  0.5171466968536377  and accuracy  0.882
ep  439  training loss:  0.4318302090343718
valid loss  0.511343261384964  and accuracy  0.8816
ep  440  training loss:  0.4290640329795467
valid loss  0.4923503159046173  and accuracy  0.8828
ep  441  training loss:  0.42783886766313906
valid loss  0.5068453399658203  and accuracy  0.8828
ep  442  training loss:  0.43061980460357024
valid loss  0.5065618663311005  and accuracy  0.882
ep  443  training loss:  0.4273211317445765
valid loss  0.507699011516571  and accuracy  0.8824
ep  444  training loss:  0.4323537772724177
valid loss  0.508129189491272  and accuracy  0.8844
ep  445  training loss:  0.4301212598530691
valid loss  0.5133265827655792  and accuracy  0.8848
ep  446  training loss:  0.4282659937388933
valid loss  0.5041646655082702  and accuracy  0.8828
ep  447  training loss:  0.4262344435631131
valid loss  0.5072037109375  and accuracy  0.8832
ep  448  training loss:  0.427884345388093
valid loss  0.5048582989692688  and accuracy  0.884
ep  449  training loss:  0.4241450729182418
valid loss  0.5113539201259613  and accuracy  0.8852
ep  450  training loss:  0.4267763244446798
valid loss  0.5151755227088928  and accuracy  0.8852
ep  451  training loss:  0.4347686555517379
valid loss  0.5174404544353485  and accuracy  0.8804
ep  452  training loss:  0.42865000776509743
valid loss  0.5131815284729004  and accuracy  0.8804
ep  453  training loss:  0.4264213720857598
valid loss  0.5063823291301728  and accuracy  0.8812
ep  454  training loss:  0.42822348662157556
valid loss  0.5001919381141663  and accuracy  0.8832
ep  455  training loss:  0.42567693715918203
valid loss  0.5142116612434388  and accuracy  0.884
ep  456  training loss:  0.4262375893704656
valid loss  0.5083932845592499  and accuracy  0.882
ep  457  training loss:  0.42354677363456394
valid loss  0.5159386437892913  and accuracy  0.882
ep  458  training loss:  0.42332861279722434
valid loss  0.5245436899185181  and accuracy  0.882
ep  459  training loss:  0.42131964135010236
valid loss  0.5202385200023651  and accuracy  0.8832
ep  460  training loss:  0.4215981097177445
valid loss  0.5098953453063965  and accuracy  0.882
ep  461  training loss:  0.42701704282457104
valid loss  0.5118054350852966  and accuracy  0.8824
ep  462  training loss:  0.42326788095573287
valid loss  0.5131607852935791  and accuracy  0.8824
ep  463  training loss:  0.41963253412614115
valid loss  0.5140520847320557  and accuracy  0.8812
ep  464  training loss:  0.42586468334373717
valid loss  0.5079664453983307  and accuracy  0.884
ep  465  training loss:  0.4230595271192004
valid loss  0.5132038700103759  and accuracy  0.8804
ep  466  training loss:  0.42052088880059707
valid loss  0.5218373849868775  and accuracy  0.8804
ep  467  training loss:  0.4213590724783726
valid loss  0.5170329308986664  and accuracy  0.8836
ep  468  training loss:  0.41956360769431594
valid loss  0.5086496139526367  and accuracy  0.8824
ep  469  training loss:  0.4213713637548475
valid loss  0.5263404213905335  and accuracy  0.8824
ep  470  training loss:  0.4206021294921287
valid loss  0.5196515124797821  and accuracy  0.884
ep  471  training loss:  0.42212968199097334
valid loss  0.515959511756897  and accuracy  0.8832
ep  472  training loss:  0.41878847149348936
valid loss  0.524375874042511  and accuracy  0.8816
ep  473  training loss:  0.4181709987633991
valid loss  0.53467270860672  and accuracy  0.8828
ep  474  training loss:  0.41637514564060485
valid loss  0.5245067484855652  and accuracy  0.882
ep  475  training loss:  0.4180768000121093
valid loss  0.5240821488380432  and accuracy  0.8828
ep  476  training loss:  0.41687066349352225
valid loss  0.5227191756248474  and accuracy  0.8832
ep  477  training loss:  0.41963462338375684
valid loss  0.5145355388641357  and accuracy  0.882
ep  478  training loss:  0.41736960033675535
valid loss  0.5046882813930511  and accuracy  0.8816
ep  479  training loss:  0.4183780853472762
valid loss  0.512846901512146  and accuracy  0.8828
ep  480  training loss:  0.4153534493254657
valid loss  0.5251511569976807  and accuracy  0.8808
ep  481  training loss:  0.4167779801678618
valid loss  0.5278224961280823  and accuracy  0.8832
ep  482  training loss:  0.4156522182363961
valid loss  0.5282760019302368  and accuracy  0.8824
ep  483  training loss:  0.41596837534976366
valid loss  0.5439472249031067  and accuracy  0.8844
ep  484  training loss:  0.4158855743344305
valid loss  0.5201942054748535  and accuracy  0.882
ep  485  training loss:  0.41781260518971797
valid loss  0.5251756041049958  and accuracy  0.8836
ep  486  training loss:  0.41847005701144935
valid loss  0.5094429207801818  and accuracy  0.8824
ep  487  training loss:  0.419412702611543
valid loss  0.51741283659935  and accuracy  0.8824
ep  488  training loss:  0.41586225610881594
valid loss  0.521921990442276  and accuracy  0.8812
ep  489  training loss:  0.4160172865818094
valid loss  0.5240685775756836  and accuracy  0.8832
ep  490  training loss:  0.4159092169510859
valid loss  0.5124934629917145  and accuracy  0.8804
ep  491  training loss:  0.41585299921794555
valid loss  0.5321036761283875  and accuracy  0.8844
ep  492  training loss:  0.4148858278141149
valid loss  0.5382870169639588  and accuracy  0.8828
ep  493  training loss:  0.4144650101761522
valid loss  0.5385576310157776  and accuracy  0.8784
ep  494  training loss:  0.41721538288309745
valid loss  0.5120570314884185  and accuracy  0.884
ep  495  training loss:  0.412754970539355
valid loss  0.5384545690536499  and accuracy  0.8812
ep  496  training loss:  0.41152535353473685
valid loss  0.523381270313263  and accuracy  0.8836
ep  497  training loss:  0.4145403685681584
valid loss  0.5182570788383484  and accuracy  0.8812
ep  498  training loss:  0.41403597350695626
valid loss  0.5178448282241821  and accuracy  0.8844
ep  499  training loss:  0.41211770183876334
valid loss  0.5218072992801667  and accuracy  0.8848

[[  204    39    33     2  1534]
 [   21   262    35    11  1881]
 [   26    49   173     8  1212]
 [    7    50     6    30  1032]
 [   50   130    74    30 43362]]

[[[48345   104]
  [ 1608   204]]

 [[47783   268]
  [ 1948   262]]

 [[48645   148]
  [ 1295   173]]

 [[49085    51]
  [ 1095    30]]

 [[  956  5659]
  [  284 43362]]]

              precision    recall  f1-score   support

           0       0.66      0.11      0.19      1812
           1       0.49      0.12      0.19      2210
           2       0.54      0.12      0.19      1468
           3       0.37      0.03      0.05      1125
           4       0.88      0.99      0.94     43646

    accuracy                           0.88     50261
   macro avg       0.59      0.27      0.31     50261
weighted avg       0.84      0.88      0.83     50261

Accuracy:  0.8760470344800143
Precision_weighted:  0.8377861545518283
Recall_weighted:  0.8760470344800143
mcc:  0.26034152920913534
f2:  0.8681178150001322

time = circa 50 min