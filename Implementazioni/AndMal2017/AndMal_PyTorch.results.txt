- train_loop(model, epochs=1000, lr=0.002)

training loss:  1.0170258551713887
valid loss 0.608 and accuracy 0.859
training loss:  0.5802822986733851
valid loss 0.588 and accuracy 0.859
training loss:  0.5644702077964294
valid loss 0.583 and accuracy 0.860
training loss:  0.5550257421642777
valid loss 0.580 and accuracy 0.860
training loss:  0.5501211015426501
valid loss 0.577 and accuracy 0.860
training loss:  0.5480137213284739
valid loss 0.572 and accuracy 0.860
training loss:  0.546066517984129
valid loss 0.571 and accuracy 0.860
training loss:  0.5408497309912726
valid loss 0.569 and accuracy 0.861
training loss:  0.5394960759723804
valid loss 0.567 and accuracy 0.860
training loss:  0.5358397479161723
valid loss 0.563 and accuracy 0.860
training loss:  0.5318583777311559
valid loss 0.564 and accuracy 0.860
training loss:  0.5324683051478348
valid loss 0.567 and accuracy 0.861
training loss:  0.5294999925608387
valid loss 0.565 and accuracy 0.860
training loss:  0.5275614974424573
valid loss 0.558 and accuracy 0.860
training loss:  0.5264039319121375
valid loss 0.560 and accuracy 0.860
training loss:  0.5235475170798018
valid loss 0.558 and accuracy 0.860
training loss:  0.5233371023854805
valid loss 0.553 and accuracy 0.860
training loss:  0.5224881901164097
valid loss 0.555 and accuracy 0.860
training loss:  0.520862258218373
valid loss 0.555 and accuracy 0.860
training loss:  0.5185253456520201
valid loss 0.552 and accuracy 0.860
training loss:  0.5184249529882504
valid loss 0.551 and accuracy 0.860
training loss:  0.5156746928683454
valid loss 0.549 and accuracy 0.860
training loss:  0.514448706134092
valid loss 0.548 and accuracy 0.860
training loss:  0.516567579017322
valid loss 0.547 and accuracy 0.860
training loss:  0.5143919174427014
valid loss 0.546 and accuracy 0.862
training loss:  0.5132452459964221
valid loss 0.544 and accuracy 0.861
training loss:  0.5121694253581961
valid loss 0.545 and accuracy 0.860
training loss:  0.5116630709497699
valid loss 0.544 and accuracy 0.861
training loss:  0.5114682365278966
valid loss 0.550 and accuracy 0.860
training loss:  0.5106530343310646
valid loss 0.538 and accuracy 0.861
training loss:  0.5090405534122058
valid loss 0.545 and accuracy 0.861
training loss:  0.5099581362673791
valid loss 0.539 and accuracy 0.862
training loss:  0.5075464182489253
valid loss 0.538 and accuracy 0.864
training loss:  0.5083753527566955
valid loss 0.538 and accuracy 0.863
training loss:  0.5042289790058406
valid loss 0.537 and accuracy 0.862
training loss:  0.5057844824054686
valid loss 0.536 and accuracy 0.863
training loss:  0.506073906999176
valid loss 0.536 and accuracy 0.862
training loss:  0.5056235256315422
valid loss 0.534 and accuracy 0.861
training loss:  0.5034742477997433
valid loss 0.537 and accuracy 0.863
training loss:  0.5045960275107887
valid loss 0.543 and accuracy 0.861
training loss:  0.5029743417549742
valid loss 0.589 and accuracy 0.863
training loss:  0.502364345348006
valid loss 0.741 and accuracy 0.861
training loss:  0.5019501037739981
valid loss 0.667 and accuracy 0.862
training loss:  0.5007163181585936
valid loss 0.618 and accuracy 0.861
training loss:  0.5002440055900812
valid loss 0.548 and accuracy 0.861
training loss:  0.5001330642980276
valid loss 0.619 and accuracy 0.862
training loss:  0.5009170085524085
valid loss 0.621 and accuracy 0.863
training loss:  0.4999783052217244
valid loss 0.551 and accuracy 0.864
training loss:  0.49963704228107103
valid loss 0.596 and accuracy 0.865
training loss:  0.4982026594678314
valid loss 0.533 and accuracy 0.865
training loss:  0.49858977935295173
valid loss 0.581 and accuracy 0.865
training loss:  0.4984132215131855
valid loss 0.585 and accuracy 0.861
training loss:  0.497312774559577
valid loss 0.714 and accuracy 0.864
training loss:  0.4975725127718894
valid loss 0.685 and accuracy 0.864
training loss:  0.4955402866093089
valid loss 0.863 and accuracy 0.865
training loss:  0.49886321103153114
valid loss 0.593 and accuracy 0.865
training loss:  0.49746375970756457
valid loss 0.574 and accuracy 0.863
training loss:  0.4974757537961811
valid loss 0.563 and accuracy 0.864
training loss:  0.4961913873394485
valid loss 0.530 and accuracy 0.865
training loss:  0.49543190614115407
valid loss 0.584 and accuracy 0.862
training loss:  0.4964572482924327
valid loss 0.526 and accuracy 0.864
training loss:  0.49553959272661674
valid loss 0.545 and accuracy 0.866
training loss:  0.49392859516611604
valid loss 0.705 and accuracy 0.864
training loss:  0.4929232760012489
valid loss 0.553 and accuracy 0.864
training loss:  0.4915809695255346
valid loss 0.549 and accuracy 0.867
training loss:  0.4930268785049846
valid loss 0.571 and accuracy 0.864
training loss:  0.4940882033024722
valid loss 0.526 and accuracy 0.867
training loss:  0.4936095274724023
valid loss 0.670 and accuracy 0.865
training loss:  0.4934782967472366
valid loss 0.702 and accuracy 0.865
training loss:  0.49022947787064636
valid loss 0.531 and accuracy 0.865
training loss:  0.49156465203857475
valid loss 0.673 and accuracy 0.863
training loss:  0.4915621784591838
valid loss 0.523 and accuracy 0.864
training loss:  0.49156371059591547
valid loss 0.544 and accuracy 0.865
training loss:  0.488325053241878
valid loss 0.573 and accuracy 0.865
training loss:  0.4892874170785801
valid loss 0.559 and accuracy 0.867
training loss:  0.48915715439632795
valid loss 0.745 and accuracy 0.867
training loss:  0.48977872938256145
valid loss 0.562 and accuracy 0.863
training loss:  0.48921689408452856
valid loss 0.581 and accuracy 0.866
training loss:  0.4912105903040683
valid loss 0.526 and accuracy 0.864
training loss:  0.49146717230710624
valid loss 0.526 and accuracy 0.866
training loss:  0.4892779955521459
valid loss 0.521 and accuracy 0.866
training loss:  0.49079379685433183
valid loss 0.600 and accuracy 0.868
training loss:  0.48889731686035887
valid loss 0.532 and accuracy 0.866
training loss:  0.4887162261930197
valid loss 0.552 and accuracy 0.867
training loss:  0.48839478539437187
valid loss 0.516 and accuracy 0.866
training loss:  0.486706895539903
valid loss 0.520 and accuracy 0.867
training loss:  0.4874285856805768
valid loss 0.645 and accuracy 0.865
training loss:  0.48756178006155826
valid loss 0.523 and accuracy 0.865
training loss:  0.4871321005552327
valid loss 0.518 and accuracy 0.867
training loss:  0.48905200218276496
valid loss 0.516 and accuracy 0.865
training loss:  0.48626220789292834
valid loss 0.521 and accuracy 0.866
training loss:  0.486881847935021
valid loss 0.523 and accuracy 0.865
training loss:  0.48623397697301235
valid loss 0.524 and accuracy 0.864
training loss:  0.4857277780641836
valid loss 0.516 and accuracy 0.870
training loss:  0.4843467044912713
valid loss 0.521 and accuracy 0.865
training loss:  0.48640301892457455
valid loss 0.517 and accuracy 0.865
training loss:  0.48363167596287693
valid loss 0.517 and accuracy 0.865
training loss:  0.48431897491010684
valid loss 0.519 and accuracy 0.868
training loss:  0.4820454904006457
valid loss 0.829 and accuracy 0.868
training loss:  0.48383816856487694
valid loss 0.658 and accuracy 0.866
training loss:  0.48468033853866593
valid loss 0.672 and accuracy 0.866
training loss:  0.48458114468119784
valid loss 0.633 and accuracy 0.866
training loss:  0.48392454953210096
valid loss 0.699 and accuracy 0.868
training loss:  0.48616633937226106
valid loss 0.615 and accuracy 0.864
training loss:  0.48455230799440036
valid loss 0.582 and accuracy 0.868
training loss:  0.4841591592554327
valid loss 0.828 and accuracy 0.867
training loss:  0.482931446511679
valid loss 1.050 and accuracy 0.866
training loss:  0.4816500722879532
valid loss 0.722 and accuracy 0.868
training loss:  0.4837491377872483
valid loss 0.618 and accuracy 0.866
training loss:  0.4825670156955923
valid loss 0.519 and accuracy 0.867
training loss:  0.4858969055929909
valid loss 0.613 and accuracy 0.865
training loss:  0.4844875786540904
valid loss 0.521 and accuracy 0.865
training loss:  0.4829815250979901
valid loss 0.640 and accuracy 0.869
training loss:  0.48240187994388584
valid loss 0.614 and accuracy 0.864
training loss:  0.48182759640554607
valid loss 0.684 and accuracy 0.868
training loss:  0.48261463434806195
valid loss 0.849 and accuracy 0.866
training loss:  0.47938770238234074
valid loss 0.638 and accuracy 0.867
training loss:  0.4827933547151628
valid loss 0.562 and accuracy 0.869
training loss:  0.48094968427909174
valid loss 0.816 and accuracy 0.870
training loss:  0.48196753875135356
valid loss 0.814 and accuracy 0.863
training loss:  0.4814566652963994
valid loss 0.668 and accuracy 0.867
training loss:  0.48021537286308935
valid loss 0.519 and accuracy 0.866
training loss:  0.47982639371504215
valid loss 0.575 and accuracy 0.867
training loss:  0.48159323597628056
valid loss 0.532 and accuracy 0.864
training loss:  0.4810179627066361
valid loss 0.519 and accuracy 0.868
training loss:  0.4805929755710793
valid loss 0.565 and accuracy 0.866
training loss:  0.4814312511331194
valid loss 0.622 and accuracy 0.867
training loss:  0.47871432236282174
valid loss 0.612 and accuracy 0.866
training loss:  0.4806230414728887
valid loss 0.780 and accuracy 0.866
training loss:  0.4818317357797589
valid loss 1.175 and accuracy 0.866
training loss:  0.47963134183081924
valid loss 0.517 and accuracy 0.869
training loss:  0.48188718283012855
valid loss 0.963 and accuracy 0.865
training loss:  0.47896752181837016
valid loss 3.797 and accuracy 0.866
training loss:  0.47818246308813117
valid loss 0.528 and accuracy 0.863
training loss:  0.4785987207917701
valid loss 0.518 and accuracy 0.865
training loss:  0.47892435851193155
valid loss 2.677 and accuracy 0.866
training loss:  0.4798696347448487
valid loss 0.516 and accuracy 0.866
training loss:  0.4789481229915923
valid loss 0.518 and accuracy 0.869
training loss:  0.4796259692630096
valid loss 0.518 and accuracy 0.869
training loss:  0.47851580737934907
valid loss 0.520 and accuracy 0.866
training loss:  0.4787432310651641
valid loss 0.522 and accuracy 0.864
training loss:  0.47795265380810126
valid loss 0.519 and accuracy 0.869
training loss:  0.4790156947148048
valid loss 0.513 and accuracy 0.869
training loss:  0.4791903564897346
valid loss 0.521 and accuracy 0.865
training loss:  0.47872563598097595
valid loss 0.601 and accuracy 0.865
training loss:  0.47695285459488707
valid loss 0.513 and accuracy 0.867
training loss:  0.4780408458869249
valid loss 0.692 and accuracy 0.867
training loss:  0.4772405841036595
valid loss 0.513 and accuracy 0.867
training loss:  0.47747813097288194
valid loss 0.513 and accuracy 0.868
training loss:  0.47724725912027
valid loss 0.575 and accuracy 0.870
training loss:  0.4770168876804225
valid loss 0.514 and accuracy 0.866
training loss:  0.48027523818901985
valid loss 0.517 and accuracy 0.866
training loss:  0.47750803807641695
valid loss 0.516 and accuracy 0.868
training loss:  0.4771686891261055
valid loss 0.518 and accuracy 0.865
training loss:  0.4764056101083809
valid loss 0.516 and accuracy 0.869
training loss:  0.47710601865604857
valid loss 0.520 and accuracy 0.865
training loss:  0.4759950761434225
valid loss 0.520 and accuracy 0.865
training loss:  0.47681973458451987
valid loss 0.517 and accuracy 0.866
training loss:  0.47620903412966037
valid loss 1.020 and accuracy 0.865
training loss:  0.47636357955450037
valid loss 0.513 and accuracy 0.868
training loss:  0.476455663620129
valid loss 0.602 and accuracy 0.865
training loss:  0.4755211498237205
valid loss 0.515 and accuracy 0.867
training loss:  0.4755327788668502
valid loss 0.517 and accuracy 0.870
training loss:  0.47673986613046854
valid loss 0.671 and accuracy 0.866
training loss:  0.4756367027646538
valid loss 0.518 and accuracy 0.866
training loss:  0.47555853763038164
valid loss 0.593 and accuracy 0.866
training loss:  0.4759865362812283
valid loss 0.514 and accuracy 0.868
training loss:  0.47479719283148214
valid loss 0.514 and accuracy 0.867
training loss:  0.4762176342964581
valid loss 0.515 and accuracy 0.869
training loss:  0.47760245858907197
valid loss 0.755 and accuracy 0.868
training loss:  0.47575240888049514
valid loss 0.511 and accuracy 0.868
training loss:  0.474672427066651
valid loss 0.535 and accuracy 0.867
training loss:  0.4747921451800154
valid loss 0.516 and accuracy 0.869
training loss:  0.4730763770286727
valid loss 0.545 and accuracy 0.865
training loss:  0.4744674912178129
valid loss 0.941 and accuracy 0.865
training loss:  0.4764141684076823
valid loss 0.708 and accuracy 0.867
training loss:  0.4755351930184088
valid loss 0.512 and accuracy 0.868
training loss:  0.47448273715910877
valid loss 0.608 and accuracy 0.867
training loss:  0.4760755944404887
valid loss 0.577 and accuracy 0.867
training loss:  0.4756887112872414
valid loss 0.524 and accuracy 0.867
training loss:  0.4738638220202748
valid loss 0.515 and accuracy 0.866
training loss:  0.47406551120050283
valid loss 0.525 and accuracy 0.866
training loss:  0.47343064116790984
valid loss 0.515 and accuracy 0.864
training loss:  0.4754960257562673
valid loss 0.513 and accuracy 0.867
training loss:  0.47603928447122124
valid loss 0.517 and accuracy 0.866
training loss:  0.4738421239943608
valid loss 0.512 and accuracy 0.869
training loss:  0.47308035861687786
valid loss 0.512 and accuracy 0.869
training loss:  0.47418148420294937
valid loss 0.570 and accuracy 0.865
training loss:  0.47360961682672675
valid loss 0.610 and accuracy 0.867
training loss:  0.4737093448225746
valid loss 0.517 and accuracy 0.866
training loss:  0.4730714150480219
valid loss 0.511 and accuracy 0.868
training loss:  0.47556451847386577
valid loss 0.587 and accuracy 0.870
training loss:  0.4747769628997223
valid loss 0.514 and accuracy 0.869
training loss:  0.4737879910180614
valid loss 0.516 and accuracy 0.864
training loss:  0.47376409011024806
valid loss 0.572 and accuracy 0.869
training loss:  0.47387666070205353
valid loss 0.533 and accuracy 0.867
training loss:  0.47285264908967267
valid loss 0.508 and accuracy 0.867
training loss:  0.4741057612459269
valid loss 0.549 and accuracy 0.866
training loss:  0.4716848062902404
valid loss 0.529 and accuracy 0.867
training loss:  0.47312050820242757
valid loss 0.523 and accuracy 0.869
training loss:  0.47266157975913614
valid loss 0.515 and accuracy 0.866
training loss:  0.471925312402054
valid loss 0.548 and accuracy 0.865
training loss:  0.47386899695060736
valid loss 0.514 and accuracy 0.869
training loss:  0.4734629466095263
valid loss 0.512 and accuracy 0.866
training loss:  0.47311766185005416
valid loss 0.546 and accuracy 0.867
training loss:  0.47263442989326443
valid loss 0.558 and accuracy 0.867
training loss:  0.4718670930837442
valid loss 0.542 and accuracy 0.867
training loss:  0.47260946550450245
valid loss 0.510 and accuracy 0.866
training loss:  0.47216827591218147
valid loss 0.518 and accuracy 0.868
training loss:  0.47193808946316096
valid loss 0.546 and accuracy 0.868
training loss:  0.4690103334895347
valid loss 0.512 and accuracy 0.869
training loss:  0.472412010621275
valid loss 0.512 and accuracy 0.865
training loss:  0.4731532701441203
valid loss 0.508 and accuracy 0.871
training loss:  0.4710863834680404
valid loss 0.517 and accuracy 0.867
training loss:  0.47128964032795617
valid loss 0.512 and accuracy 0.868
training loss:  0.4723156430822923
valid loss 0.572 and accuracy 0.866
training loss:  0.47063990435266534
valid loss 0.631 and accuracy 0.865
training loss:  0.4722474701267399
valid loss 0.545 and accuracy 0.868
training loss:  0.47239888338819647
valid loss 0.514 and accuracy 0.865
training loss:  0.4743611910032855
valid loss 0.523 and accuracy 0.865
training loss:  0.4722992893038094
valid loss 0.511 and accuracy 0.865
training loss:  0.47198433693963265
valid loss 0.515 and accuracy 0.867
training loss:  0.4708157089335534
valid loss 0.557 and accuracy 0.869
training loss:  0.47327307466896873
valid loss 0.509 and accuracy 0.865
training loss:  0.4704579105660146
valid loss 0.512 and accuracy 0.868
training loss:  0.47015112438572104
valid loss 0.637 and accuracy 0.865
training loss:  0.47049685251192935
valid loss 0.513 and accuracy 0.867
training loss:  0.46924022132777915
valid loss 0.512 and accuracy 0.869
training loss:  0.4709265842800167
valid loss 0.508 and accuracy 0.869
training loss:  0.470023157173745
valid loss 0.543 and accuracy 0.865
training loss:  0.4694044878218924
valid loss 0.510 and accuracy 0.868
training loss:  0.47029113651109417
valid loss 0.513 and accuracy 0.866
training loss:  0.4717667042551649
valid loss 1.019 and accuracy 0.865
training loss:  0.47018660050667876
valid loss 0.507 and accuracy 0.869
training loss:  0.4698712380335269
valid loss 0.511 and accuracy 0.869
training loss:  0.47146305500186275
valid loss 0.511 and accuracy 0.869
training loss:  0.4710994384923563
valid loss 0.735 and accuracy 0.865
training loss:  0.4715135620054444
valid loss 0.507 and accuracy 0.869
training loss:  0.4708285482879487
valid loss 0.507 and accuracy 0.869
training loss:  0.4704780425564147
valid loss 0.506 and accuracy 0.867
training loss:  0.4689689255787105
valid loss 0.510 and accuracy 0.867
training loss:  0.46809605621737915
valid loss 0.945 and accuracy 0.865
training loss:  0.4704996793395861
valid loss 0.604 and accuracy 0.867
training loss:  0.4690574846169706
valid loss 0.511 and accuracy 0.869
training loss:  0.4726737375141427
valid loss 0.511 and accuracy 0.868
training loss:  0.4687548269834034
valid loss 0.524 and accuracy 0.868
training loss:  0.4690541205989439
valid loss 0.511 and accuracy 0.867
training loss:  0.4703826464234284
valid loss 0.542 and accuracy 0.868
training loss:  0.47071908625475545
valid loss 0.508 and accuracy 0.869
training loss:  0.4684612695563301
valid loss 0.507 and accuracy 0.871
training loss:  0.46853726873638635
valid loss 0.508 and accuracy 0.867
training loss:  0.4712121691201236
valid loss 0.505 and accuracy 0.869
training loss:  0.4679934964844419
valid loss 0.507 and accuracy 0.871
training loss:  0.46848086428891633
valid loss 0.509 and accuracy 0.869
training loss:  0.4696398222601259
valid loss 0.510 and accuracy 0.867
training loss:  0.46984172596798074
valid loss 0.509 and accuracy 0.866
training loss:  0.4685168704742753
valid loss 0.512 and accuracy 0.868
training loss:  0.46834698285105547
valid loss 0.512 and accuracy 0.868
training loss:  0.4698090218968337
valid loss 0.513 and accuracy 0.869
training loss:  0.4685690120698314
valid loss 0.509 and accuracy 0.867
training loss:  0.46932786364127993
valid loss 0.509 and accuracy 0.869
training loss:  0.46807859244957917
valid loss 0.506 and accuracy 0.866
training loss:  0.46772614237063936
valid loss 0.505 and accuracy 0.868
training loss:  0.4678837507516592
valid loss 0.504 and accuracy 0.871
training loss:  0.4713886383288317
valid loss 0.511 and accuracy 0.868
training loss:  0.46834596396574085
valid loss 0.547 and accuracy 0.866
training loss:  0.4683341577917373
valid loss 0.507 and accuracy 0.870
training loss:  0.47039615553502884
valid loss 0.514 and accuracy 0.869
training loss:  0.47048503409107856
valid loss 0.744 and accuracy 0.866
training loss:  0.47204927479412195
valid loss 0.506 and accuracy 0.870
training loss:  0.4691951613622683
valid loss 0.509 and accuracy 0.867
training loss:  0.4691119345548736
valid loss 0.507 and accuracy 0.870
training loss:  0.46776993287958774
valid loss 0.508 and accuracy 0.867
training loss:  0.46925651168587007
valid loss 0.506 and accuracy 0.871
training loss:  0.46906560738214687
valid loss 0.507 and accuracy 0.870
training loss:  0.4671405612961831
valid loss 0.509 and accuracy 0.869
training loss:  0.46736480945884523
valid loss 0.507 and accuracy 0.865
training loss:  0.4664798935271833
valid loss 0.626 and accuracy 0.865
training loss:  0.4684622927076258
valid loss 0.511 and accuracy 0.866
training loss:  0.4690784220083129
valid loss 0.506 and accuracy 0.868
training loss:  0.4664921340869422
valid loss 0.508 and accuracy 0.866
training loss:  0.46797866412153466
valid loss 0.509 and accuracy 0.867
training loss:  0.46709819506166395
valid loss 0.714 and accuracy 0.868
training loss:  0.4682721752699135
valid loss 0.508 and accuracy 0.870
training loss:  0.46827897985544564
valid loss 0.501 and accuracy 0.871
training loss:  0.4680266768860062
valid loss 0.505 and accuracy 0.870
training loss:  0.4692389120963053
valid loss 0.508 and accuracy 0.869
training loss:  0.4678326144262484
valid loss 0.507 and accuracy 0.870
training loss:  0.4660820521683296
valid loss 0.509 and accuracy 0.869
training loss:  0.46688902533203336
valid loss 1.618 and accuracy 0.868
training loss:  0.46730511480922127
valid loss 0.503 and accuracy 0.869
training loss:  0.46662179191250225
valid loss 0.507 and accuracy 0.867
training loss:  0.4688377980688991
valid loss 0.505 and accuracy 0.869
training loss:  0.4668112719503027
valid loss 0.508 and accuracy 0.868
training loss:  0.4684551276360601
valid loss 0.508 and accuracy 0.870
training loss:  0.4681410870604754
valid loss 0.502 and accuracy 0.870
training loss:  0.46618805812859493
valid loss 0.503 and accuracy 0.870
training loss:  0.46817111041791876
valid loss 0.507 and accuracy 0.869
training loss:  0.46705900016816576
valid loss 0.508 and accuracy 0.865
training loss:  0.46538260026554973
valid loss 1.233 and accuracy 0.863
training loss:  0.46730427286338555
valid loss 0.511 and accuracy 0.866
training loss:  0.46651530094342525
valid loss 0.506 and accuracy 0.870
training loss:  0.46735837885238757
valid loss 0.508 and accuracy 0.870
training loss:  0.4653426487999139
valid loss 0.507 and accuracy 0.866
training loss:  0.4666226248220653
valid loss 0.508 and accuracy 0.867
training loss:  0.4673168604331497
valid loss 0.504 and accuracy 0.871
training loss:  0.46825522742576875
valid loss 0.553 and accuracy 0.868
training loss:  0.4664834265287344
valid loss 0.541 and accuracy 0.866
training loss:  0.46677654273147995
valid loss 0.510 and accuracy 0.869
training loss:  0.4669892508885569
valid loss 1.259 and accuracy 0.868
training loss:  0.46762958247181974
valid loss 0.514 and accuracy 0.868
training loss:  0.46612265775632866
valid loss 0.505 and accuracy 0.869
training loss:  0.46560811569436533
valid loss 0.508 and accuracy 0.867
training loss:  0.4657657839777555
valid loss 0.505 and accuracy 0.866
training loss:  0.46662649002147427
valid loss 0.508 and accuracy 0.868
training loss:  0.466104996869337
valid loss 0.508 and accuracy 0.870
training loss:  0.4635947938480543
valid loss 0.509 and accuracy 0.867
training loss:  0.46859920244155767
valid loss 0.505 and accuracy 0.868
training loss:  0.46515141768291113
valid loss 0.504 and accuracy 0.869
training loss:  0.4655837748618871
valid loss 0.505 and accuracy 0.870
training loss:  0.4657598568403057
valid loss 0.506 and accuracy 0.871
training loss:  0.46508776503466315
valid loss 0.504 and accuracy 0.870
training loss:  0.46607408277370965
valid loss 0.507 and accuracy 0.867
training loss:  0.4668372371329982
valid loss 0.504 and accuracy 0.870
training loss:  0.4646887388765811
valid loss 0.541 and accuracy 0.870
training loss:  0.4670223860145794
valid loss 0.507 and accuracy 0.869
training loss:  0.4657912590052414
valid loss 0.504 and accuracy 0.870
training loss:  0.465101816716351
valid loss 0.504 and accuracy 0.870
training loss:  0.4661323427643605
valid loss 0.505 and accuracy 0.868
training loss:  0.466071916753265
valid loss 0.777 and accuracy 0.869
training loss:  0.4650423144277791
valid loss 0.507 and accuracy 0.870
training loss:  0.4649754026252858
valid loss 0.504 and accuracy 0.873
training loss:  0.46581272188937956
valid loss 0.507 and accuracy 0.869
training loss:  0.46562391091976235
valid loss 0.507 and accuracy 0.869
training loss:  0.4684019753013236
valid loss 0.502 and accuracy 0.870
training loss:  0.4655719383204904
valid loss 0.583 and accuracy 0.866
training loss:  0.4630192044972231
valid loss 0.509 and accuracy 0.870
training loss:  0.4636914811551592
valid loss 0.521 and accuracy 0.869
training loss:  0.4652646801984965
valid loss 0.508 and accuracy 0.867
training loss:  0.4632171115057848
valid loss 0.504 and accuracy 0.868
training loss:  0.4636556807692091
valid loss 0.508 and accuracy 0.867
training loss:  0.4640178409945421
valid loss 1.113 and accuracy 0.865
training loss:  0.4643501727608342
valid loss 0.503 and accuracy 0.871
training loss:  0.4660206853622797
valid loss 0.882 and accuracy 0.865
training loss:  0.4647731982948812
valid loss 0.510 and accuracy 0.870
training loss:  0.46398920940537197
valid loss 1.276 and accuracy 0.866
training loss:  0.4637262801565855
valid loss 0.513 and accuracy 0.868
training loss:  0.4654184362868115
valid loss 0.509 and accuracy 0.869
training loss:  0.46632173909086483
valid loss 0.509 and accuracy 0.866
training loss:  0.46404069932355957
valid loss 0.506 and accuracy 0.870
training loss:  0.46370482726812523
valid loss 0.505 and accuracy 0.870
training loss:  0.46420851712873457
valid loss 0.575 and accuracy 0.869
training loss:  0.46463203059112596
valid loss 0.616 and accuracy 0.868
training loss:  0.4645771319517319
valid loss 0.664 and accuracy 0.867
training loss:  0.46515438941842874
valid loss 0.585 and accuracy 0.866
training loss:  0.463901780661308
valid loss 0.907 and accuracy 0.868
training loss:  0.46375431180639504
valid loss 0.658 and accuracy 0.867
training loss:  0.46566756323668645
valid loss 0.579 and accuracy 0.869
training loss:  0.4647311482490945
valid loss 0.509 and accuracy 0.865
training loss:  0.4638492994514911
valid loss 0.855 and accuracy 0.868
training loss:  0.46428408725353704
valid loss 0.612 and accuracy 0.868
training loss:  0.46411304955803395
valid loss 0.509 and accuracy 0.869
training loss:  0.4643253545747352
valid loss 0.503 and accuracy 0.871
training loss:  0.46408258571328537
valid loss 0.616 and accuracy 0.870
training loss:  0.4654425202824391
valid loss 0.506 and accuracy 0.870
training loss:  0.4632500745458616
valid loss 0.619 and accuracy 0.871
training loss:  0.46355839578059665
valid loss 0.543 and accuracy 0.865
training loss:  0.46215053978550874
valid loss 0.500 and accuracy 0.870
training loss:  0.4636823151087979
valid loss 0.502 and accuracy 0.870
training loss:  0.46275244152833445
valid loss 0.506 and accuracy 0.872
training loss:  0.4634987587451099
valid loss 0.508 and accuracy 0.869
training loss:  0.4630332287450253
valid loss 0.506 and accuracy 0.869
training loss:  0.46294444074309726
valid loss 0.501 and accuracy 0.872
training loss:  0.4641177132469003
valid loss 0.509 and accuracy 0.870
training loss:  0.4632210338557298
valid loss 0.508 and accuracy 0.870
training loss:  0.465672303933772
valid loss 0.531 and accuracy 0.869
training loss:  0.46373511520875055
valid loss 0.507 and accuracy 0.869
training loss:  0.4640423532555394
valid loss 0.691 and accuracy 0.866
training loss:  0.46242419822701775
valid loss 0.506 and accuracy 0.870
training loss:  0.46272463546695786
valid loss 0.503 and accuracy 0.870
training loss:  0.4621752013145265
valid loss 0.509 and accuracy 0.870
training loss:  0.46242934763802185
valid loss 0.507 and accuracy 0.871
training loss:  0.4627068632105789
valid loss 0.578 and accuracy 0.867
training loss:  0.4618635619048078
valid loss 0.505 and accuracy 0.866
training loss:  0.4620780755367509
valid loss 0.509 and accuracy 0.867
training loss:  0.4640935759845355
valid loss 0.508 and accuracy 0.868
training loss:  0.46437167242379307
valid loss 0.508 and accuracy 0.870
training loss:  0.4625555554320512
valid loss 0.513 and accuracy 0.867
training loss:  0.464064459176323
valid loss 0.509 and accuracy 0.866
training loss:  0.46403084679693424
valid loss 0.507 and accuracy 0.867
training loss:  0.4635570998398321
valid loss 0.505 and accuracy 0.868
training loss:  0.4629933143535052
valid loss 0.505 and accuracy 0.870
training loss:  0.46444674077601544
valid loss 0.510 and accuracy 0.865
training loss:  0.46253820183848326
valid loss 0.505 and accuracy 0.871
training loss:  0.463990472892842
valid loss 0.511 and accuracy 0.870
training loss:  0.461825056838859
valid loss 0.514 and accuracy 0.867
training loss:  0.4635210815546658
valid loss 0.509 and accuracy 0.869
training loss:  0.46170352010149074
valid loss 0.504 and accuracy 0.867
training loss:  0.4602140469342946
valid loss 0.507 and accuracy 0.870
training loss:  0.4612039301950231
valid loss 0.502 and accuracy 0.871
training loss:  0.4637860861822804
valid loss 0.509 and accuracy 0.870
training loss:  0.46247849730437274
valid loss 0.510 and accuracy 0.869
training loss:  0.46253271799485723
valid loss 0.506 and accuracy 0.870
training loss:  0.46236838641369676
valid loss 0.508 and accuracy 0.871
training loss:  0.4620337577322484
valid loss 0.503 and accuracy 0.871
training loss:  0.4631404998934415
valid loss 0.526 and accuracy 0.870
training loss:  0.4627075920334849
valid loss 0.507 and accuracy 0.870
training loss:  0.4624852692920009
valid loss 0.502 and accuracy 0.867
training loss:  0.4630072610681192
valid loss 0.503 and accuracy 0.870
training loss:  0.4642142152575538
valid loss 0.504 and accuracy 0.872
training loss:  0.4637256594566437
valid loss 0.511 and accuracy 0.870
training loss:  0.46216871711140306
valid loss 0.506 and accuracy 0.869
training loss:  0.4611140815496527
valid loss 0.507 and accuracy 0.871
training loss:  0.4611134481708579
valid loss 0.502 and accuracy 0.872
training loss:  0.46227370874884516
valid loss 0.506 and accuracy 0.868
training loss:  0.4607319423235088
valid loss 0.785 and accuracy 0.869
training loss:  0.4630196007212532
valid loss 0.502 and accuracy 0.871
training loss:  0.4645284861049758
valid loss 0.505 and accuracy 0.870
training loss:  0.46174205375030264
valid loss 0.529 and accuracy 0.870
training loss:  0.4621374407701238
valid loss 0.505 and accuracy 0.866
training loss:  0.46177657760840585
valid loss 0.500 and accuracy 0.869
training loss:  0.46312773658239176
valid loss 0.506 and accuracy 0.872
training loss:  0.4633888528098822
valid loss 0.505 and accuracy 0.870
training loss:  0.463328934223898
valid loss 0.503 and accuracy 0.871
training loss:  0.4617887267172209
valid loss 0.502 and accuracy 0.869
training loss:  0.4593525800609635
valid loss 0.502 and accuracy 0.871
training loss:  0.46133564861946685
valid loss 0.505 and accuracy 0.871
training loss:  0.4619438120678598
valid loss 0.505 and accuracy 0.870
training loss:  0.4610058133969232
valid loss 0.507 and accuracy 0.870
training loss:  0.46243768706837257
valid loss 0.506 and accuracy 0.865
training loss:  0.46154419367421634
valid loss 0.508 and accuracy 0.870
training loss:  0.46191905767339825
valid loss 0.505 and accuracy 0.870
training loss:  0.4605529366837342
valid loss 0.507 and accuracy 0.868
training loss:  0.46174228315205235
valid loss 0.507 and accuracy 0.870
training loss:  0.46093986225071304
valid loss 0.508 and accuracy 0.872
training loss:  0.4632820393463681
valid loss 0.506 and accuracy 0.867
training loss:  0.4629932881348257
valid loss 0.509 and accuracy 0.869
training loss:  0.4606147748700593
valid loss 0.511 and accuracy 0.865
training loss:  0.46164756397879786
valid loss 0.506 and accuracy 0.867
training loss:  0.46172537301939903
valid loss 0.506 and accuracy 0.867
training loss:  0.46288117582155064
valid loss 0.509 and accuracy 0.867
training loss:  0.46103778269855317
valid loss 0.509 and accuracy 0.867
training loss:  0.4609714700467516
valid loss 0.510 and accuracy 0.867
training loss:  0.4623113885045224
valid loss 0.509 and accuracy 0.866
training loss:  0.46244225488537527
valid loss 0.509 and accuracy 0.870
training loss:  0.4618515931945559
valid loss 0.527 and accuracy 0.866
training loss:  0.46107038460246125
valid loss 0.508 and accuracy 0.871
training loss:  0.45909512926575635
valid loss 0.508 and accuracy 0.870
training loss:  0.4614975513481127
valid loss 0.506 and accuracy 0.867
training loss:  0.4615755742321797
valid loss 0.508 and accuracy 0.866
training loss:  0.4612814288349185
valid loss 0.506 and accuracy 0.866
training loss:  0.4605055812977707
valid loss 0.507 and accuracy 0.867
training loss:  0.45883024380854476
valid loss 0.511 and accuracy 0.867
training loss:  0.45946369914860874
valid loss 0.508 and accuracy 0.869
training loss:  0.4605590695871748
valid loss 0.506 and accuracy 0.870
training loss:  0.46125650235633414
valid loss 0.506 and accuracy 0.870
training loss:  0.45968392835000926
valid loss 0.501 and accuracy 0.871
training loss:  0.45934740716488603
valid loss 0.510 and accuracy 0.869
training loss:  0.4594931849450442
valid loss 0.505 and accuracy 0.870
training loss:  0.46140800363152407
valid loss 0.503 and accuracy 0.871
training loss:  0.46298299638713986
valid loss 0.505 and accuracy 0.865
training loss:  0.46195960546133363
valid loss 0.511 and accuracy 0.868
training loss:  0.45964513349325037
valid loss 0.512 and accuracy 0.868
training loss:  0.46143603871186045
valid loss 0.505 and accuracy 0.872
training loss:  0.461973627201558
valid loss 0.502 and accuracy 0.870
training loss:  0.4630463900628125
valid loss 0.509 and accuracy 0.867
training loss:  0.4616339608710044
valid loss 0.505 and accuracy 0.872
training loss:  0.4589522753012854
valid loss 0.502 and accuracy 0.871
training loss:  0.4603944786409274
valid loss 0.506 and accuracy 0.867
training loss:  0.46043759594877653
valid loss 0.510 and accuracy 0.868
training loss:  0.4625317925145379
valid loss 0.506 and accuracy 0.871
training loss:  0.460837270980139
valid loss 0.503 and accuracy 0.871
training loss:  0.4596884538498894
valid loss 0.505 and accuracy 0.870
training loss:  0.4597467491750885
valid loss 0.507 and accuracy 0.871
training loss:  0.4605355065892957
valid loss 0.505 and accuracy 0.872
training loss:  0.4598382867558432
valid loss 0.502 and accuracy 0.870
training loss:  0.4606355254738923
valid loss 0.504 and accuracy 0.870
training loss:  0.4614870578798573
valid loss 0.501 and accuracy 0.870
training loss:  0.4580409820479509
valid loss 0.539 and accuracy 0.870
training loss:  0.4592622662662874
valid loss 0.501 and accuracy 0.871
training loss:  0.45947762439174583
valid loss 0.505 and accuracy 0.869
training loss:  0.4588786909180622
valid loss 0.503 and accuracy 0.867
training loss:  0.4609756840589241
valid loss 0.503 and accuracy 0.871
training loss:  0.4616946702888246
valid loss 0.523 and accuracy 0.870
training loss:  0.4586822635687713
valid loss 0.510 and accuracy 0.870
training loss:  0.4600336961208949
valid loss 0.503 and accuracy 0.868
training loss:  0.4603330313207887
valid loss 0.514 and accuracy 0.866
training loss:  0.4602844568892656
valid loss 0.501 and accuracy 0.870
training loss:  0.4591541943401806
valid loss 0.510 and accuracy 0.870
training loss:  0.4583232309668117
valid loss 0.506 and accuracy 0.869
training loss:  0.4589881122163313
valid loss 0.504 and accuracy 0.870
training loss:  0.4596510576121282
valid loss 0.502 and accuracy 0.870
training loss:  0.46018235894862725
valid loss 0.508 and accuracy 0.867
training loss:  0.45981915281726404
valid loss 0.500 and accuracy 0.872
training loss:  0.46136338104603647
valid loss 0.501 and accuracy 0.870
training loss:  0.4612100835358001
valid loss 0.596 and accuracy 0.868
training loss:  0.45875516338203437
valid loss 0.501 and accuracy 0.870
training loss:  0.46048000852032006
valid loss 0.503 and accuracy 0.870
training loss:  0.4582707226348095
valid loss 0.503 and accuracy 0.870
training loss:  0.46020297481101374
valid loss 0.504 and accuracy 0.871
training loss:  0.4576848085568137
valid loss 0.501 and accuracy 0.867
training loss:  0.4597027973888453
valid loss 0.929 and accuracy 0.865
training loss:  0.4581745641604945
valid loss 0.502 and accuracy 0.870
training loss:  0.45874644037187323
valid loss 0.505 and accuracy 0.871
training loss:  0.4608055329352709
valid loss 0.509 and accuracy 0.870
training loss:  0.45716820944436853
valid loss 0.505 and accuracy 0.869
training loss:  0.4584335569514006
valid loss 0.502 and accuracy 0.868
training loss:  0.46058232327378046
valid loss 0.503 and accuracy 0.871
training loss:  0.4590131779052827
valid loss 0.508 and accuracy 0.869
training loss:  0.458078037304952
valid loss 0.506 and accuracy 0.869
training loss:  0.45915321924724883
valid loss 0.512 and accuracy 0.868
training loss:  0.4610539106419163
valid loss 0.505 and accuracy 0.869
training loss:  0.45802341991893747
valid loss 0.503 and accuracy 0.870
training loss:  0.4605978742281657
valid loss 0.508 and accuracy 0.870
training loss:  0.4580044767093886
valid loss 0.503 and accuracy 0.870
training loss:  0.4613234123281194
valid loss 0.505 and accuracy 0.870
training loss:  0.46035084164471446
valid loss 0.508 and accuracy 0.868
training loss:  0.460290867887765
valid loss 0.503 and accuracy 0.869
training loss:  0.45798708647614106
valid loss 0.504 and accuracy 0.870
training loss:  0.45930889261668384
valid loss 0.510 and accuracy 0.870
training loss:  0.45818203943711777
valid loss 0.505 and accuracy 0.869
training loss:  0.4612475874499372
valid loss 0.505 and accuracy 0.869
training loss:  0.4571228087569365
valid loss 0.502 and accuracy 0.870
training loss:  0.4588427264884006
valid loss 0.509 and accuracy 0.870
training loss:  0.45738061436012656
valid loss 0.729 and accuracy 0.870
training loss:  0.45874986000927626
valid loss 0.504 and accuracy 0.870
training loss:  0.45753315919296084
valid loss 0.506 and accuracy 0.871
training loss:  0.45874788340240125
valid loss 0.507 and accuracy 0.871
training loss:  0.461525406507462
valid loss 0.510 and accuracy 0.871
training loss:  0.45850429934544795
valid loss 0.506 and accuracy 0.871
training loss:  0.4573343765530499
valid loss 0.512 and accuracy 0.869
training loss:  0.46032177015457604
valid loss 0.507 and accuracy 0.871
training loss:  0.4585990137095242
valid loss 0.510 and accuracy 0.869
training loss:  0.459528350587523
valid loss 0.504 and accuracy 0.868
training loss:  0.4570476351174334
valid loss 0.506 and accuracy 0.869
training loss:  0.4596265601219612
valid loss 0.510 and accuracy 0.870
training loss:  0.4582985085085745
valid loss 0.504 and accuracy 0.870
training loss:  0.4601094525355797
valid loss 0.506 and accuracy 0.870
training loss:  0.4589883221972645
valid loss 0.503 and accuracy 0.870
training loss:  0.4564718184859948
valid loss 0.509 and accuracy 0.868
training loss:  0.45766662615500336
valid loss 0.509 and accuracy 0.869
training loss:  0.45772368827445364
valid loss 0.510 and accuracy 0.870
training loss:  0.4557487615683691
valid loss 0.511 and accuracy 0.869
training loss:  0.4591797228257987
valid loss 0.505 and accuracy 0.868
training loss:  0.45688024237677527
valid loss 0.612 and accuracy 0.870
training loss:  0.45773806647412596
valid loss 0.503 and accuracy 0.870
training loss:  0.4573899740787473
valid loss 0.499 and accuracy 0.869
training loss:  0.45699327438841897
valid loss 0.656 and accuracy 0.870
training loss:  0.4575351537782027
valid loss 0.539 and accuracy 0.867
training loss:  0.456919760221628
valid loss 0.506 and accuracy 0.870
training loss:  0.45847301374612087
valid loss 0.503 and accuracy 0.871
training loss:  0.456759644285877
valid loss 0.503 and accuracy 0.871
training loss:  0.4593136039270475
valid loss 0.666 and accuracy 0.867
training loss:  0.4585242996208724
valid loss 0.504 and accuracy 0.873
training loss:  0.4585208897877523
valid loss 0.503 and accuracy 0.867
training loss:  0.4581017464228601
valid loss 0.507 and accuracy 0.869
training loss:  0.4564742250528444
valid loss 0.504 and accuracy 0.868
training loss:  0.4564207849651985
valid loss 0.508 and accuracy 0.865
training loss:  0.45926286002814765
valid loss 0.509 and accuracy 0.870
training loss:  0.45607592301056743
valid loss 0.504 and accuracy 0.870
training loss:  0.4572670282248233
valid loss 0.566 and accuracy 0.871
training loss:  0.45785569301421797
valid loss 0.508 and accuracy 0.868
training loss:  0.4563096712919042
valid loss 0.510 and accuracy 0.866
training loss:  0.45903001608254385
valid loss 0.507 and accuracy 0.870
training loss:  0.460717623441085
valid loss 0.502 and accuracy 0.870
training loss:  0.4568058072885108
valid loss 0.499 and accuracy 0.870
training loss:  0.4590292250137426
valid loss 0.505 and accuracy 0.871
training loss:  0.45722939903630294
valid loss 0.506 and accuracy 0.869
training loss:  0.4568377377582293
valid loss 0.503 and accuracy 0.870
training loss:  0.4565039478509287
valid loss 0.501 and accuracy 0.866
training loss:  0.4576513435485086
valid loss 0.508 and accuracy 0.869
training loss:  0.45896871556598756
valid loss 0.506 and accuracy 0.870
training loss:  0.4579864890789917
valid loss 0.502 and accuracy 0.870
training loss:  0.45734527398559083
valid loss 0.507 and accuracy 0.871
training loss:  0.4589374371254397
valid loss 0.506 and accuracy 0.871
training loss:  0.4578634219383833
valid loss 0.506 and accuracy 0.871
training loss:  0.4589448620840233
valid loss 0.505 and accuracy 0.870
training loss:  0.4567712697490095
valid loss 0.508 and accuracy 0.872
training loss:  0.45831116723898596
valid loss 0.522 and accuracy 0.871
training loss:  0.45723900364751335
valid loss 0.506 and accuracy 0.871
training loss:  0.45760800768644716
valid loss 0.505 and accuracy 0.868
training loss:  0.45639978562264844
valid loss 0.506 and accuracy 0.871
training loss:  0.4589633089487102
valid loss 0.510 and accuracy 0.868
training loss:  0.45742036918110873
valid loss 0.613 and accuracy 0.870
training loss:  0.4576122822553784
valid loss 0.510 and accuracy 0.871
training loss:  0.45638822841035226
valid loss 0.506 and accuracy 0.870
training loss:  0.45621953865104176
valid loss 0.506 and accuracy 0.871
training loss:  0.4578753930444141
valid loss 0.509 and accuracy 0.866
training loss:  0.45564361330483355
valid loss 0.507 and accuracy 0.867
training loss:  0.45719278960318477
valid loss 0.506 and accuracy 0.869
training loss:  0.4551629243394004
valid loss 0.504 and accuracy 0.872
training loss:  0.4560107409640548
valid loss 0.506 and accuracy 0.869
training loss:  0.45527222790424776
valid loss 0.504 and accuracy 0.870
training loss:  0.4557467223516446
valid loss 0.503 and accuracy 0.871
training loss:  0.45644297998975536
valid loss 0.506 and accuracy 0.869
training loss:  0.4559411449061395
valid loss 0.506 and accuracy 0.871
training loss:  0.45631587840433663
valid loss 0.509 and accuracy 0.869
training loss:  0.4560699998790446
valid loss 0.504 and accuracy 0.869
training loss:  0.456569811688225
valid loss 0.506 and accuracy 0.869
training loss:  0.4577036276525011
valid loss 0.505 and accuracy 0.870
training loss:  0.4559949597584907
valid loss 0.508 and accuracy 0.869
training loss:  0.45784527287149956
valid loss 0.595 and accuracy 0.870
training loss:  0.45570966074579916
valid loss 1.386 and accuracy 0.869
training loss:  0.4568066468306688
valid loss 0.507 and accuracy 0.869
training loss:  0.4557216439188267
valid loss 0.507 and accuracy 0.868
training loss:  0.4589160938248746
valid loss 0.502 and accuracy 0.873
training loss:  0.4554699009121122
valid loss 0.508 and accuracy 0.869
training loss:  0.4572041874928451
valid loss 0.506 and accuracy 0.869
training loss:  0.45632277320859366
valid loss 0.499 and accuracy 0.870
training loss:  0.4568222290832952
valid loss 0.503 and accuracy 0.871
training loss:  0.4573990433230046
valid loss 0.503 and accuracy 0.870
training loss:  0.45930666907810336
valid loss 0.504 and accuracy 0.870
training loss:  0.45615918286366963
valid loss 0.502 and accuracy 0.871
training loss:  0.45865214687499184
valid loss 0.505 and accuracy 0.870
training loss:  0.4556524905276888
valid loss 0.503 and accuracy 0.870
training loss:  0.45665590297279235
valid loss 0.504 and accuracy 0.869
training loss:  0.45689395223860324
valid loss 0.506 and accuracy 0.870
training loss:  0.4563813177178829
valid loss 0.503 and accuracy 0.871
training loss:  0.45730916052730436
valid loss 0.503 and accuracy 0.870
training loss:  0.45672254867613404
valid loss 0.505 and accuracy 0.870
training loss:  0.45592292995624856
valid loss 0.503 and accuracy 0.870
training loss:  0.45648179926547355
valid loss 0.506 and accuracy 0.870
training loss:  0.4557893251491231
valid loss 0.507 and accuracy 0.870
training loss:  0.45667572135920337
valid loss 0.502 and accuracy 0.870
training loss:  0.45630849265073276
valid loss 0.511 and accuracy 0.869
training loss:  0.45688671868591973
valid loss 0.504 and accuracy 0.866
training loss:  0.45542575751216857
valid loss 0.501 and accuracy 0.869
training loss:  0.4571256506160853
valid loss 0.508 and accuracy 0.870
training loss:  0.4553413986521746
valid loss 0.506 and accuracy 0.869
training loss:  0.4556567199395608
valid loss 0.498 and accuracy 0.872
training loss:  0.4566565461947567
valid loss 0.504 and accuracy 0.871
training loss:  0.4561964462490222
valid loss 0.502 and accuracy 0.872
training loss:  0.4556180319668634
valid loss 0.505 and accuracy 0.870
training loss:  0.4558864263548234
valid loss 0.503 and accuracy 0.870
training loss:  0.45714117380483366
valid loss 0.505 and accuracy 0.870
training loss:  0.45760056499010754
valid loss 0.504 and accuracy 0.870
training loss:  0.4570222273643657
valid loss 0.499 and accuracy 0.871
training loss:  0.45643360026337354
valid loss 0.502 and accuracy 0.870
training loss:  0.4546603280907754
valid loss 0.505 and accuracy 0.869
training loss:  0.45509177941136525
valid loss 0.503 and accuracy 0.870
training loss:  0.4552951152900587
valid loss 0.502 and accuracy 0.870
training loss:  0.45527802709386084
valid loss 0.504 and accuracy 0.870
training loss:  0.45450369721001066
valid loss 0.515 and accuracy 0.870
training loss:  0.4533829596062611
valid loss 0.506 and accuracy 0.871
training loss:  0.4566892857007065
valid loss 0.505 and accuracy 0.871
training loss:  0.45436133146857155
valid loss 0.505 and accuracy 0.869
training loss:  0.45590690017541263
valid loss 0.507 and accuracy 0.870
training loss:  0.45693638813359777
valid loss 0.504 and accuracy 0.872
training loss:  0.45603813847921126
valid loss 0.510 and accuracy 0.870
training loss:  0.4559236322086728
valid loss 0.502 and accuracy 0.872
training loss:  0.45687280393973306
valid loss 0.523 and accuracy 0.868
training loss:  0.4569539136703246
valid loss 0.503 and accuracy 0.869
training loss:  0.4545022671414787
valid loss 0.503 and accuracy 0.870
training loss:  0.45342406222389076
valid loss 0.504 and accuracy 0.870
training loss:  0.4559166520033756
valid loss 0.500 and accuracy 0.868
training loss:  0.45436253973646806
valid loss 0.500 and accuracy 0.871
training loss:  0.45460702539007974
valid loss 0.507 and accuracy 0.870
training loss:  0.45507065324747453
valid loss 0.507 and accuracy 0.870
training loss:  0.45571225058699716
valid loss 0.503 and accuracy 0.866
training loss:  0.4565703863476528
valid loss 0.503 and accuracy 0.870
training loss:  0.4547222861043223
valid loss 0.500 and accuracy 0.871
training loss:  0.4568380542872195
valid loss 0.505 and accuracy 0.869
training loss:  0.4550044806226585
valid loss 0.511 and accuracy 0.869
training loss:  0.45844745319182095
valid loss 0.501 and accuracy 0.871
training loss:  0.4554665412846252
valid loss 0.498 and accuracy 0.871
training loss:  0.45499875443229965
valid loss 0.507 and accuracy 0.870
training loss:  0.45587576522093454
valid loss 0.503 and accuracy 0.870
training loss:  0.4562822833610783
valid loss 0.503 and accuracy 0.870
training loss:  0.4579426462657493
valid loss 0.501 and accuracy 0.870
training loss:  0.456208455733085
valid loss 0.505 and accuracy 0.870
training loss:  0.45496777223712914
valid loss 0.498 and accuracy 0.870
training loss:  0.45506575924727666
valid loss 0.506 and accuracy 0.870
training loss:  0.4586041729059756
valid loss 0.503 and accuracy 0.870
training loss:  0.4548630066249866
valid loss 0.506 and accuracy 0.870
training loss:  0.45758199862995
valid loss 0.504 and accuracy 0.870
training loss:  0.45386979022583407
valid loss 0.504 and accuracy 0.871
training loss:  0.4553187383683358
valid loss 0.504 and accuracy 0.870
training loss:  0.45513779683412914
valid loss 0.506 and accuracy 0.870
training loss:  0.45456923017220097
valid loss 0.507 and accuracy 0.869
training loss:  0.45637318115707914
valid loss 0.509 and accuracy 0.870
training loss:  0.4555253693580083
valid loss 0.507 and accuracy 0.868
training loss:  0.4549019498347157
valid loss 0.506 and accuracy 0.870
training loss:  0.4552439470138411
valid loss 0.507 and accuracy 0.870
training loss:  0.4558296543943906
valid loss 0.584 and accuracy 0.869
training loss:  0.4567077877624856
valid loss 0.512 and accuracy 0.870
training loss:  0.4557223146214622
valid loss 0.727 and accuracy 0.869
training loss:  0.4568075009771739
valid loss 0.508 and accuracy 0.870
training loss:  0.4579156307014195
valid loss 0.505 and accuracy 0.870
training loss:  0.4541472132033307
valid loss 0.542 and accuracy 0.870
training loss:  0.4567586937461848
valid loss 0.504 and accuracy 0.870
training loss:  0.45436392942673887
valid loss 0.515 and accuracy 0.870
training loss:  0.4540848936380146
valid loss 0.504 and accuracy 0.871
training loss:  0.45495327230539934
valid loss 0.502 and accuracy 0.871
training loss:  0.4554693995478413
valid loss 0.503 and accuracy 0.872
training loss:  0.45428025192126953
valid loss 0.507 and accuracy 0.870
training loss:  0.4568917770725174
valid loss 0.510 and accuracy 0.869
training loss:  0.4558872944227298
valid loss 0.511 and accuracy 0.870
training loss:  0.45386994215515586
valid loss 0.505 and accuracy 0.872
training loss:  0.4551489352687773
valid loss 0.506 and accuracy 0.871
training loss:  0.45572218604536385
valid loss 0.772 and accuracy 0.870
training loss:  0.45611136133983016
valid loss 0.511 and accuracy 0.867
training loss:  0.4547889935389195
valid loss 0.503 and accuracy 0.868
training loss:  0.4542888211452739
valid loss 0.537 and accuracy 0.865
training loss:  0.45370104366414404
valid loss 0.506 and accuracy 0.871
training loss:  0.45628772061856265
valid loss 0.506 and accuracy 0.870
training loss:  0.4532816663927331
valid loss 0.516 and accuracy 0.870
training loss:  0.4566403193980217
valid loss 0.504 and accuracy 0.871
training loss:  0.4549202987799166
valid loss 0.501 and accuracy 0.873
training loss:  0.45593726727577955
valid loss 0.502 and accuracy 0.872
training loss:  0.45628859242930386
valid loss 0.510 and accuracy 0.869
training loss:  0.455455571939832
valid loss 0.508 and accuracy 0.870
training loss:  0.45401555588734926
valid loss 0.503 and accuracy 0.871
training loss:  0.45454843939243855
valid loss 0.502 and accuracy 0.873
training loss:  0.45400363926690745
valid loss 0.505 and accuracy 0.870
training loss:  0.45383866534723744
valid loss 0.590 and accuracy 0.871
training loss:  0.4571857563480645
valid loss 0.504 and accuracy 0.872
training loss:  0.4552276084059913
valid loss 0.500 and accuracy 0.874
training loss:  0.45443411144853674
valid loss 0.499 and accuracy 0.872
training loss:  0.454369794392421
valid loss 0.506 and accuracy 0.870
training loss:  0.45531151144520926
valid loss 0.507 and accuracy 0.870
training loss:  0.4549646488985725
valid loss 0.952 and accuracy 0.869
training loss:  0.45425871494176123
valid loss 0.501 and accuracy 0.872
training loss:  0.45524858805430346
valid loss 0.501 and accuracy 0.872
training loss:  0.4562865554048112
valid loss 0.508 and accuracy 0.872
training loss:  0.45382569943945444
valid loss 0.504 and accuracy 0.870
training loss:  0.4531324045558805
valid loss 0.503 and accuracy 0.870
training loss:  0.4537218697930372
valid loss 0.792 and accuracy 0.870
training loss:  0.4564383998335498
valid loss 0.798 and accuracy 0.870
training loss:  0.4527001652786322
valid loss 0.643 and accuracy 0.873
training loss:  0.456360795976248
valid loss 0.507 and accuracy 0.870
training loss:  0.45359389862678046
valid loss 0.504 and accuracy 0.869
training loss:  0.455612407324596
valid loss 0.501 and accuracy 0.873
training loss:  0.4525372807205595
valid loss 0.503 and accuracy 0.872
training loss:  0.45354116301903163
valid loss 0.616 and accuracy 0.873
training loss:  0.4547976317331622
valid loss 0.507 and accuracy 0.871
training loss:  0.4540131742893397
valid loss 0.505 and accuracy 0.872
training loss:  0.45535467159515003
valid loss 0.504 and accuracy 0.869
training loss:  0.4542793242651237
valid loss 0.502 and accuracy 0.870
training loss:  0.45462991879541415
valid loss 0.501 and accuracy 0.870
training loss:  0.4538769758088637
valid loss 0.504 and accuracy 0.872
training loss:  0.4544998387142862
valid loss 0.503 and accuracy 0.870
training loss:  0.45646003709612
valid loss 0.505 and accuracy 0.870
training loss:  0.45423425504095744
valid loss 0.504 and accuracy 0.870
training loss:  0.4515277771781438
valid loss 0.504 and accuracy 0.871
training loss:  0.45444942706124486
valid loss 0.507 and accuracy 0.870
training loss:  0.45415291775815986
valid loss 0.504 and accuracy 0.870
training loss:  0.45254817991940244
valid loss 0.507 and accuracy 0.870
training loss:  0.4536080051817081
valid loss 0.508 and accuracy 0.871
training loss:  0.4540224033040362
valid loss 0.613 and accuracy 0.870
training loss:  0.4541230704014487
valid loss 0.500 and accuracy 0.870
training loss:  0.4542051222369869
valid loss 0.500 and accuracy 0.871
training loss:  0.45286315503163005
valid loss 0.502 and accuracy 0.870
training loss:  0.45590343490762764
valid loss 0.719 and accuracy 0.869
training loss:  0.45261125934702673
valid loss 0.789 and accuracy 0.870
training loss:  0.4523965984339252
valid loss 0.506 and accuracy 0.870
training loss:  0.45599827635795953
valid loss 0.632 and accuracy 0.869
training loss:  0.4518086722261493
valid loss 1.900 and accuracy 0.869
training loss:  0.4529038166479223
valid loss 0.509 and accuracy 0.869
training loss:  0.4542917244245854
valid loss 0.520 and accuracy 0.869
training loss:  0.45409960018394896
valid loss 0.741 and accuracy 0.870
training loss:  0.45057194092803105
valid loss 1.437 and accuracy 0.870
training loss:  0.45400398444377427
valid loss 0.698 and accuracy 0.871
training loss:  0.45336700351248604
valid loss 0.502 and accuracy 0.870
training loss:  0.45303201064956244
valid loss 0.537 and accuracy 0.871
training loss:  0.4517656593883547
valid loss 0.576 and accuracy 0.870
training loss:  0.4528440316380019
valid loss 0.503 and accuracy 0.872
training loss:  0.45335250623962725
valid loss 0.503 and accuracy 0.871
training loss:  0.4524348929484016
valid loss 0.561 and accuracy 0.870
training loss:  0.45378392123770633
valid loss 0.504 and accuracy 0.870
training loss:  0.45465063344433626
valid loss 0.502 and accuracy 0.869
training loss:  0.45456627260234106
valid loss 0.500 and accuracy 0.870
training loss:  0.4552659292916556
valid loss 0.500 and accuracy 0.874
training loss:  0.45366027399840764
valid loss 0.506 and accuracy 0.870
training loss:  0.45329184705174425
valid loss 0.506 and accuracy 0.870
training loss:  0.4551077656130871
valid loss 0.504 and accuracy 0.869
training loss:  0.45435357376407215
valid loss 0.504 and accuracy 0.870
training loss:  0.4533756315488208
valid loss 0.508 and accuracy 0.870
training loss:  0.45529467486290615
valid loss 0.506 and accuracy 0.870
training loss:  0.451824221517525
valid loss 0.554 and accuracy 0.870
training loss:  0.4538496593850189
valid loss 0.506 and accuracy 0.871
training loss:  0.4547475846425632
valid loss 0.504 and accuracy 0.872
training loss:  0.45362143070414057
valid loss 0.569 and accuracy 0.867
training loss:  0.4542305529817543
valid loss 1.449 and accuracy 0.872
training loss:  0.4545078968165222
valid loss 0.511 and accuracy 0.870
training loss:  0.4534450466686215
valid loss 0.508 and accuracy 0.870
training loss:  0.4531132364839843
valid loss 0.540 and accuracy 0.869
training loss:  0.45324336356685446
valid loss 0.507 and accuracy 0.870
training loss:  0.4522121221786508
valid loss 0.508 and accuracy 0.872
training loss:  0.4550698965829812
valid loss 0.506 and accuracy 0.870
training loss:  0.4519557357010617
valid loss 0.506 and accuracy 0.870
training loss:  0.45313514796024496
valid loss 0.508 and accuracy 0.870
training loss:  0.4552356529939201
valid loss 0.515 and accuracy 0.868
training loss:  0.45362275050973044
valid loss 0.499 and accuracy 0.870
training loss:  0.45461380534156115
valid loss 1.024 and accuracy 0.870
training loss:  0.4527738222181669
valid loss 0.499 and accuracy 0.873
training loss:  0.4529770651688335
valid loss 0.504 and accuracy 0.870
training loss:  0.45392681898787646
valid loss 0.607 and accuracy 0.870
training loss:  0.45278240703144296
valid loss 0.527 and accuracy 0.870
training loss:  0.45394625181733667
valid loss 0.504 and accuracy 0.870
training loss:  0.45264035316110907
valid loss 0.516 and accuracy 0.870
training loss:  0.45265494497275044
valid loss 0.506 and accuracy 0.870
training loss:  0.45269775047633076
valid loss 0.500 and accuracy 0.870
training loss:  0.4531114133424517
valid loss 0.501 and accuracy 0.870
training loss:  0.4549219787531559
valid loss 0.502 and accuracy 0.874
training loss:  0.45550548247711214
valid loss 0.505 and accuracy 0.870
training loss:  0.4550188756328846
valid loss 0.497 and accuracy 0.870
training loss:  0.45331460984976335
valid loss 1.001 and accuracy 0.870
training loss:  0.4530305809443771
valid loss 0.502 and accuracy 0.870
training loss:  0.4514989369459824
valid loss 0.509 and accuracy 0.868
training loss:  0.45537933447973017
valid loss 0.524 and accuracy 0.870
training loss:  0.45326682029435117
valid loss 0.507 and accuracy 0.871
training loss:  0.45370006296425125
valid loss 0.500 and accuracy 0.870
training loss:  0.45564669453346024
valid loss 0.502 and accuracy 0.870
training loss:  0.45509370755156714
valid loss 0.502 and accuracy 0.869
training loss:  0.45337358643044784
valid loss 0.503 and accuracy 0.870
training loss:  0.45429292336942056
valid loss 0.502 and accuracy 0.871
training loss:  0.4538868638972808
valid loss 0.504 and accuracy 0.872
training loss:  0.4538365052873788
valid loss 0.501 and accuracy 0.870
training loss:  0.45436872813533397
valid loss 0.504 and accuracy 0.870
training loss:  0.4528936582776141
valid loss 0.502 and accuracy 0.870
training loss:  0.4524922851087189
valid loss 0.500 and accuracy 0.873
training loss:  0.45100373302522634
valid loss 0.504 and accuracy 0.871
training loss:  0.4526858635688937
valid loss 0.543 and accuracy 0.871
training loss:  0.4553573783379612
valid loss 0.767 and accuracy 0.869
training loss:  0.4528023955157225
valid loss 0.504 and accuracy 0.871
training loss:  0.4548154192841253
valid loss 0.592 and accuracy 0.869
training loss:  0.45140603275536667
valid loss 0.496 and accuracy 0.871
training loss:  0.4516269954247986
valid loss 0.501 and accuracy 0.872
training loss:  0.452269803465668
valid loss 0.512 and accuracy 0.871
training loss:  0.45387318815114935
valid loss 0.504 and accuracy 0.870
training loss:  0.4538131761838664
valid loss 0.617 and accuracy 0.870
training loss:  0.45216718459533267
valid loss 0.503 and accuracy 0.870
training loss:  0.45287544647692984
valid loss 0.502 and accuracy 0.870
training loss:  0.45369976812547497
valid loss 0.500 and accuracy 0.870
training loss:  0.45321080334633895
valid loss 0.520 and accuracy 0.869
training loss:  0.453693984524526
valid loss 0.505 and accuracy 0.873
training loss:  0.45210725873026386
valid loss 0.501 and accuracy 0.872
training loss:  0.4511569039486568
valid loss 0.502 and accuracy 0.873
training loss:  0.451602739034563
valid loss 0.502 and accuracy 0.872
training loss:  0.4528001764393788
valid loss 0.500 and accuracy 0.871
training loss:  0.45302500604717966
valid loss 0.498 and accuracy 0.871
training loss:  0.45429123317265224
valid loss 0.502 and accuracy 0.871
training loss:  0.4538165216517717
valid loss 0.503 and accuracy 0.873
training loss:  0.4541070176125078
valid loss 1.402 and accuracy 0.868
training loss:  0.4540829105966232
valid loss 0.505 and accuracy 0.871
training loss:  0.4529619125395697
valid loss 0.505 and accuracy 0.870
training loss:  0.45342411135528166
valid loss 0.509 and accuracy 0.869
training loss:  0.452004046954181
valid loss 0.507 and accuracy 0.870
training loss:  0.4527615265943809
valid loss 0.633 and accuracy 0.872
training loss:  0.45279438437816477
valid loss 0.505 and accuracy 0.870
training loss:  0.4518351224917841
valid loss 0.977 and accuracy 0.871
training loss:  0.4516094674205957
valid loss 0.507 and accuracy 0.867
training loss:  0.4529760900783321
valid loss 0.503 and accuracy 0.870
training loss:  0.4535622224091691
valid loss 0.505 and accuracy 0.870
training loss:  0.451362472947831
valid loss 0.525 and accuracy 0.870
training loss:  0.45270951225460165
valid loss 0.503 and accuracy 0.872
training loss:  0.4519825139022189
valid loss 0.570 and accuracy 0.870
training loss:  0.45506370713478483
valid loss 0.507 and accuracy 0.870
training loss:  0.45175064029869855
valid loss 0.504 and accuracy 0.872
training loss:  0.45179210130370073
valid loss 0.505 and accuracy 0.872
training loss:  0.45453114579784976
valid loss 0.507 and accuracy 0.871
training loss:  0.4544559022384248
valid loss 0.507 and accuracy 0.870
training loss:  0.45178646207409906
valid loss 0.505 and accuracy 0.872
training loss:  0.45334306496062526
valid loss 0.503 and accuracy 0.869
training loss:  0.45066301366449674
valid loss 0.504 and accuracy 0.870
training loss:  0.4519353734041909
valid loss 0.504 and accuracy 0.873
training loss:  0.45329374520126675
valid loss 0.506 and accuracy 0.871
training loss:  0.45494877335230444
valid loss 0.498 and accuracy 0.872
training loss:  0.4536011426612243
valid loss 0.501 and accuracy 0.873
training loss:  0.4536424993669083
valid loss 0.503 and accuracy 0.871
training loss:  0.4535612580732419
valid loss 0.505 and accuracy 0.871
training loss:  0.4519935382867351
valid loss 0.500 and accuracy 0.872
training loss:  0.4537798608069284
valid loss 0.504 and accuracy 0.871
training loss:  0.4527902278397732
valid loss 0.504 and accuracy 0.871
training loss:  0.4517216001651312
valid loss 0.507 and accuracy 0.872
training loss:  0.45359803875016785
valid loss 0.503 and accuracy 0.870
training loss:  0.45398310999707076
valid loss 0.530 and accuracy 0.871
training loss:  0.4515400802341304
valid loss 0.571 and accuracy 0.870
training loss:  0.45311330189306115
valid loss 0.504 and accuracy 0.871
training loss:  0.45404577642352933
valid loss 0.507 and accuracy 0.870
training loss:  0.4511847421595264
valid loss 0.506 and accuracy 0.870
training loss:  0.45376757133330053
valid loss 0.508 and accuracy 0.872
training loss:  0.454492869242297
valid loss 0.504 and accuracy 0.872
training loss:  0.4521127211862623
valid loss 0.501 and accuracy 0.871
training loss:  0.45125469029242604
valid loss 0.503 and accuracy 0.870
training loss:  0.4528580537203068
valid loss 0.514 and accuracy 0.869
training loss:  0.45140952563141945
valid loss 0.505 and accuracy 0.871
training loss:  0.45358131264042995
valid loss 0.505 and accuracy 0.872
training loss:  0.45263624093338567
valid loss 0.506 and accuracy 0.868
training loss:  0.45273909034776444
valid loss 0.521 and accuracy 0.866
training loss:  0.45120988674148166
valid loss 0.507 and accuracy 0.870
training loss:  0.4524823816345772
valid loss 0.647 and accuracy 0.868
training loss:  0.4540507706211171
valid loss 0.649 and accuracy 0.869
training loss:  0.45387587934236023
valid loss 0.501 and accuracy 0.870
training loss:  0.4520282583252832
valid loss 0.504 and accuracy 0.870
training loss:  0.4503326005926093
valid loss 0.611 and accuracy 0.870
training loss:  0.4520911631484472
valid loss 0.504 and accuracy 0.870
training loss:  0.45258605007904235
valid loss 0.499 and accuracy 0.872
training loss:  0.4537185264857684
valid loss 0.508 and accuracy 0.871
training loss:  0.45105860668666625
valid loss 0.537 and accuracy 0.871
training loss:  0.4527895170061709
valid loss 0.507 and accuracy 0.870
training loss:  0.45030491187275046
valid loss 0.502 and accuracy 0.870
training loss:  0.45018633126461527
valid loss 0.805 and accuracy 0.870
training loss:  0.45210095006427153
valid loss 0.501 and accuracy 0.873
training loss:  0.45316716268627955
valid loss 0.567 and accuracy 0.872
training loss:  0.4533835398829791
valid loss 0.505 and accuracy 0.872
training loss:  0.45251624494369963
valid loss 0.504 and accuracy 0.873
training loss:  0.4523210336590677
valid loss 0.505 and accuracy 0.873
training loss:  0.4529055088904901
valid loss 0.506 and accuracy 0.871
training loss:  0.4519260482733742
valid loss 0.508 and accuracy 0.872
training loss:  0.45009639025082043
valid loss 0.506 and accuracy 0.873
training loss:  0.45279095105754097
valid loss 0.509 and accuracy 0.870
training loss:  0.45332957758966985
valid loss 0.505 and accuracy 0.871
training loss:  0.4525067663068312
valid loss 0.508 and accuracy 0.871
training loss:  0.4498830997760868
valid loss 0.502 and accuracy 0.874
training loss:  0.45126751051947706
valid loss 0.736 and accuracy 0.869
training loss:  0.45454919144826267
valid loss 0.562 and accuracy 0.871
training loss:  0.451320859303219
valid loss 0.501 and accuracy 0.870
training loss:  0.45061498068142586
valid loss 0.522 and accuracy 0.872
training loss:  0.4533558174958348
valid loss 0.505 and accuracy 0.872
training loss:  0.4523280164345258
valid loss 0.502 and accuracy 0.871
training loss:  0.4537065009263515
valid loss 0.545 and accuracy 0.870
training loss:  0.4525466662015499
valid loss 0.505 and accuracy 0.872
training loss:  0.45167596743287614
valid loss 0.504 and accuracy 0.871
training loss:  0.452657071791861
valid loss 1.067 and accuracy 0.870
training loss:  0.4510536151904817
valid loss 0.520 and accuracy 0.871
training loss:  0.4521213385142007
valid loss 0.507 and accuracy 0.870
training loss:  0.4508139497898544
valid loss 0.559 and accuracy 0.871
training loss:  0.45223581748088054
valid loss 0.503 and accuracy 0.874
training loss:  0.4505239830349765
valid loss 0.503 and accuracy 0.872
training loss:  0.4528957080350364
valid loss 0.530 and accuracy 0.871
training loss:  0.4504408439940207
valid loss 0.503 and accuracy 0.873
training loss:  0.4512099814607224
valid loss 0.503 and accuracy 0.872
training loss:  0.45046858256406486
valid loss 0.505 and accuracy 0.871
training loss:  0.4507182199357494
valid loss 0.499 and accuracy 0.873
training loss:  0.4529550513479138
valid loss 0.502 and accuracy 0.872
training loss:  0.4520936821485971
valid loss 0.498 and accuracy 0.870
training loss:  0.4527704914621545
valid loss 0.502 and accuracy 0.870
training loss:  0.451497925169016
valid loss 0.501 and accuracy 0.873
training loss:  0.4515074869928191
valid loss 0.507 and accuracy 0.871
training loss:  0.45306149497559856
valid loss 0.503 and accuracy 0.871
training loss:  0.450407767476174
valid loss 0.502 and accuracy 0.870
training loss:  0.452647948543844
valid loss 0.503 and accuracy 0.870
training loss:  0.45218886967747873
valid loss 0.501 and accuracy 0.870
training loss:  0.45185003039311866
valid loss 0.499 and accuracy 0.870
training loss:  0.4515791040363291
valid loss 0.503 and accuracy 0.872
training loss:  0.4517402185707438
valid loss 0.503 and accuracy 0.870
training loss:  0.45251026756233775
valid loss 0.504 and accuracy 0.870
training loss:  0.45100854967208653
valid loss 0.501 and accuracy 0.873
training loss:  0.4523157859992567
valid loss 0.500 and accuracy 0.872
training loss:  0.45036585997710277
valid loss 0.502 and accuracy 0.873
training loss:  0.4537558448269947
valid loss 0.506 and accuracy 0.872
training loss:  0.4527523034205726
valid loss 0.509 and accuracy 0.871
training loss:  0.45036119546614944
valid loss 0.570 and accuracy 0.869
training loss:  0.4518510318815425
valid loss 0.509 and accuracy 0.870
training loss:  0.45373164687723644
valid loss 0.505 and accuracy 0.869
training loss:  0.4533737318310748
valid loss 0.510 and accuracy 0.869
training loss:  0.45215419208243457
valid loss 0.509 and accuracy 0.870
training loss:  0.44986285702572526
valid loss 0.506 and accuracy 0.872
training loss:  0.4491003560918792
valid loss 0.501 and accuracy 0.872
training loss:  0.45357144144337785
valid loss 0.501 and accuracy 0.871
training loss:  0.4511610074056301
valid loss 0.505 and accuracy 0.872
training loss:  0.45126528227622603
valid loss 0.506 and accuracy 0.872
training loss:  0.4504271840279483
valid loss 0.503 and accuracy 0.873
training loss:  0.4527106288090698
valid loss 0.504 and accuracy 0.870
training loss:  0.45022399465632695
valid loss 0.954 and accuracy 0.870
training loss:  0.45077902850892027
valid loss 0.505 and accuracy 0.869
training loss:  0.45063358559338973
valid loss 0.503 and accuracy 0.872
training loss:  0.45165952906997153
valid loss 0.506 and accuracy 0.872
training loss:  0.4514519895800548
valid loss 0.503 and accuracy 0.870
training loss:  0.4504162225753112
valid loss 0.811 and accuracy 0.871
training loss:  0.45076915240000504
valid loss 0.505 and accuracy 0.872
training loss:  0.45098359459362625
valid loss 0.501 and accuracy 0.872
training loss:  0.45122854128489376
valid loss 0.507 and accuracy 0.871
training loss:  0.45057842535475234
valid loss 0.506 and accuracy 0.871
training loss:  0.44993094193959454
valid loss 0.636 and accuracy 0.871
training loss:  0.45220245597897163
valid loss 0.505 and accuracy 0.870
training loss:  0.4524754530014123
valid loss 0.504 and accuracy 0.871
training loss:  0.4515650078701986
valid loss 0.500 and accuracy 0.874
training loss:  0.4510884382645336
valid loss 0.502 and accuracy 0.870
training loss:  0.4537713666626111
valid loss 0.498 and accuracy 0.870
training loss:  0.45087664013582257
valid loss 0.505 and accuracy 0.870
training loss:  0.45067611184868067
valid loss 0.500 and accuracy 0.871
training loss:  0.4521601678056477
valid loss 0.505 and accuracy 0.871
training loss:  0.45137362335590503
valid loss 0.503 and accuracy 0.874
training loss:  0.45186658912704986
valid loss 0.505 and accuracy 0.873
training loss:  0.45254285282339046
valid loss 0.506 and accuracy 0.871
training loss:  0.4493110503194558
valid loss 0.514 and accuracy 0.872
training loss:  0.4514348325843551
valid loss 0.510 and accuracy 0.870
training loss:  0.45076287491923966
valid loss 0.506 and accuracy 0.871
training loss:  0.44981435085719057
valid loss 0.507 and accuracy 0.870
training loss:  0.4535297142012982
valid loss 0.508 and accuracy 0.870
training loss:  0.4527026281587409
valid loss 0.506 and accuracy 0.872
training loss:  0.4514034188779993
valid loss 0.505 and accuracy 0.870
training loss:  0.4506912584620666
valid loss 0.501 and accuracy 0.870
training loss:  0.4511521472491295
valid loss 0.500 and accuracy 0.870
training loss:  0.4516407849312354
valid loss 0.502 and accuracy 0.871
training loss:  0.45162203457307426
valid loss 0.508 and accuracy 0.871
training loss:  0.4506180447215696
valid loss 0.505 and accuracy 0.869
training loss:  0.4521457726781542
valid loss 0.508 and accuracy 0.871


True positive:  [  168    70   112    33 43297]
True negative:  [48347 47927 48707 48971   514]
False positive:  [ 122  134   78  153 6095]
False negative:  [1625 2131 1365 1105  356]

[[  208     8    41     1  1613]
 [   15   150    21     3  1953]
 [   38     4   158     3  1311]
 [    5    17     5    33  1060]
 [    7     0     6     4 43597]]

              precision    recall  f1-score   support

           0       0.76      0.11      0.19      1871
           1       0.84      0.07      0.13      2142
           2       0.68      0.10      0.18      1514
           3       0.75      0.03      0.06      1120
           4       0.88      1.00      0.94     43614

    accuracy                           0.88     50261
   macro avg       0.78      0.26      0.30     50261
weighted avg       0.87      0.88      0.83     50261

Accuracy:  0.8783350908258889
Precision_weighted:  0.8651359802216099
Recall_weighted:  0.8783350908258889
mcc:  0.2714576535023915
f2:  0.8756631467638083

- train_loop(model, epochs=500, lr=0.0001, wd=0.000001)

ep  0  training loss:  1.6558501484770893
valid loss  1.5571205137585136  and accuracy  0.5367464905037159
ep  1  training loss:  1.5797270816606364
valid loss  1.4934940036912283  and accuracy  0.6779521056977704
ep  2  training loss:  1.514571761680532
valid loss  1.431022137575756  and accuracy  0.7357555739058629
ep  3  training loss:  1.43931648881337
valid loss  1.3484962320249008  and accuracy  0.8018166804293972
ep  4  training loss:  1.3471300384773164
valid loss  1.247971533252423  and accuracy  0.8191577208918249
ep  5  training loss:  1.235917317130363
valid loss  1.1354216468127476  and accuracy  0.838150289017341
ep  6  training loss:  1.1166458592253863
valid loss  1.003421720446485  and accuracy  0.8488852188274154
ep  7  training loss:  1.0003230682381448
valid loss  0.893201584170221  and accuracy  0.8513625103220479
ep  8  training loss:  0.8950713838591425
valid loss  0.7931306519161857  and accuracy  0.8571428571428571
ep  9  training loss:  0.8087348013246182
valid loss  0.7188596612751533  and accuracy  0.8579686209744013
ep  10  training loss:  0.7392286589261934
valid loss  0.6654980462213669  and accuracy  0.8596201486374897
ep  11  training loss:  0.6920307240037384
valid loss  0.6312280944513939  and accuracy  0.8596201486374897
ep  12  training loss:  0.6530223892458771
valid loss  0.6068901163206722  and accuracy  0.8596201486374897
ep  13  training loss:  0.6321622347330079
valid loss  0.5914092299664542  and accuracy  0.8596201486374897
ep  14  training loss:  0.6152723083836747
valid loss  0.583856012336879  and accuracy  0.8596201486374897
ep  15  training loss:  0.6025904279668264
valid loss  0.5770951449428089  and accuracy  0.8596201486374897
ep  16  training loss:  0.5960133417534257
valid loss  0.5739852187556732  and accuracy  0.8596201486374897
ep  17  training loss:  0.5882387735913267
valid loss  0.5746917613373622  and accuracy  0.8596201486374897
ep  18  training loss:  0.5875701037010892
valid loss  0.5735381943051427  and accuracy  0.8596201486374897
ep  19  training loss:  0.5826279327329269
valid loss  0.5716495473063947  and accuracy  0.8596201486374897
ep  20  training loss:  0.5812091438664521
valid loss  0.5695888935092261  and accuracy  0.8596201486374897
ep  21  training loss:  0.5767874991576836
valid loss  0.569178575626786  and accuracy  0.8596201486374897
ep  22  training loss:  0.5797102196806505
valid loss  0.5694388264865544  and accuracy  0.8596201486374897
ep  23  training loss:  0.5744930889182215
valid loss  0.5693707288935793  and accuracy  0.8596201486374897
ep  24  training loss:  0.5742037360167702
valid loss  0.5669272196479911  and accuracy  0.8596201486374897
ep  25  training loss:  0.5718299034582501
valid loss  0.5684737372162123  and accuracy  0.8596201486374897
ep  26  training loss:  0.5750961852995301
valid loss  0.5666492170878031  and accuracy  0.8596201486374897
ep  27  training loss:  0.57382569949247
valid loss  0.5657409656943614  and accuracy  0.8596201486374897
ep  28  training loss:  0.5659871726299728
valid loss  0.5662489230725314  and accuracy  0.8596201486374897
ep  29  training loss:  0.5669311275491511
valid loss  0.5657145791463277  and accuracy  0.8604459124690339
ep  30  training loss:  0.5630680542652495
valid loss  0.5657496665943564  and accuracy  0.8596201486374897
ep  31  training loss:  0.5681803702549801
valid loss  0.5631414548263975  and accuracy  0.8604459124690339
ep  32  training loss:  0.5635943541449409
valid loss  0.5646933129166494  and accuracy  0.8604459124690339
ep  33  training loss:  0.5661683242796607
valid loss  0.5644597231505037  and accuracy  0.8604459124690339
ep  34  training loss:  0.5627714163268144
valid loss  0.5636872363228329  and accuracy  0.8604459124690339
ep  35  training loss:  0.5628588877033787
valid loss  0.5619900598691377  and accuracy  0.8604459124690339
ep  36  training loss:  0.5585544294740107
valid loss  0.5617472314814907  and accuracy  0.8604459124690339
ep  37  training loss:  0.5615400474508208
valid loss  0.561011680436469  and accuracy  0.8604459124690339
ep  38  training loss:  0.559456021113762
valid loss  0.5620131108211546  and accuracy  0.8604459124690339
ep  39  training loss:  0.5568738029882387
valid loss  0.5609144961705983  and accuracy  0.8604459124690339
ep  40  training loss:  0.5589436059032329
valid loss  0.5615779323384941  and accuracy  0.8604459124690339
ep  41  training loss:  0.5551959086623321
valid loss  0.561110761788736  and accuracy  0.8604459124690339
ep  42  training loss:  0.5577777551799706
valid loss  0.5613818887981081  and accuracy  0.8604459124690339
ep  43  training loss:  0.5594039336734019
valid loss  0.5600697647727885  and accuracy  0.8604459124690339
ep  44  training loss:  0.5568944633619526
valid loss  0.5614306124943923  and accuracy  0.8604459124690339
ep  45  training loss:  0.5561821446335543
valid loss  0.5591296577729241  and accuracy  0.8604459124690339
ep  46  training loss:  0.5548757682653921
valid loss  0.559982037662377  and accuracy  0.8604459124690339
ep  47  training loss:  0.5541895461051348
valid loss  0.5583211847517138  and accuracy  0.8604459124690339
ep  48  training loss:  0.5535137332627094
valid loss  0.5600499458592554  and accuracy  0.8604459124690339
ep  49  training loss:  0.5539294697588565
valid loss  0.5574303046914785  and accuracy  0.8604459124690339
ep  50  training loss:  0.5505611289093884
valid loss  0.5574837737000746  and accuracy  0.8604459124690339
ep  51  training loss:  0.5509230940926427
valid loss  0.5565655560753348  and accuracy  0.8604459124690339
ep  52  training loss:  0.5494040629444145
valid loss  0.5574732808708456  and accuracy  0.8604459124690339
ep  53  training loss:  0.549835439559356
valid loss  0.5577625178483574  and accuracy  0.8604459124690339
ep  54  training loss:  0.5481814910373491
valid loss  0.5579384373890084  and accuracy  0.8604459124690339
ep  55  training loss:  0.5510388325563828
valid loss  0.5567963238494835  and accuracy  0.8604459124690339
ep  56  training loss:  0.5479090230108338
valid loss  0.5573341978866142  and accuracy  0.8604459124690339
ep  57  training loss:  0.5462833323985791
valid loss  0.5559683669017033  and accuracy  0.8604459124690339
ep  58  training loss:  0.5506932948643223
valid loss  0.5561549091614739  and accuracy  0.8604459124690339
ep  59  training loss:  0.5470880307627821
valid loss  0.5558956244777393  and accuracy  0.8604459124690339
ep  60  training loss:  0.5456409611450753
valid loss  0.5564033651922873  and accuracy  0.8604459124690339
ep  61  training loss:  0.5462813031865924
valid loss  0.5555474408198545  and accuracy  0.8604459124690339
ep  62  training loss:  0.5454034429937333
valid loss  0.5546170103461362  and accuracy  0.8604459124690339
ep  63  training loss:  0.5452417995816555
valid loss  0.5545160352051701  and accuracy  0.8604459124690339
ep  64  training loss:  0.5435409177732087
valid loss  0.5532592882507601  and accuracy  0.8604459124690339
ep  65  training loss:  0.5420864487118209
valid loss  0.5544361281158705  and accuracy  0.8604459124690339
ep  66  training loss:  0.541701352580728
valid loss  0.5527145138070368  and accuracy  0.8604459124690339
ep  67  training loss:  0.5439463406631091
valid loss  0.5530472691643838  and accuracy  0.8604459124690339
ep  68  training loss:  0.5408223250079088
valid loss  0.5535898500242871  and accuracy  0.8604459124690339
ep  69  training loss:  0.5408745375198323
valid loss  0.5515061680882751  and accuracy  0.8604459124690339
ep  70  training loss:  0.5394189756256083
valid loss  0.5521868247320394  and accuracy  0.8604459124690339
ep  71  training loss:  0.5413344238403668
valid loss  0.5540814222037349  and accuracy  0.8604459124690339
ep  72  training loss:  0.5417192454264965
valid loss  0.5520625056854267  and accuracy  0.8604459124690339
ep  73  training loss:  0.5407868827114563
valid loss  0.55285851818584  and accuracy  0.8604459124690339
ep  74  training loss:  0.5375164891413152
valid loss  0.5514479572865512  and accuracy  0.8604459124690339
ep  75  training loss:  0.5372086316760301
valid loss  0.5500765613393287  and accuracy  0.8604459124690339
ep  76  training loss:  0.5375110586244468
valid loss  0.5519725065109259  and accuracy  0.8596201486374897
ep  77  training loss:  0.5388355180560386
valid loss  0.5513873966049302  and accuracy  0.8604459124690339
ep  78  training loss:  0.5358115119367104
valid loss  0.5507061781960022  and accuracy  0.8604459124690339
ep  79  training loss:  0.5370697322239438
valid loss  0.5509176303984015  and accuracy  0.8596201486374897
ep  80  training loss:  0.53667329871777
valid loss  0.5499757748807785  and accuracy  0.8604459124690339
ep  81  training loss:  0.5356595693045233
valid loss  0.5481536093081057  and accuracy  0.8596201486374897
ep  82  training loss:  0.5352231712291533
valid loss  0.5491231410194289  and accuracy  0.8596201486374897
ep  83  training loss:  0.5336553884688063
valid loss  0.5496333978276524  and accuracy  0.8596201486374897
ep  84  training loss:  0.5347709500989912
valid loss  0.5498545045797536  and accuracy  0.8604459124690339
ep  85  training loss:  0.5342942979183736
valid loss  0.5490473319832497  and accuracy  0.8596201486374897
ep  86  training loss:  0.5356284792116816
valid loss  0.5495165833552744  and accuracy  0.8604459124690339
ep  87  training loss:  0.5330781479135381
valid loss  0.5498205238536407  and accuracy  0.8604459124690339
ep  88  training loss:  0.5316148544481695
valid loss  0.5482642168500619  and accuracy  0.8604459124690339
ep  89  training loss:  0.5334104570298731
valid loss  0.548233689649552  and accuracy  0.8604459124690339
ep  90  training loss:  0.5306431336516444
valid loss  0.5489188025895473  and accuracy  0.8604459124690339
ep  91  training loss:  0.5313299731103073
valid loss  0.5473231582184845  and accuracy  0.8604459124690339
ep  92  training loss:  0.5317739164278997
valid loss  0.546537340868612  and accuracy  0.8604459124690339
ep  93  training loss:  0.5301391448812838
valid loss  0.5469070774872966  and accuracy  0.8604459124690339
ep  94  training loss:  0.5291738255225004
valid loss  0.546074044645374  and accuracy  0.8604459124690339
ep  95  training loss:  0.5313374180133454
valid loss  0.5462337072775642  and accuracy  0.8596201486374897
ep  96  training loss:  0.5308425922686232
valid loss  0.5468605846092782  and accuracy  0.8604459124690339
ep  97  training loss:  0.5292640004086467
valid loss  0.5444680585731267  and accuracy  0.8604459124690339
ep  98  training loss:  0.5296696347778879
valid loss  0.5469698861155207  and accuracy  0.8604459124690339
ep  99  training loss:  0.5287504296593674
valid loss  0.5453712615368289  and accuracy  0.8604459124690339
ep  100  training loss:  0.5277211209472018
valid loss  0.5458226298911821  and accuracy  0.8596201486374897
ep  101  training loss:  0.5283787267532031
valid loss  0.5434609354280618  and accuracy  0.8604459124690339
ep  102  training loss:  0.5289923454688163
valid loss  0.5435168926721757  and accuracy  0.8604459124690339
ep  103  training loss:  0.5280612703346541
valid loss  0.5430351765957083  and accuracy  0.8596201486374897
ep  104  training loss:  0.5263969914264924
valid loss  0.5449726824224931  and accuracy  0.8596201486374897
ep  105  training loss:  0.5292860250783343
valid loss  0.5433101808109331  and accuracy  0.8604459124690339
ep  106  training loss:  0.5253843010064926
valid loss  0.5437171981907599  and accuracy  0.8604459124690339
ep  107  training loss:  0.5259847088206983
valid loss  0.5425466247376679  and accuracy  0.8604459124690339
ep  108  training loss:  0.5253400534872634
valid loss  0.5448679357496399  and accuracy  0.8604459124690339
ep  109  training loss:  0.5248433494427877
valid loss  0.5416684135191703  and accuracy  0.8596201486374897
ep  110  training loss:  0.5249453169586724
valid loss  0.5400929851880455  and accuracy  0.8604459124690339
ep  111  training loss:  0.5230145627650055
valid loss  0.539902612406985  and accuracy  0.8604459124690339
ep  112  training loss:  0.5246183803489441
valid loss  0.5422672066533004  and accuracy  0.8604459124690339
ep  113  training loss:  0.5240768288306089
valid loss  0.5420699764341486  and accuracy  0.8604459124690339
ep  114  training loss:  0.52409224529831
valid loss  0.5411368109882616  and accuracy  0.8604459124690339
ep  115  training loss:  0.5245825302404479
valid loss  0.5408166975742721  and accuracy  0.8596201486374897
ep  116  training loss:  0.5237963162283585
valid loss  0.5403219651427769  and accuracy  0.8604459124690339
ep  117  training loss:  0.522208531681388
valid loss  0.5395423024031468  and accuracy  0.8604459124690339
ep  118  training loss:  0.5206772741097413
valid loss  0.540881444608545  and accuracy  0.8604459124690339
ep  119  training loss:  0.5238606283324828
valid loss  0.5397177085612649  and accuracy  0.8604459124690339
ep  120  training loss:  0.5211420690735291
valid loss  0.5395729647107994  and accuracy  0.8596201486374897
ep  121  training loss:  0.5214129515153862
valid loss  0.5391255930176853  and accuracy  0.8604459124690339
ep  122  training loss:  0.5212370060120568
valid loss  0.538835411668316  and accuracy  0.8604459124690339
ep  123  training loss:  0.5197573293433689
valid loss  0.5389913363362225  and accuracy  0.8587943848059455
ep  124  training loss:  0.5206985902657927
valid loss  0.5403209634303653  and accuracy  0.8604459124690339
ep  125  training loss:  0.52142525144083
valid loss  0.5396072746898201  and accuracy  0.8604459124690339
ep  126  training loss:  0.5207115944807301
valid loss  0.5385702484457282  and accuracy  0.8596201486374897
ep  127  training loss:  0.5192693053285832
valid loss  0.5395923226850275  and accuracy  0.8596201486374897
ep  128  training loss:  0.5203844496776454
valid loss  0.5374927866271859  and accuracy  0.8596201486374897
ep  129  training loss:  0.5184566452771139
valid loss  0.5368305081970726  and accuracy  0.8596201486374897
ep  130  training loss:  0.5188213046369768
valid loss  0.5370676688277751  and accuracy  0.8596201486374897
ep  131  training loss:  0.5187085269374766
valid loss  0.5358751035740709  and accuracy  0.8604459124690339
ep  132  training loss:  0.5192525952135714
valid loss  0.5366510532298037  and accuracy  0.8596201486374897
ep  133  training loss:  0.5179998766654585
valid loss  0.5367186463243404  and accuracy  0.8596201486374897
ep  134  training loss:  0.5183720221587135
valid loss  0.5362307262952224  and accuracy  0.8604459124690339
ep  135  training loss:  0.5159564212686455
valid loss  0.5345629787366317  and accuracy  0.8604459124690339
ep  136  training loss:  0.5177036516722021
valid loss  0.5355326508215692  and accuracy  0.8604459124690339
ep  137  training loss:  0.5185617531343378
valid loss  0.5368545820012159  and accuracy  0.8604459124690339
ep  138  training loss:  0.5162694731314368
valid loss  0.5368278144215081  and accuracy  0.8604459124690339
ep  139  training loss:  0.5152481599823139
valid loss  0.535040460963765  and accuracy  0.8604459124690339
ep  140  training loss:  0.5165973346242312
valid loss  0.5358983844297175  and accuracy  0.8596201486374897
ep  141  training loss:  0.5156756253912134
valid loss  0.5339556369625961  and accuracy  0.8604459124690339
ep  142  training loss:  0.515208502749871
valid loss  0.5356198582247601  and accuracy  0.8596201486374897
ep  143  training loss:  0.5159357992573403
valid loss  0.5348617412352936  and accuracy  0.8604459124690339
ep  144  training loss:  0.5143073174930517
valid loss  0.5343967780507762  and accuracy  0.8596201486374897
ep  145  training loss:  0.5160943700601879
valid loss  0.5329414032688818  and accuracy  0.8604459124690339
ep  146  training loss:  0.5155200209766942
valid loss  0.5323591342058978  and accuracy  0.8604459124690339
ep  147  training loss:  0.5150366480782922
valid loss  0.531967205414587  and accuracy  0.8596201486374897
ep  148  training loss:  0.5140909456522308
valid loss  0.5332701512351694  and accuracy  0.8604459124690339
ep  149  training loss:  0.5137514012067164
valid loss  0.5317107105826071  and accuracy  0.8620974401321222
ep  150  training loss:  0.514539371305219
valid loss  0.5335246664891058  and accuracy  0.8604459124690339
ep  151  training loss:  0.5134542400433294
valid loss  0.5334021314228005  and accuracy  0.8596201486374897
ep  152  training loss:  0.5127507009206403
valid loss  0.5317568335525267  and accuracy  0.8596201486374897
ep  153  training loss:  0.5122785471018834
valid loss  0.5328115993152464  and accuracy  0.8604459124690339
ep  154  training loss:  0.5128616445402088
valid loss  0.5308067072136751  and accuracy  0.8596201486374897
ep  155  training loss:  0.5123298546896928
valid loss  0.5319469907035718  and accuracy  0.8596201486374897
ep  156  training loss:  0.5136937264458155
valid loss  0.5310621450106828  and accuracy  0.8604459124690339
ep  157  training loss:  0.5125957675614806
valid loss  0.5312319307362708  and accuracy  0.8604459124690339
ep  158  training loss:  0.5115523128180145
valid loss  0.5324105159198814  and accuracy  0.8596201486374897
ep  159  training loss:  0.5127507337950664
valid loss  0.5322113676961053  and accuracy  0.8596201486374897
ep  160  training loss:  0.510744446147579
valid loss  0.5317006884873356  and accuracy  0.8604459124690339
ep  161  training loss:  0.5122411689426203
valid loss  0.5304876737708794  and accuracy  0.8596201486374897
ep  162  training loss:  0.5103013643800794
valid loss  0.5297873584225983  and accuracy  0.861271676300578
ep  163  training loss:  0.5085319624826388
valid loss  0.5306072151926704  and accuracy  0.8604459124690339
ep  164  training loss:  0.5125659732596167
valid loss  0.5304128933701016  and accuracy  0.8629232039636664
ep  165  training loss:  0.5106858514180057
valid loss  0.5302567947117579  and accuracy  0.8604459124690339
ep  166  training loss:  0.5104112719812861
valid loss  0.5336179272090965  and accuracy  0.8596201486374897
ep  167  training loss:  0.5109338163843437
valid loss  0.5306655582960335  and accuracy  0.8596201486374897
ep  168  training loss:  0.509867767586908
valid loss  0.5299775120052398  and accuracy  0.8596201486374897
ep  169  training loss:  0.5104804670970138
valid loss  0.5285445450259869  and accuracy  0.8604459124690339
ep  170  training loss:  0.5116623022139821
valid loss  0.5299534001791408  and accuracy  0.861271676300578
ep  171  training loss:  0.5082257618680123
valid loss  0.5270850139152502  and accuracy  0.8620974401321222
ep  172  training loss:  0.5098600682436866
valid loss  0.5295790326585462  and accuracy  0.8629232039636664
ep  173  training loss:  0.5085639162838236
valid loss  0.529179766048192  and accuracy  0.8629232039636664
ep  174  training loss:  0.5103913375962288
valid loss  0.5269317077665069  and accuracy  0.8620974401321222
ep  175  training loss:  0.507044686280073
valid loss  0.5280750881631349  and accuracy  0.8629232039636664
ep  176  training loss:  0.5076779655308926
valid loss  0.5270944479130005  and accuracy  0.8604459124690339
ep  177  training loss:  0.5082815664177364
valid loss  0.5296690134742258  and accuracy  0.8604459124690339
ep  178  training loss:  0.5077857329646714
valid loss  0.5307896017880601  and accuracy  0.8604459124690339
ep  179  training loss:  0.507242259495489
valid loss  0.5289211916490393  and accuracy  0.8629232039636664
ep  180  training loss:  0.5066078769705693
valid loss  0.5282468922762316  and accuracy  0.861271676300578
ep  181  training loss:  0.5061350784037713
valid loss  0.5275522369820652  and accuracy  0.8637489677952106
ep  182  training loss:  0.5067318754413158
valid loss  0.52521250948839  and accuracy  0.8637489677952106
ep  183  training loss:  0.5079849352651505
valid loss  0.5268451324875546  and accuracy  0.8645747316267548
ep  184  training loss:  0.5052864423215807
valid loss  0.5267125915191471  and accuracy  0.8637489677952106
ep  185  training loss:  0.5055652298695539
valid loss  0.5251309292607815  and accuracy  0.8620974401321222
ep  186  training loss:  0.5057355886162095
valid loss  0.5282210899127799  and accuracy  0.8620974401321222
ep  187  training loss:  0.5042952867748881
valid loss  0.5246146652542983  and accuracy  0.8629232039636664
ep  188  training loss:  0.5070977740839204
valid loss  0.5248506080503802  and accuracy  0.8620974401321222
ep  189  training loss:  0.5065135113023619
valid loss  0.5238574971648704  and accuracy  0.8637489677952106
ep  190  training loss:  0.5048886227911049
valid loss  0.5258770830664725  and accuracy  0.8620974401321222
ep  191  training loss:  0.5053234353304259
valid loss  0.5250338615217452  and accuracy  0.8637489677952106
ep  192  training loss:  0.5041919228614903
valid loss  0.5233889797519398  and accuracy  0.8637489677952106
ep  193  training loss:  0.5050172116021079
valid loss  0.5270441809190787  and accuracy  0.8629232039636664
ep  194  training loss:  0.502858629333236
valid loss  0.5245922472632493  and accuracy  0.8645747316267548
ep  195  training loss:  0.5044462473303823
valid loss  0.5236292333272151  and accuracy  0.8645747316267548
ep  196  training loss:  0.5043714576173113
valid loss  0.523197172275759  and accuracy  0.8637489677952106
ep  197  training loss:  0.5036497789132344
valid loss  0.5236327953322872  and accuracy  0.8637489677952106
ep  198  training loss:  0.5030963629012667
valid loss  0.5252253885706233  and accuracy  0.8629232039636664
ep  199  training loss:  0.5046316316146832
valid loss  0.5238349246250313  and accuracy  0.8637489677952106
ep  200  training loss:  0.5026980361236948
valid loss  0.5244624690513391  and accuracy  0.8637489677952106
ep  201  training loss:  0.5029350961135468
valid loss  0.5235356701296484  and accuracy  0.8645747316267548
ep  202  training loss:  0.5018694772842444
valid loss  0.5202019190512641  and accuracy  0.8645747316267548
ep  203  training loss:  0.5033158287977126
valid loss  0.5216375250249338  and accuracy  0.8645747316267548
ep  204  training loss:  0.5027737157746525
valid loss  0.5206140073540779  and accuracy  0.8637489677952106
ep  205  training loss:  0.502701177106676
valid loss  0.5219776047152197  and accuracy  0.8645747316267548
ep  206  training loss:  0.5027424662192131
valid loss  0.5219211109800835  and accuracy  0.8645747316267548
ep  207  training loss:  0.5007150963476209
valid loss  0.5209664614805006  and accuracy  0.8645747316267548
ep  208  training loss:  0.5032970636980975
valid loss  0.5203664496258011  and accuracy  0.8637489677952106
ep  209  training loss:  0.5035040970242173
valid loss  0.5187789144395896  and accuracy  0.8645747316267548
ep  210  training loss:  0.5019804324436452
valid loss  0.5208933250801707  and accuracy  0.8645747316267548
ep  211  training loss:  0.49898696933479414
valid loss  0.5176593376328195  and accuracy  0.8645747316267548
ep  212  training loss:  0.5016131113927257
valid loss  0.5186046165045778  and accuracy  0.8637489677952106
ep  213  training loss:  0.5003560132351149
valid loss  0.5216409635878515  and accuracy  0.8645747316267548
ep  214  training loss:  0.5011396215296826
valid loss  0.5186535958216074  and accuracy  0.8645747316267548
ep  215  training loss:  0.5026475958170071
valid loss  0.5187348622019679  and accuracy  0.8645747316267548
ep  216  training loss:  0.49767412249405374
valid loss  0.5192008555476868  and accuracy  0.8645747316267548
ep  217  training loss:  0.5021198615711289
valid loss  0.5231295440385993  and accuracy  0.8645747316267548
ep  218  training loss:  0.49979849389396197
valid loss  0.5205101036119816  and accuracy  0.8654004954582989
ep  219  training loss:  0.4986771082600068
valid loss  0.5190596778367198  and accuracy  0.8654004954582989
ep  220  training loss:  0.4999906642122771
valid loss  0.5209275169573374  and accuracy  0.8645747316267548
ep  221  training loss:  0.5005089723253071
valid loss  0.5196286408887433  and accuracy  0.8637489677952106
ep  222  training loss:  0.5005383718180979
valid loss  0.5190052006187565  and accuracy  0.8637489677952106
ep  223  training loss:  0.49980236624664204
valid loss  0.5188687899504487  and accuracy  0.8645747316267548
ep  224  training loss:  0.4984238424565193
valid loss  0.5181547619408561  and accuracy  0.8645747316267548
ep  225  training loss:  0.49961459499124095
valid loss  0.5187210547933886  and accuracy  0.8637489677952106
ep  226  training loss:  0.49828916624774167
valid loss  0.5204217755824057  and accuracy  0.8637489677952106
ep  227  training loss:  0.4981548338375051
valid loss  0.5197597949786387  and accuracy  0.8645747316267548
ep  228  training loss:  0.4988258414236606
valid loss  0.5178741667508883  and accuracy  0.8645747316267548
ep  229  training loss:  0.4986553066726844
valid loss  0.5200390366362108  and accuracy  0.8645747316267548
ep  230  training loss:  0.4963048989728854
valid loss  0.51895564363871  and accuracy  0.8645747316267548
ep  231  training loss:  0.49725992845159006
valid loss  0.5175845015649457  and accuracy  0.8637489677952106
ep  232  training loss:  0.4988796617054974
valid loss  0.5201933899268348  and accuracy  0.8629232039636664
ep  233  training loss:  0.4993241064127731
valid loss  0.5175340247488928  and accuracy  0.8645747316267548
ep  234  training loss:  0.49711203758877187
valid loss  0.5162125347961974  and accuracy  0.8637489677952106
ep  235  training loss:  0.4964263804719919
valid loss  0.5182529470085803  and accuracy  0.8645747316267548
ep  236  training loss:  0.4973196693188875
valid loss  0.5190238726719857  and accuracy  0.8645747316267548
ep  237  training loss:  0.4981366417289772
valid loss  0.5179433226093194  and accuracy  0.8645747316267548
ep  238  training loss:  0.49692310625937886
valid loss  0.5161919881990387  and accuracy  0.8654004954582989
ep  239  training loss:  0.49649951637734246
valid loss  0.5196416565054808  and accuracy  0.8637489677952106
ep  240  training loss:  0.49748672915601627
valid loss  0.5133616006098731  and accuracy  0.8645747316267548
ep  241  training loss:  0.4967859941202788
valid loss  0.5185946071571205  and accuracy  0.8645747316267548
ep  242  training loss:  0.4960618076365684
valid loss  0.5178766957892552  and accuracy  0.8645747316267548
ep  243  training loss:  0.49647150078949115
valid loss  0.5187432144812372  and accuracy  0.8645747316267548
ep  244  training loss:  0.49535501965728246
valid loss  0.5163973906776513  and accuracy  0.8629232039636664
ep  245  training loss:  0.49574530558744784
valid loss  0.5176120309126859  and accuracy  0.8637489677952106
ep  246  training loss:  0.4959046717200266
valid loss  0.5178141193533613  and accuracy  0.8645747316267548
ep  247  training loss:  0.4953693908611454
valid loss  0.51644875509103  and accuracy  0.8654004954582989
ep  248  training loss:  0.4968045355201915
valid loss  0.5157185942057634  and accuracy  0.8654004954582989
ep  249  training loss:  0.49708376002630583
valid loss  0.51379311493268  and accuracy  0.8645747316267548
ep  250  training loss:  0.49549602883769334
valid loss  0.5153497455781595  and accuracy  0.8654004954582989
ep  251  training loss:  0.49552205884034706
valid loss  0.5126750484957959  and accuracy  0.8654004954582989
ep  252  training loss:  0.49408904993501035
valid loss  0.5171448459416554  and accuracy  0.8637489677952106
ep  253  training loss:  0.4956353181261278
valid loss  0.5146197244512651  and accuracy  0.8637489677952106
ep  254  training loss:  0.49530782682668945
valid loss  0.5168425558422539  and accuracy  0.8645747316267548
ep  255  training loss:  0.4956462234623221
valid loss  0.5153244617406639  and accuracy  0.8654004954582989
ep  256  training loss:  0.49385031471048674
valid loss  0.5161271122838721  and accuracy  0.8620974401321222
ep  257  training loss:  0.4954145673417555
valid loss  0.5159655386115971  and accuracy  0.8654004954582989
ep  258  training loss:  0.4927255190851793
valid loss  0.5119021287838552  and accuracy  0.8645747316267548
ep  259  training loss:  0.49341255770074155
valid loss  0.5138040496533218  and accuracy  0.8645747316267548
ep  260  training loss:  0.4946542062877617
valid loss  0.5130129935670549  and accuracy  0.8645747316267548
ep  261  training loss:  0.4929154628331501
valid loss  0.5132406485858385  and accuracy  0.8654004954582989
ep  262  training loss:  0.4936852137623633
valid loss  0.51391237964323  and accuracy  0.8645747316267548
ep  263  training loss:  0.4943059653972183
valid loss  0.5114100523520658  and accuracy  0.8654004954582989
ep  264  training loss:  0.49482117107925205
valid loss  0.5120837995082657  and accuracy  0.8654004954582989
ep  265  training loss:  0.4925012372870407
valid loss  0.514386605419077  and accuracy  0.8654004954582989
ep  266  training loss:  0.4932529488233617
valid loss  0.5116472531211563  and accuracy  0.8654004954582989
ep  267  training loss:  0.49239053983436365
valid loss  0.5124565700082913  and accuracy  0.8645747316267548
ep  268  training loss:  0.4939031912245694
valid loss  0.5135643688191273  and accuracy  0.8637489677952106
ep  269  training loss:  0.49140524678509495
valid loss  0.5119568800798238  and accuracy  0.8662262592898431
ep  270  training loss:  0.4930513910165406
valid loss  0.5130163472511865  and accuracy  0.8654004954582989
ep  271  training loss:  0.49208875227238924
valid loss  0.5149088513742883  and accuracy  0.8645747316267548
ep  272  training loss:  0.49253399504123513
valid loss  0.513986385085187  and accuracy  0.8654004954582989
ep  273  training loss:  0.49264487747635854
valid loss  0.5130255492534842  and accuracy  0.8654004954582989
ep  274  training loss:  0.4925833495925912
valid loss  0.5124200931961728  and accuracy  0.8645747316267548
ep  275  training loss:  0.4921171412180579
valid loss  0.5133023054318228  and accuracy  0.8654004954582989
ep  276  training loss:  0.49147711160268115
valid loss  0.513674147162528  and accuracy  0.8654004954582989
ep  277  training loss:  0.4919378575775484
valid loss  0.5118680186354356  and accuracy  0.8654004954582989
ep  278  training loss:  0.4908546071576753
valid loss  0.5134881477922965  and accuracy  0.8645747316267548
ep  279  training loss:  0.4905866437976385
valid loss  0.5094461186083755  and accuracy  0.8670520231213873
ep  280  training loss:  0.4921988101441871
valid loss  0.5156183211297461  and accuracy  0.8629232039636664
ep  281  training loss:  0.4904324311578941
valid loss  0.5103842002808014  and accuracy  0.8654004954582989
ep  282  training loss:  0.49047793922154437
valid loss  0.5117590823860625  and accuracy  0.8645747316267548
ep  283  training loss:  0.49096161306944625
valid loss  0.5121419384005048  and accuracy  0.8645747316267548
ep  284  training loss:  0.49101645645903175
valid loss  0.5105914588953029  and accuracy  0.8654004954582989
ep  285  training loss:  0.4893452115557435
valid loss  0.5121330246267783  and accuracy  0.8654004954582989
ep  286  training loss:  0.4896626021622484
valid loss  0.5116436442592536  and accuracy  0.8654004954582989
ep  287  training loss:  0.4909777193751154
valid loss  0.5099815487812414  and accuracy  0.8654004954582989
ep  288  training loss:  0.4926132343367989
valid loss  0.5099229226911688  and accuracy  0.8662262592898431
ep  289  training loss:  0.4907073470142727
valid loss  0.5100328296693665  and accuracy  0.8654004954582989
ep  290  training loss:  0.4902898132159414
valid loss  0.5114991171954586  and accuracy  0.8654004954582989
ep  291  training loss:  0.49065011661469965
valid loss  0.5117107164899147  and accuracy  0.8654004954582989
ep  292  training loss:  0.4919720659091099
valid loss  0.5112668494367087  and accuracy  0.8654004954582989
ep  293  training loss:  0.4884114158452967
valid loss  0.5085425913137015  and accuracy  0.8654004954582989
ep  294  training loss:  0.48998552862669625
valid loss  0.5128030871970903  and accuracy  0.8645747316267548
ep  295  training loss:  0.48916150627333
valid loss  0.5087312997421088  and accuracy  0.8654004954582989
ep  296  training loss:  0.48928262992582633
valid loss  0.5093401858473887  and accuracy  0.8654004954582989
ep  297  training loss:  0.48969257173929415
valid loss  0.5116109211616138  and accuracy  0.8645747316267548
ep  298  training loss:  0.4907571227306294
valid loss  0.5110621951908045  and accuracy  0.8654004954582989
ep  299  training loss:  0.4894161317646358
valid loss  0.5104727130060251  and accuracy  0.8645747316267548
ep  300  training loss:  0.48880991370307353
valid loss  0.5117100820277566  and accuracy  0.8645747316267548
ep  301  training loss:  0.4889585210660254
valid loss  0.5111506448414186  and accuracy  0.8654004954582989
ep  302  training loss:  0.48899774012995706
valid loss  0.5093393383638412  and accuracy  0.8637489677952106
ep  303  training loss:  0.48784532329811003
valid loss  0.50925204563889  and accuracy  0.8654004954582989
ep  304  training loss:  0.4873757796446197
valid loss  0.5085074555076518  and accuracy  0.8654004954582989
ep  305  training loss:  0.48677978202970396
valid loss  0.5118525230028135  and accuracy  0.8654004954582989
ep  306  training loss:  0.48823709174677127
valid loss  0.5097560145754543  and accuracy  0.8654004954582989
ep  307  training loss:  0.4888290016555102
valid loss  0.5095278800862767  and accuracy  0.8654004954582989
ep  308  training loss:  0.48916924591846866
valid loss  0.5119476661910507  and accuracy  0.8637489677952106
ep  309  training loss:  0.487942353515597
valid loss  0.5098321857184639  and accuracy  0.8645747316267548
ep  310  training loss:  0.4903962012751615
valid loss  0.508118136179142  and accuracy  0.8654004954582989
ep  311  training loss:  0.4899746719532078
valid loss  0.5090098861423432  and accuracy  0.8654004954582989
ep  312  training loss:  0.48839333037571
valid loss  0.5079512825000394  and accuracy  0.8654004954582989
ep  313  training loss:  0.48718271833359755
valid loss  0.5065285672735912  and accuracy  0.8662262592898431
ep  314  training loss:  0.4866251897178025
valid loss  0.5090993592111066  and accuracy  0.8662262592898431
ep  315  training loss:  0.4869109505377048
valid loss  0.510889934518926  and accuracy  0.8654004954582989
ep  316  training loss:  0.48594565611545926
valid loss  0.5097611528797449  and accuracy  0.8645747316267548
ep  317  training loss:  0.48819570183326205
valid loss  0.5089799854304552  and accuracy  0.8654004954582989
ep  318  training loss:  0.48728741657051444
valid loss  0.5094071911151108  and accuracy  0.8654004954582989
ep  319  training loss:  0.4870380139957534
valid loss  0.5084988778037143  and accuracy  0.8654004954582989
ep  320  training loss:  0.48636085906205295
valid loss  0.5074697138847741  and accuracy  0.8654004954582989
ep  321  training loss:  0.4863074816346577
valid loss  0.5062226428658779  and accuracy  0.8645747316267548
ep  322  training loss:  0.48710968849846015
valid loss  0.5051914576062676  and accuracy  0.8662262592898431
ep  323  training loss:  0.48541071536081726
valid loss  0.5100398636080823  and accuracy  0.8654004954582989
ep  324  training loss:  0.4861002365035152
valid loss  0.5066830860889224  and accuracy  0.8654004954582989
ep  325  training loss:  0.48655962030663225
valid loss  0.5059794437580124  and accuracy  0.8662262592898431
ep  326  training loss:  0.48540170297033924
valid loss  0.5053490803174792  and accuracy  0.8662262592898431
ep  327  training loss:  0.4865308649056577
valid loss  0.5088316818636178  and accuracy  0.8645747316267548
ep  328  training loss:  0.4862766960300681
valid loss  0.505495055636328  and accuracy  0.8662262592898431
ep  329  training loss:  0.48539721472406056
valid loss  0.5086817106218204  and accuracy  0.8637489677952106
ep  330  training loss:  0.4865514237395392
valid loss  0.5069111783724989  and accuracy  0.8645747316267548
ep  331  training loss:  0.48584819900680376
valid loss  0.5062754373723667  and accuracy  0.8662262592898431
ep  332  training loss:  0.4881481210718029
valid loss  0.5112108070431023  and accuracy  0.8645747316267548
ep  333  training loss:  0.48628086055342007
valid loss  0.5110284883458195  and accuracy  0.8662262592898431
ep  334  training loss:  0.48481601743047925
valid loss  0.5057048347496376  and accuracy  0.8654004954582989
ep  335  training loss:  0.4873174926990048
valid loss  0.5067123245297139  and accuracy  0.8645747316267548
ep  336  training loss:  0.48514953646957726
valid loss  0.5061060249510134  and accuracy  0.8654004954582989
ep  337  training loss:  0.4860327772527588
valid loss  0.5063297146366806  and accuracy  0.8654004954582989
ep  338  training loss:  0.48669679956050393
valid loss  0.5076841829632256  and accuracy  0.8645747316267548
ep  339  training loss:  0.48395739167398855
valid loss  0.5033088372228167  and accuracy  0.8645747316267548
ep  340  training loss:  0.4849922338816883
valid loss  0.5049252308517522  and accuracy  0.8654004954582989
ep  341  training loss:  0.48486925464453423
valid loss  0.5044126117357433  and accuracy  0.8662262592898431
ep  342  training loss:  0.4851742558116879
valid loss  0.5070015286533032  and accuracy  0.8654004954582989
ep  343  training loss:  0.48360579128736697
valid loss  0.5050052084970041  and accuracy  0.8662262592898431
ep  344  training loss:  0.4844109042300192
valid loss  0.5045016453494522  and accuracy  0.8662262592898431
ep  345  training loss:  0.48427425396615464
valid loss  0.5051630913571421  and accuracy  0.8654004954582989
ep  346  training loss:  0.4827066559889561
valid loss  0.5042114755420228  and accuracy  0.8662262592898431
ep  347  training loss:  0.48317039881069884
valid loss  0.5068440333956239  and accuracy  0.8662262592898431
ep  348  training loss:  0.48338415844466515
valid loss  0.5055788700291187  and accuracy  0.8654004954582989
ep  349  training loss:  0.48422157923387016
valid loss  0.5043122825988751  and accuracy  0.8670520231213873
ep  350  training loss:  0.48285754967758576
valid loss  0.5042652063976133  and accuracy  0.8662262592898431
ep  351  training loss:  0.48547322871906295
valid loss  0.5067420657462665  and accuracy  0.8645747316267548
ep  352  training loss:  0.4841387752820881
valid loss  0.506314420877263  and accuracy  0.8654004954582989
ep  353  training loss:  0.48372428600170975
valid loss  0.5039295219469425  and accuracy  0.8662262592898431
ep  354  training loss:  0.4818860449345305
valid loss  0.5027135551517212  and accuracy  0.8678777869529315
ep  355  training loss:  0.48363891989114216
valid loss  0.5040329998923568  and accuracy  0.8645747316267548
ep  356  training loss:  0.481085018876559
valid loss  0.5053409360146148  and accuracy  0.8645747316267548
ep  357  training loss:  0.4831508983845929
valid loss  0.5041108489233438  and accuracy  0.8662262592898431
ep  358  training loss:  0.4820391383709283
valid loss  0.5019321186940203  and accuracy  0.8654004954582989
ep  359  training loss:  0.4838230000625639
valid loss  0.4999610859928399  and accuracy  0.8670520231213873
ep  360  training loss:  0.4828363772250178
valid loss  0.5010142159501351  and accuracy  0.8645747316267548
ep  361  training loss:  0.4843685029381699
valid loss  0.5012652712553419  and accuracy  0.8662262592898431
ep  362  training loss:  0.4847685172388282
valid loss  0.5034595636372523  and accuracy  0.8662262592898431
ep  363  training loss:  0.4835580639885059
valid loss  0.5046603350379465  and accuracy  0.8670520231213873
ep  364  training loss:  0.4830828400979208
valid loss  0.5059720827074311  and accuracy  0.8654004954582989
ep  365  training loss:  0.48243680742665423
valid loss  0.5067719179278657  and accuracy  0.8654004954582989
ep  366  training loss:  0.4807835730472332
valid loss  0.502740750472276  and accuracy  0.8662262592898431
ep  367  training loss:  0.48363916316578387
valid loss  0.5025577832607078  and accuracy  0.8662262592898431
ep  368  training loss:  0.48218495436310477
valid loss  0.5033850488241401  and accuracy  0.8654004954582989
ep  369  training loss:  0.48190489976193385
valid loss  0.5017292323090832  and accuracy  0.8670520231213873
ep  370  training loss:  0.4845968590068926
valid loss  0.5043252443942425  and accuracy  0.8670520231213873
ep  371  training loss:  0.48280466945947936
valid loss  0.5025404123212561  and accuracy  0.8662262592898431
ep  372  training loss:  0.48211906946827315
valid loss  0.5050415344517847  and accuracy  0.8670520231213873
ep  373  training loss:  0.48147220661931434
valid loss  0.5046851352952316  and accuracy  0.8678777869529315
ep  374  training loss:  0.48299416081587654
valid loss  0.5068037574641671  and accuracy  0.8670520231213873
ep  375  training loss:  0.48022653282301164
valid loss  0.5078485437801906  and accuracy  0.8654004954582989
ep  376  training loss:  0.48321430137448707
valid loss  0.5037507657469255  and accuracy  0.8670520231213873
ep  377  training loss:  0.48239273581452713
valid loss  0.5061642960710628  and accuracy  0.8678777869529315
ep  378  training loss:  0.4810394168123688
valid loss  0.5047149081844421  and accuracy  0.8678777869529315
ep  379  training loss:  0.48173734247733085
valid loss  0.5057602646329205  and accuracy  0.8654004954582989
ep  380  training loss:  0.4828630896916185
valid loss  0.507633930636673  and accuracy  0.8654004954582989
ep  381  training loss:  0.4819392040724312
valid loss  0.5049137913619655  and accuracy  0.8662262592898431
ep  382  training loss:  0.4810098119358355
valid loss  0.504447881246184  and accuracy  0.8654004954582989
ep  383  training loss:  0.4798392967772589
valid loss  0.5060580108994595  and accuracy  0.8654004954582989
ep  384  training loss:  0.48205252010949456
valid loss  0.5050483755490487  and accuracy  0.8662262592898431
ep  385  training loss:  0.4825691685816958
valid loss  0.5020931620030832  and accuracy  0.8678777869529315
ep  386  training loss:  0.48182811256898517
valid loss  0.5044901760327334  and accuracy  0.8645747316267548
ep  387  training loss:  0.48226395017500706
valid loss  0.5069688419041606  and accuracy  0.8654004954582989
ep  388  training loss:  0.48027830747266204
valid loss  0.5029087410693716  and accuracy  0.8662262592898431
ep  389  training loss:  0.48061707995004394
valid loss  0.5023757446215824  and accuracy  0.8670520231213873
ep  390  training loss:  0.4825877871730747
valid loss  0.5022500350542249  and accuracy  0.8670520231213873
ep  391  training loss:  0.4799036427279939
valid loss  0.5011279280770425  and accuracy  0.8687035507844756
ep  392  training loss:  0.4805541814365194
valid loss  0.5020596543242378  and accuracy  0.8662262592898431
ep  393  training loss:  0.4811457015380757
valid loss  0.5041141523052896  and accuracy  0.8654004954582989
ep  394  training loss:  0.4804583044780096
valid loss  0.5025348026923755  and accuracy  0.8662262592898431
ep  395  training loss:  0.48137700141127593
valid loss  0.5035769817850788  and accuracy  0.8678777869529315
ep  396  training loss:  0.4821914431905303
valid loss  0.5006674037158834  and accuracy  0.8670520231213873
ep  397  training loss:  0.4804756396105736
valid loss  0.5033278707922441  and accuracy  0.8662262592898431
ep  398  training loss:  0.48146293782970095
valid loss  0.5014281265505279  and accuracy  0.8670520231213873
ep  399  training loss:  0.47974385396727653
valid loss  0.5038782343895941  and accuracy  0.8670520231213873
ep  400  training loss:  0.47746012754686074
valid loss  0.5023501880596138  and accuracy  0.8670520231213873
ep  401  training loss:  0.4800291335557456
valid loss  0.502574083039231  and accuracy  0.8662262592898431
ep  402  training loss:  0.47902598854107564
valid loss  0.5044249203163764  and accuracy  0.8654004954582989
ep  403  training loss:  0.47916983012331776
valid loss  0.5045290849210801  and accuracy  0.8654004954582989
ep  404  training loss:  0.479920462276978
valid loss  0.503659574873284  and accuracy  0.8662262592898431
ep  405  training loss:  0.47866703119038406
valid loss  0.5018315690790298  and accuracy  0.8678777869529315
ep  406  training loss:  0.47899252102398754
valid loss  0.5010227898149624  and accuracy  0.8670520231213873
ep  407  training loss:  0.4780406018363926
valid loss  0.5019842177555642  and accuracy  0.8678777869529315
ep  408  training loss:  0.4789395102609178
valid loss  0.5036824921455194  and accuracy  0.8670520231213873
ep  409  training loss:  0.4791707926153495
valid loss  0.5031572786566643  and accuracy  0.8670520231213873
ep  410  training loss:  0.4772540607959484
valid loss  0.5035078663357421  and accuracy  0.8662262592898431
ep  411  training loss:  0.48010876802495167
valid loss  0.5046286207403061  and accuracy  0.8662262592898431
ep  412  training loss:  0.4799856118744844
valid loss  0.5046037280490633  and accuracy  0.8662262592898431
ep  413  training loss:  0.478441074279097
valid loss  0.5057151304307208  and accuracy  0.8670520231213873
ep  414  training loss:  0.4799298579366964
valid loss  0.5016297320508445  and accuracy  0.8670520231213873
ep  415  training loss:  0.4803287952073809
valid loss  0.5042791066092956  and accuracy  0.8654004954582989
ep  416  training loss:  0.4781695517751292
valid loss  0.5013081932441938  and accuracy  0.8670520231213873
ep  417  training loss:  0.4795245433870604
valid loss  0.4993325049575944  and accuracy  0.8670520231213873
ep  418  training loss:  0.47918033491941014
valid loss  0.5032388456196553  and accuracy  0.8662262592898431
ep  419  training loss:  0.47704103832590194
valid loss  0.5016439889306179  and accuracy  0.8678777869529315
ep  420  training loss:  0.4778315002261824
valid loss  0.499758302377926  and accuracy  0.8678777869529315
ep  421  training loss:  0.4774594761831028
valid loss  0.5019570289565747  and accuracy  0.8678777869529315
ep  422  training loss:  0.47868611904560227
valid loss  0.5038193616366997  and accuracy  0.8687035507844756
ep  423  training loss:  0.47627272624950817
valid loss  0.502139382673432  and accuracy  0.8695293146160198
ep  424  training loss:  0.47790736934135797
valid loss  0.5014001471901807  and accuracy  0.8695293146160198
ep  425  training loss:  0.4791373726672594
valid loss  0.503176607812957  and accuracy  0.8670520231213873
ep  426  training loss:  0.47617376358644004
valid loss  0.5009708389775208  and accuracy  0.8678777869529315
ep  427  training loss:  0.4768527855571342
valid loss  0.5029449790199368  and accuracy  0.8662262592898431
ep  428  training loss:  0.4789396257272962
valid loss  0.499543040793952  and accuracy  0.8678777869529315
ep  429  training loss:  0.4795759644093822
valid loss  0.5001533645967331  and accuracy  0.8662262592898431
ep  430  training loss:  0.4786012707011693
valid loss  0.4988891533787441  and accuracy  0.8670520231213873
ep  431  training loss:  0.47819896881099316
valid loss  0.5025115367994143  and accuracy  0.8670520231213873
ep  432  training loss:  0.47475636338860966
valid loss  0.5013621991375672  and accuracy  0.8662262592898431
ep  433  training loss:  0.47721034282573266
valid loss  0.4987026980649726  and accuracy  0.8662262592898431
ep  434  training loss:  0.47636065052804305
valid loss  0.49975669826976926  and accuracy  0.8662262592898431
ep  435  training loss:  0.47932919407334185
valid loss  0.5007468888230012  and accuracy  0.8678777869529315
ep  436  training loss:  0.4770667797938546
valid loss  0.4983764460370326  and accuracy  0.8670520231213873
ep  437  training loss:  0.476872780834981
valid loss  0.5013709609957395  and accuracy  0.8662262592898431
ep  438  training loss:  0.4755810948729496
valid loss  0.49888421817814194  and accuracy  0.8687035507844756
ep  439  training loss:  0.4778785289082475
valid loss  0.500749356534046  and accuracy  0.8654004954582989
ep  440  training loss:  0.4759228199628025
valid loss  0.5006992072974424  and accuracy  0.8670520231213873
ep  441  training loss:  0.47673300938667706
valid loss  0.498350066945732  and accuracy  0.8678777869529315
ep  442  training loss:  0.4776126861941696
valid loss  0.5014014480578614  and accuracy  0.8645747316267548
ep  443  training loss:  0.4762407553767598
valid loss  0.4982227288328056  and accuracy  0.8670520231213873
ep  444  training loss:  0.47501248030362325
valid loss  0.4963232079485445  and accuracy  0.8670520231213873
ep  445  training loss:  0.4762440293546097
valid loss  0.49801259386352426  and accuracy  0.8670520231213873
ep  446  training loss:  0.47695417632443776
valid loss  0.49957794416157497  and accuracy  0.8678777869529315
ep  447  training loss:  0.4781695390297876
valid loss  0.4992372557016368  and accuracy  0.8678777869529315
ep  448  training loss:  0.4754788579987072
valid loss  0.4995283075446043  and accuracy  0.8670520231213873
ep  449  training loss:  0.47523049336723355
valid loss  0.49973119328968985  and accuracy  0.8678777869529315
ep  450  training loss:  0.47489722324144856
valid loss  0.498963178286958  and accuracy  0.8678777869529315
ep  451  training loss:  0.47638571347911873
valid loss  0.4973426097734815  and accuracy  0.8687035507844756
ep  452  training loss:  0.4754866563589416
valid loss  0.5035272770239045  and accuracy  0.8678777869529315
ep  453  training loss:  0.4753287335706757
valid loss  0.503314557742915  and accuracy  0.8645747316267548
ep  454  training loss:  0.47638378954987565
valid loss  0.5025758449446555  and accuracy  0.8670520231213873
ep  455  training loss:  0.4767633970274542
valid loss  0.4995672729367761  and accuracy  0.8662262592898431
ep  456  training loss:  0.47639868808130986
valid loss  0.4978035793729699  and accuracy  0.8654004954582989
ep  457  training loss:  0.4750415684396768
valid loss  0.505262110520748  and accuracy  0.8654004954582989
ep  458  training loss:  0.47599742328375944
valid loss  0.497235601421824  and accuracy  0.8678777869529315
ep  459  training loss:  0.47542399062532487
valid loss  0.4973090894452606  and accuracy  0.8678777869529315
ep  460  training loss:  0.4750331901907046
valid loss  0.5017371661499647  and accuracy  0.8687035507844756
ep  461  training loss:  0.4761272901093429
valid loss  0.49888364617535147  and accuracy  0.8678777869529315
ep  462  training loss:  0.4777096938812758
valid loss  0.49872432315674425  and accuracy  0.8662262592898431
ep  463  training loss:  0.4736628522298947
valid loss  0.4985122749817263  and accuracy  0.8670520231213873
ep  464  training loss:  0.47636968488503467
valid loss  0.5001765742565757  and accuracy  0.8670520231213873
ep  465  training loss:  0.4740947294227452
valid loss  0.4986073743893429  and accuracy  0.8662262592898431
ep  466  training loss:  0.4768412160583702
valid loss  0.4964883981609423  and accuracy  0.8662262592898431
ep  467  training loss:  0.47617609087025975
valid loss  0.49773337738165474  and accuracy  0.8670520231213873
ep  468  training loss:  0.4752784617484598
valid loss  0.49723476324073546  and accuracy  0.8670520231213873
ep  469  training loss:  0.4753420238801306
valid loss  0.4969768065445488  and accuracy  0.8678777869529315
ep  470  training loss:  0.4763676962714817
valid loss  0.4987132162667817  and accuracy  0.8670520231213873
ep  471  training loss:  0.4743253032927061
valid loss  0.5003405928513317  and accuracy  0.8678777869529315
ep  472  training loss:  0.4739958997885723
valid loss  0.4980594710709141  and accuracy  0.8662262592898431
ep  473  training loss:  0.4745425192377653
valid loss  0.4979975161770963  and accuracy  0.8678777869529315
ep  474  training loss:  0.4735364187961495
valid loss  0.4957680564887459  and accuracy  0.8662262592898431
ep  475  training loss:  0.47339620005538074
valid loss  0.4974735059293216  and accuracy  0.8662262592898431
ep  476  training loss:  0.47582240424329625
valid loss  0.4956104545234945  and accuracy  0.8678777869529315
ep  477  training loss:  0.4757723919992598
valid loss  0.499415548649039  and accuracy  0.8670520231213873
ep  478  training loss:  0.4749233625468296
valid loss  0.49805525631869163  and accuracy  0.8678777869529315
ep  479  training loss:  0.47411364021882246
valid loss  0.4978267993442683  and accuracy  0.8670520231213873
ep  480  training loss:  0.4754727922903735
valid loss  0.49961045972578394  and accuracy  0.8670520231213873
ep  481  training loss:  0.47494114003316007
valid loss  0.4991015318402764  and accuracy  0.8670520231213873
ep  482  training loss:  0.4738781718022865
valid loss  0.4967070324769402  and accuracy  0.8678777869529315
ep  483  training loss:  0.473511032063816
valid loss  0.4977543182258858  and accuracy  0.8687035507844756
ep  484  training loss:  0.4747068138510266
valid loss  0.49732172336586244  and accuracy  0.8687035507844756
ep  485  training loss:  0.47523450144662144
valid loss  0.49467859673756004  and accuracy  0.8678777869529315
ep  486  training loss:  0.4735748835516839
valid loss  0.5010826880039999  and accuracy  0.8678777869529315
ep  487  training loss:  0.4746388935513917
valid loss  0.4989157421445177  and accuracy  0.8654004954582989
ep  488  training loss:  0.47393657715027765
valid loss  0.5045807009192167  and accuracy  0.8678777869529315
ep  489  training loss:  0.47353790532004636
valid loss  0.4949041381785931  and accuracy  0.8678777869529315
ep  490  training loss:  0.4731796179067017
valid loss  0.4982281070322561  and accuracy  0.8678777869529315
ep  491  training loss:  0.47655098803512735
valid loss  0.49786245749077457  and accuracy  0.8678777869529315
ep  492  training loss:  0.47473046256013884
valid loss  0.49904720188663776  and accuracy  0.8662262592898431
ep  493  training loss:  0.4735262768242531
valid loss  0.495237387230729  and accuracy  0.8678777869529315
ep  494  training loss:  0.47387802726758516
valid loss  0.4942717286892709  and accuracy  0.8678777869529315
ep  495  training loss:  0.474574419943082
valid loss  0.49483736953294344  and accuracy  0.8678777869529315
ep  496  training loss:  0.4742050017779345
valid loss  0.496290294691908  and accuracy  0.8687035507844756
ep  497  training loss:  0.4741229921788381
valid loss  0.4972407402429179  and accuracy  0.8687035507844756
ep  498  training loss:  0.4738422267696749
valid loss  0.49869390925920076  and accuracy  0.8670520231213873
ep  499  training loss:  0.47320085468387124
valid loss  0.49458770629889115  and accuracy  0.8678777869529315

True positive:  [  143    93    97    21 43615]
True negative:  [48437 48048 48700 49107   463]
False positive:  [  32   13   85   17 6146]
False negative:  [1650 2108 1380 1117   38]

[[  143     2    49     2  1597]
 [    5    93    18     4  2081]
 [   14     1    97     2  1363]
 [    3     6     3    21  1105]
 [   10     4    15     9 43615]]

              precision    recall  f1-score   support

           0       0.82      0.08      0.15      1793
           1       0.88      0.04      0.08      2201
           2       0.53      0.07      0.12      1477
           3       0.55      0.02      0.04      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.73      0.24      0.26     50262
weighted avg       0.86      0.87      0.82     50262

Accuracy:  0.8747960686005332
Precision_weighted:  0.8569832402971056
Recall_weighted:  0.8747960686005332
mcc:  0.21132678284865614
f2:  0.8711745084087079

time = 12 min

- train_loop(model, epochs=500, lr=0.0003)

ep  0  training loss:  1.6048275179622653
valid loss  1.4537909423684798  and accuracy  0.726672171758877
ep  1  training loss:  1.3667617871970925
valid loss  1.1677157298087286  and accuracy  0.8398018166804294
ep  2  training loss:  1.03132453081056
valid loss  0.8158296664621494  and accuracy  0.8579686209744013
ep  3  training loss:  0.7585654501288114
valid loss  0.6316452193220817  and accuracy  0.8596201486374897
ep  4  training loss:  0.638168946416513
valid loss  0.5892102150854053  and accuracy  0.8596201486374897
ep  5  training loss:  0.6014498209821091
valid loss  0.5687735402121414  and accuracy  0.8596201486374897
ep  6  training loss:  0.5886444779481453
valid loss  0.5707014371893998  and accuracy  0.8596201486374897
ep  7  training loss:  0.5798579193876494
valid loss  0.5697803068613829  and accuracy  0.8604459124690339
ep  8  training loss:  0.5744911145763401
valid loss  0.5684735838979065  and accuracy  0.8596201486374897
ep  9  training loss:  0.5679591803736602
valid loss  0.5665158752465819  and accuracy  0.8604459124690339
ep  10  training loss:  0.568657207364574
valid loss  0.5602519809953641  and accuracy  0.8604459124690339
ep  11  training loss:  0.5655026888792909
valid loss  0.5622415783285208  and accuracy  0.8604459124690339
ep  12  training loss:  0.565300706043035
valid loss  0.5613419474106403  and accuracy  0.8604459124690339
ep  13  training loss:  0.5612642090932286
valid loss  0.5596316015789274  and accuracy  0.8604459124690339
ep  14  training loss:  0.5598889661445798
valid loss  0.5568171023190465  and accuracy  0.861271676300578
ep  15  training loss:  0.5580643619707059
valid loss  0.5549963679223096  and accuracy  0.861271676300578
ep  16  training loss:  0.5563831987903628
valid loss  0.5552927917138689  and accuracy  0.8596201486374897
ep  17  training loss:  0.5553868582326074
valid loss  0.5545571236252096  and accuracy  0.8604459124690339
ep  18  training loss:  0.5500427863712977
valid loss  0.5545260003536423  and accuracy  0.8604459124690339
ep  19  training loss:  0.5499522823860431
valid loss  0.553489964526119  and accuracy  0.8604459124690339
ep  20  training loss:  0.551423757777403
valid loss  0.552243392246996  and accuracy  0.8604459124690339
ep  21  training loss:  0.5486123267092726
valid loss  0.5538774285308592  and accuracy  0.8604459124690339
ep  22  training loss:  0.5485891966266785
valid loss  0.5525777151917348  and accuracy  0.8604459124690339
ep  23  training loss:  0.5463897456082176
valid loss  0.5532829789806258  and accuracy  0.861271676300578
ep  24  training loss:  0.5448342430574084
valid loss  0.5481354579957037  and accuracy  0.861271676300578
ep  25  training loss:  0.5430173285535179
valid loss  0.5516308979688844  and accuracy  0.8604459124690339
ep  26  training loss:  0.5432904825244015
valid loss  0.5479394864632412  and accuracy  0.861271676300578
ep  27  training loss:  0.5432098056106929
valid loss  0.5468955675399964  and accuracy  0.8604459124690339
ep  28  training loss:  0.5411233504353992
valid loss  0.5501381848391966  and accuracy  0.861271676300578
ep  29  training loss:  0.5426042846201529
valid loss  0.5484702202724485  and accuracy  0.8604459124690339
ep  30  training loss:  0.5367732882460807
valid loss  0.5494869379442059  and accuracy  0.861271676300578
ep  31  training loss:  0.5373711533792216
valid loss  0.546347456663527  and accuracy  0.8604459124690339
ep  32  training loss:  0.53698376754897
valid loss  0.544732300729224  and accuracy  0.861271676300578
ep  33  training loss:  0.5338289578651997
valid loss  0.54564877485658  and accuracy  0.8604459124690339
ep  34  training loss:  0.535292743411583
valid loss  0.5468397655829628  and accuracy  0.8604459124690339
ep  35  training loss:  0.5334863726008952
valid loss  0.5451117846761038  and accuracy  0.8604459124690339
ep  36  training loss:  0.5313528319242008
valid loss  0.5451583709232478  and accuracy  0.8604459124690339
ep  37  training loss:  0.531014465766559
valid loss  0.5423724010500605  and accuracy  0.8604459124690339
ep  38  training loss:  0.5320557497139177
valid loss  0.544121936861884  and accuracy  0.8596201486374897
ep  39  training loss:  0.5317574332694093
valid loss  0.542118521045792  and accuracy  0.861271676300578
ep  40  training loss:  0.5286388124081501
valid loss  0.5443505387873221  and accuracy  0.8604459124690339
ep  41  training loss:  0.5289162363163742
valid loss  0.5421900292943077  and accuracy  0.8604459124690339
ep  42  training loss:  0.530515874361568
valid loss  0.5428399915640065  and accuracy  0.8604459124690339
ep  43  training loss:  0.5258332630769792
valid loss  0.5410243486393393  and accuracy  0.8604459124690339
ep  44  training loss:  0.5261168568339634
valid loss  0.5435786276785034  and accuracy  0.8596201486374897
ep  45  training loss:  0.5232872631825547
valid loss  0.5384092917135193  and accuracy  0.8604459124690339
ep  46  training loss:  0.5237395968722628
valid loss  0.5388645396854738  and accuracy  0.8604459124690339
ep  47  training loss:  0.5216600001490227
valid loss  0.5378223753831881  and accuracy  0.8604459124690339
ep  48  training loss:  0.5232941543875113
valid loss  0.5383567320325964  and accuracy  0.8620974401321222
ep  49  training loss:  0.5232632467591574
valid loss  0.5385422552055213  and accuracy  0.8604459124690339
ep  50  training loss:  0.5217779224118407
valid loss  0.5358273034052451  and accuracy  0.8604459124690339
ep  51  training loss:  0.519273265080669
valid loss  0.5351557619408277  and accuracy  0.8629232039636664
ep  52  training loss:  0.5190665801320802
valid loss  0.5346350211136406  and accuracy  0.861271676300578
ep  53  training loss:  0.518348383448521
valid loss  0.533532622035725  and accuracy  0.8604459124690339
ep  54  training loss:  0.518857683302858
valid loss  0.5362329971288277  and accuracy  0.8620974401321222
ep  55  training loss:  0.5156898989294
valid loss  0.5366274594866866  and accuracy  0.8620974401321222
ep  56  training loss:  0.5173322918208577
valid loss  0.5334935968666408  and accuracy  0.861271676300578
ep  57  training loss:  0.5175953072273524
valid loss  0.5343926911950604  and accuracy  0.861271676300578
ep  58  training loss:  0.5171006098110122
valid loss  0.5360400379442362  and accuracy  0.8604459124690339
ep  59  training loss:  0.5138248493486314
valid loss  0.5315552381175692  and accuracy  0.8637489677952106
ep  60  training loss:  0.5138405261917033
valid loss  0.5314245059114168  and accuracy  0.861271676300578
ep  61  training loss:  0.5123231696413656
valid loss  0.5331833435916192  and accuracy  0.8629232039636664
ep  62  training loss:  0.5135357487339741
valid loss  0.5318341406783617  and accuracy  0.861271676300578
ep  63  training loss:  0.5138498644001661
valid loss  0.5320325574532311  and accuracy  0.8629232039636664
ep  64  training loss:  0.513309220022526
valid loss  0.5316166303356653  and accuracy  0.8637489677952106
ep  65  training loss:  0.5110832136036269
valid loss  0.530646621393429  and accuracy  0.861271676300578
ep  66  training loss:  0.5116316439267026
valid loss  0.527842197164241  and accuracy  0.8629232039636664
ep  67  training loss:  0.5127131574996979
valid loss  0.5285513281182352  and accuracy  0.8629232039636664
ep  68  training loss:  0.5105563893906931
valid loss  0.5304938457801655  and accuracy  0.8620974401321222
ep  69  training loss:  0.5098363548988737
valid loss  0.5283736769666758  and accuracy  0.8645747316267548
ep  70  training loss:  0.5116647140584282
valid loss  0.5283144703834732  and accuracy  0.8620974401321222
ep  71  training loss:  0.5090075015263424
valid loss  0.5268375164806892  and accuracy  0.8654004954582989
ep  72  training loss:  0.50780103715476
valid loss  0.5256233773725669  and accuracy  0.8670520231213873
ep  73  training loss:  0.5079477139192816
valid loss  0.5287681086116542  and accuracy  0.8645747316267548
ep  74  training loss:  0.5079087552112772
valid loss  0.5249849318178305  and accuracy  0.8654004954582989
ep  75  training loss:  0.5063232563094552
valid loss  0.5245101832585528  and accuracy  0.8654004954582989
ep  76  training loss:  0.5072470049915646
valid loss  0.5260126359397769  and accuracy  0.8654004954582989
ep  77  training loss:  0.5073531986770341
valid loss  0.5255190064895261  and accuracy  0.8654004954582989
ep  78  training loss:  0.5058725484820179
valid loss  0.5235393202865941  and accuracy  0.8637489677952106
ep  79  training loss:  0.5051043337699231
valid loss  0.5248762440523955  and accuracy  0.8637489677952106
ep  80  training loss:  0.5045374239313527
valid loss  0.524219587605222  and accuracy  0.8637489677952106
ep  81  training loss:  0.50511293457142
valid loss  0.5253023675509861  and accuracy  0.8654004954582989
ep  82  training loss:  0.5045415554927819
valid loss  0.52402102878328  and accuracy  0.8662262592898431
ep  83  training loss:  0.5037040280208874
valid loss  0.5225998594877053  and accuracy  0.8654004954582989
ep  84  training loss:  0.5031248307056984
valid loss  0.5224748940432397  and accuracy  0.8662262592898431
ep  85  training loss:  0.5029377616204284
valid loss  0.5229420631656363  and accuracy  0.8662262592898431
ep  86  training loss:  0.5030962429920042
valid loss  0.5230917701486907  and accuracy  0.8654004954582989
ep  87  training loss:  0.5012732993825344
valid loss  0.5214368953230359  and accuracy  0.8662262592898431
ep  88  training loss:  0.5021389086383374
valid loss  0.5222004039488383  and accuracy  0.8662262592898431
ep  89  training loss:  0.49928220817304286
valid loss  0.5219092008793875  and accuracy  0.8662262592898431
ep  90  training loss:  0.5010814119748318
valid loss  0.5189812471805576  and accuracy  0.8662262592898431
ep  91  training loss:  0.5016000433684445
valid loss  0.5212387420932287  and accuracy  0.8662262592898431
ep  92  training loss:  0.5003762049185785
valid loss  0.5205304616005533  and accuracy  0.8662262592898431
ep  93  training loss:  0.4982345170617745
valid loss  0.5225126492544012  and accuracy  0.8662262592898431
ep  94  training loss:  0.4994931470956991
valid loss  0.5223286927829194  and accuracy  0.8654004954582989
ep  95  training loss:  0.49857753420941614
valid loss  0.5228408884647757  and accuracy  0.8654004954582989
ep  96  training loss:  0.49868211853545014
valid loss  0.5186972261754862  and accuracy  0.8670520231213873
ep  97  training loss:  0.4983612518581981
valid loss  0.5187307137974425  and accuracy  0.8654004954582989
ep  98  training loss:  0.49805517408397104
valid loss  0.5182847159889293  and accuracy  0.8670520231213873
ep  99  training loss:  0.49918427434875223
valid loss  0.517673608884646  and accuracy  0.8662262592898431
ep  100  training loss:  0.49707272696934485
valid loss  0.5180280812016999  and accuracy  0.8662262592898431
ep  101  training loss:  0.4966372538256501
valid loss  0.5176057738031266  and accuracy  0.8654004954582989
ep  102  training loss:  0.4956803679553744
valid loss  0.5164278461163345  and accuracy  0.8670520231213873
ep  103  training loss:  0.4978062655151503
valid loss  0.5204698784010751  and accuracy  0.8662262592898431
ep  104  training loss:  0.4956701734660598
valid loss  0.5189263863715133  and accuracy  0.8662262592898431
ep  105  training loss:  0.49586559342844144
valid loss  0.5173963636627481  and accuracy  0.8662262592898431
ep  106  training loss:  0.4950452918641584
valid loss  0.5159481234239409  and accuracy  0.8662262592898431
ep  107  training loss:  0.49590933459012354
valid loss  0.5166432305764403  and accuracy  0.8662262592898431
ep  108  training loss:  0.4959140451944899
valid loss  0.5142364109674827  and accuracy  0.8662262592898431
ep  109  training loss:  0.4953284283394799
valid loss  0.5161910460666622  and accuracy  0.8662262592898431
ep  110  training loss:  0.4936926965125524
valid loss  0.5137402950536112  and accuracy  0.8670520231213873
ep  111  training loss:  0.49412744487309956
valid loss  0.5135509118277805  and accuracy  0.8654004954582989
ep  112  training loss:  0.49483230079567814
valid loss  0.5133601662346787  and accuracy  0.8662262592898431
ep  113  training loss:  0.4933759201712368
valid loss  0.5155222088123136  and accuracy  0.8654004954582989
ep  114  training loss:  0.49186528634138393
valid loss  0.511443581958136  and accuracy  0.8662262592898431
ep  115  training loss:  0.49327273197536503
valid loss  0.5123795069366521  and accuracy  0.8654004954582989
ep  116  training loss:  0.4910580305577958
valid loss  0.5149026494396314  and accuracy  0.8662262592898431
ep  117  training loss:  0.49200149207864924
valid loss  0.511899413007237  and accuracy  0.8670520231213873
ep  118  training loss:  0.49186759225658366
valid loss  0.5130163008127204  and accuracy  0.8670520231213873
ep  119  training loss:  0.4897703181868995
valid loss  0.515111922405654  and accuracy  0.8662262592898431
ep  120  training loss:  0.4918166061216497
valid loss  0.5147726710033653  and accuracy  0.8654004954582989
ep  121  training loss:  0.491293453738577
valid loss  0.5140280769345782  and accuracy  0.8654004954582989
ep  122  training loss:  0.49158947732432634
valid loss  0.5154455161409669  and accuracy  0.8670520231213873
ep  123  training loss:  0.4898740796995404
valid loss  0.5127729812404718  and accuracy  0.8670520231213873
ep  124  training loss:  0.49150564020754484
valid loss  0.5139482856584324  and accuracy  0.8654004954582989
ep  125  training loss:  0.49151098202228
valid loss  0.5126998085900834  and accuracy  0.8670520231213873
ep  126  training loss:  0.4887688546774924
valid loss  0.5129217606846899  and accuracy  0.8662262592898431
ep  127  training loss:  0.48866139534849307
valid loss  0.5113716898970522  and accuracy  0.8662262592898431
ep  128  training loss:  0.4898434128910619
valid loss  0.510899140803308  and accuracy  0.8678777869529315
ep  129  training loss:  0.4895758958172787
valid loss  0.5131774203133525  and accuracy  0.8662262592898431
ep  130  training loss:  0.4888396073838195
valid loss  0.5124488740199662  and accuracy  0.8662262592898431
ep  131  training loss:  0.48711514511362725
valid loss  0.512687900064407  and accuracy  0.8662262592898431
ep  132  training loss:  0.4889078659356817
valid loss  0.5111918809933667  and accuracy  0.8662262592898431
ep  133  training loss:  0.4876748455268799
valid loss  0.5104163981586523  and accuracy  0.8662262592898431
ep  134  training loss:  0.4874351300242052
valid loss  0.5091991507249072  and accuracy  0.8654004954582989
ep  135  training loss:  0.48820504343951043
valid loss  0.509971682145318  and accuracy  0.8678777869529315
ep  136  training loss:  0.48778421493635116
valid loss  0.5094476846453575  and accuracy  0.8662262592898431
ep  137  training loss:  0.4870239001194623
valid loss  0.5079400554164001  and accuracy  0.8662262592898431
ep  138  training loss:  0.48702319431336044
valid loss  0.5104328362435767  and accuracy  0.8678777869529315
ep  139  training loss:  0.48819940466609363
valid loss  0.5104224168004131  and accuracy  0.8670520231213873
ep  140  training loss:  0.48371489178568555
valid loss  0.5123650243368945  and accuracy  0.8670520231213873
ep  141  training loss:  0.4866246201585273
valid loss  0.5107068412565772  and accuracy  0.8654004954582989
ep  142  training loss:  0.48720460848208647
valid loss  0.5100853695641068  and accuracy  0.8662262592898431
ep  143  training loss:  0.486435734570425
valid loss  0.5147996350175383  and accuracy  0.8645747316267548
ep  144  training loss:  0.4865751209898342
valid loss  0.5119560327931537  and accuracy  0.8670520231213873
ep  145  training loss:  0.48589620173531595
valid loss  0.5119832458818973  and accuracy  0.8662262592898431
ep  146  training loss:  0.48592763313790593
valid loss  0.5092037378708062  and accuracy  0.8670520231213873
ep  147  training loss:  0.48589529903264406
valid loss  0.5149944152938345  and accuracy  0.8670520231213873
ep  148  training loss:  0.48520568250890383
valid loss  0.5112978214079639  and accuracy  0.8670520231213873
ep  149  training loss:  0.4853117189872442
valid loss  0.5127267254275  and accuracy  0.8662262592898431
ep  150  training loss:  0.4834277843344058
valid loss  0.5102395355258472  and accuracy  0.8670520231213873
ep  151  training loss:  0.4844729506625299
valid loss  0.5097793194501484  and accuracy  0.8670520231213873
ep  152  training loss:  0.4861431437883422
valid loss  0.5132647267212856  and accuracy  0.8662262592898431
ep  153  training loss:  0.48250090432797027
valid loss  0.5090600829002386  and accuracy  0.8678777869529315
ep  154  training loss:  0.48483212255494396
valid loss  0.5085712108210431  and accuracy  0.8670520231213873
ep  155  training loss:  0.4859012826448396
valid loss  0.5059674784085851  and accuracy  0.8662262592898431
ep  156  training loss:  0.48464435418693147
valid loss  0.5098007494363777  and accuracy  0.8662262592898431
ep  157  training loss:  0.4848894712256483
valid loss  0.5085881028561233  and accuracy  0.8670520231213873
ep  158  training loss:  0.4834994047787815
valid loss  0.5077971030029751  and accuracy  0.8670520231213873
ep  159  training loss:  0.4815604880299037
valid loss  0.5084149510185153  and accuracy  0.8670520231213873
ep  160  training loss:  0.48406511315259587
valid loss  0.509111604200327  and accuracy  0.8654004954582989
ep  161  training loss:  0.48395955604395496
valid loss  0.5068050031077261  and accuracy  0.8670520231213873
ep  162  training loss:  0.4818578698306114
valid loss  0.5044590460181138  and accuracy  0.8678777869529315
ep  163  training loss:  0.4810023740568318
valid loss  0.5106270269703806  and accuracy  0.8670520231213873
ep  164  training loss:  0.48291862340526426
valid loss  0.5070773193798805  and accuracy  0.8670520231213873
ep  165  training loss:  0.4807304075949928
valid loss  0.5050855717021113  and accuracy  0.8645747316267548
ep  166  training loss:  0.48161560727690156
valid loss  0.5070714809794155  and accuracy  0.8662262592898431
ep  167  training loss:  0.48113382850866143
valid loss  0.5039267490020377  and accuracy  0.8678777869529315
ep  168  training loss:  0.48166321512342686
valid loss  0.5065361275149218  and accuracy  0.8670520231213873
ep  169  training loss:  0.4811891610481245
valid loss  0.5045656957575154  and accuracy  0.8654004954582989
ep  170  training loss:  0.4811745081148535
valid loss  0.507931897947357  and accuracy  0.8678777869529315
ep  171  training loss:  0.4805526133268814
valid loss  0.5067408545562593  and accuracy  0.8662262592898431
ep  172  training loss:  0.48046064297111907
valid loss  0.5106755828286478  and accuracy  0.8670520231213873
ep  173  training loss:  0.4798887555176507
valid loss  0.509589315642019  and accuracy  0.8662262592898431
ep  174  training loss:  0.4797495414957567
valid loss  0.5089184541184483  and accuracy  0.8687035507844756
ep  175  training loss:  0.4807592341939148
valid loss  0.5088992074045832  and accuracy  0.8662262592898431
ep  176  training loss:  0.47931069913413665
valid loss  0.5063986519365444  and accuracy  0.8687035507844756
ep  177  training loss:  0.4805632047543536
valid loss  0.5077936314138669  and accuracy  0.8670520231213873
ep  178  training loss:  0.4786687306470378
valid loss  0.5056294968657805  and accuracy  0.8670520231213873
ep  179  training loss:  0.47935947966875053
valid loss  0.5090968065720565  and accuracy  0.8662262592898431
ep  180  training loss:  0.48099252887348465
valid loss  0.5056717242218068  and accuracy  0.8654004954582989
ep  181  training loss:  0.4798885807382829
valid loss  0.504043678327792  and accuracy  0.8662262592898431
ep  182  training loss:  0.4789609361085703
valid loss  0.504736112376858  and accuracy  0.8670520231213873
ep  183  training loss:  0.4791718733396252
valid loss  0.5065879918247289  and accuracy  0.8687035507844756
ep  184  training loss:  0.4792402620670205
valid loss  0.5028070798843582  and accuracy  0.8670520231213873
ep  185  training loss:  0.47942659437724455
valid loss  0.506851169341463  and accuracy  0.8670520231213873
ep  186  training loss:  0.47810793404205065
valid loss  0.5066332064021432  and accuracy  0.8687035507844756
ep  187  training loss:  0.4778822314688674
valid loss  0.5083608899502395  and accuracy  0.8678777869529315
ep  188  training loss:  0.4783512832406411
valid loss  0.5060624577947926  and accuracy  0.8678777869529315
ep  189  training loss:  0.4791455277123433
valid loss  0.5103669736032935  and accuracy  0.8670520231213873
ep  190  training loss:  0.47763175842637945
valid loss  0.5061922372125772  and accuracy  0.8662262592898431
ep  191  training loss:  0.47754800168983
valid loss  0.5055902807994089  and accuracy  0.8662262592898431
ep  192  training loss:  0.4776824907044411
valid loss  0.5074285268291375  and accuracy  0.8662262592898431
ep  193  training loss:  0.4774499231558448
valid loss  0.5054576483076019  and accuracy  0.8670520231213873
ep  194  training loss:  0.4778627051729262
valid loss  0.5079726548257886  and accuracy  0.8670520231213873
ep  195  training loss:  0.47938578175674
valid loss  0.5045605311208673  and accuracy  0.8662262592898431
ep  196  training loss:  0.477595910748345
valid loss  0.5048237214100253  and accuracy  0.8662262592898431
ep  197  training loss:  0.47787321340083067
valid loss  0.5044915882099964  and accuracy  0.8662262592898431
ep  198  training loss:  0.4782055426608328
valid loss  0.5056185647537648  and accuracy  0.8670520231213873
ep  199  training loss:  0.47742188381266
valid loss  0.5028877203667291  and accuracy  0.8670520231213873
ep  200  training loss:  0.47636752523502507
valid loss  0.5040002834639403  and accuracy  0.8678777869529315
ep  201  training loss:  0.4779290605808545
valid loss  0.5058122459617554  and accuracy  0.8670520231213873
ep  202  training loss:  0.47640852024224273
valid loss  0.5064142659909463  and accuracy  0.8662262592898431
ep  203  training loss:  0.47791930730785015
valid loss  0.5037244243084351  and accuracy  0.8678777869529315
ep  204  training loss:  0.4766999865274331
valid loss  0.5044518272114824  and accuracy  0.8662262592898431
ep  205  training loss:  0.47455707581780937
valid loss  0.5012554294013662  and accuracy  0.8670520231213873
ep  206  training loss:  0.47663763002633
valid loss  0.5019570001386395  and accuracy  0.8654004954582989
ep  207  training loss:  0.4761087525835863
valid loss  0.5023997081989695  and accuracy  0.8670520231213873
ep  208  training loss:  0.47563915277184604
valid loss  0.5024139713788209  and accuracy  0.8678777869529315
ep  209  training loss:  0.4747036613379348
valid loss  0.4986095870456648  and accuracy  0.8695293146160198
ep  210  training loss:  0.4756274091304953
valid loss  0.5020902824244353  and accuracy  0.8670520231213873
ep  211  training loss:  0.4747880922213909
valid loss  0.4984314960206864  and accuracy  0.8687035507844756
ep  212  training loss:  0.47814918849134186
valid loss  0.5011714950363858  and accuracy  0.8687035507844756
ep  213  training loss:  0.47711316708677687
valid loss  0.49969222762681154  and accuracy  0.8687035507844756
ep  214  training loss:  0.47538228138802474
valid loss  0.5022094384094195  and accuracy  0.8662262592898431
ep  215  training loss:  0.47410337614052606
valid loss  0.500442900581856  and accuracy  0.8678777869529315
ep  216  training loss:  0.47594853626089295
valid loss  0.5036093973355487  and accuracy  0.8695293146160198
ep  217  training loss:  0.47638066725228473
valid loss  0.5022478188294994  and accuracy  0.8687035507844756
ep  218  training loss:  0.47624984939672727
valid loss  0.4999604319659862  and accuracy  0.8662262592898431
ep  219  training loss:  0.4745901219900427
valid loss  0.49927183555040744  and accuracy  0.8687035507844756
ep  220  training loss:  0.47489011020824506
valid loss  0.5024885555666208  and accuracy  0.8670520231213873
ep  221  training loss:  0.4736133739777557
valid loss  0.5023508945050857  and accuracy  0.8662262592898431
ep  222  training loss:  0.47241161336149834
valid loss  0.499190452151019  and accuracy  0.8678777869529315
ep  223  training loss:  0.47580868231299417
valid loss  0.5001466890980053  and accuracy  0.8662262592898431
ep  224  training loss:  0.4744657908376872
valid loss  0.5009607688195443  and accuracy  0.8678777869529315
ep  225  training loss:  0.47418000123625653
valid loss  0.49949350335399145  and accuracy  0.8687035507844756
ep  226  training loss:  0.47457851012832986
valid loss  0.5039030669101696  and accuracy  0.8687035507844756
ep  227  training loss:  0.47285434255162884
valid loss  0.5074101774088121  and accuracy  0.8670520231213873
ep  228  training loss:  0.47344396080711776
valid loss  0.5002979380497747  and accuracy  0.8687035507844756
ep  229  training loss:  0.47460606045774295
valid loss  0.5006032161826836  and accuracy  0.8687035507844756
ep  230  training loss:  0.47339195048044913
valid loss  0.49916474441178665  and accuracy  0.8695293146160198
ep  231  training loss:  0.4734740317684464
valid loss  0.49812705875132124  and accuracy  0.8678777869529315
ep  232  training loss:  0.475126727061722
valid loss  0.4981056046476057  and accuracy  0.8678777869529315
ep  233  training loss:  0.471973789397392
valid loss  0.49793424732324804  and accuracy  0.8687035507844756
ep  234  training loss:  0.47409347789047634
valid loss  0.49767291356373383  and accuracy  0.8678777869529315
ep  235  training loss:  0.4725465296367082
valid loss  0.5015397802745872  and accuracy  0.8670520231213873
ep  236  training loss:  0.4718791619587287
valid loss  0.49600019605567297  and accuracy  0.8687035507844756
ep  237  training loss:  0.4731006638209673
valid loss  0.5009458639422888  and accuracy  0.8662262592898431
ep  238  training loss:  0.4740231087158796
valid loss  0.4985235505218254  and accuracy  0.8687035507844756
ep  239  training loss:  0.474121978447811
valid loss  0.5020874505885665  and accuracy  0.8678777869529315
ep  240  training loss:  0.4710749067503444
valid loss  0.5015782182320989  and accuracy  0.8678777869529315
ep  241  training loss:  0.4706568791988409
valid loss  0.5018578153518878  and accuracy  0.8687035507844756
ep  242  training loss:  0.47368073072621686
valid loss  0.5004401411968848  and accuracy  0.8687035507844756
ep  243  training loss:  0.47480113939717083
valid loss  0.4993206927049859  and accuracy  0.8687035507844756
ep  244  training loss:  0.47176804242233644
valid loss  0.5027975516315337  and accuracy  0.8678777869529315
ep  245  training loss:  0.47190962348554405
valid loss  0.4996775221371828  and accuracy  0.8678777869529315
ep  246  training loss:  0.4714343375501304
valid loss  0.5032823659190454  and accuracy  0.8687035507844756
ep  247  training loss:  0.4716476841125339
valid loss  0.49801999935918756  and accuracy  0.8687035507844756
ep  248  training loss:  0.47186242905084624
valid loss  0.49989963089701167  and accuracy  0.870355078447564
ep  249  training loss:  0.47100486380691103
valid loss  0.5029305578213306  and accuracy  0.8678777869529315
ep  250  training loss:  0.4706823620073287
valid loss  0.49804467218755  and accuracy  0.8695293146160198
ep  251  training loss:  0.4718717354785908
valid loss  0.5008745710776917  and accuracy  0.8678777869529315
ep  252  training loss:  0.47200650075893186
valid loss  0.503200397326078  and accuracy  0.8678777869529315
ep  253  training loss:  0.4716707416020955
valid loss  0.5016820956910572  and accuracy  0.8670520231213873
ep  254  training loss:  0.4693347654593864
valid loss  0.5042988379615875  and accuracy  0.8670520231213873
ep  255  training loss:  0.4703333204886474
valid loss  0.5047535663493697  and accuracy  0.8654004954582989
ep  256  training loss:  0.47066195532521576
valid loss  0.5025997446107038  and accuracy  0.8662262592898431
ep  257  training loss:  0.47030490437041605
valid loss  0.5072811914563278  and accuracy  0.8678777869529315
ep  258  training loss:  0.4702824649521469
valid loss  0.5023504596028222  and accuracy  0.8678777869529315
ep  259  training loss:  0.4721515927068212
valid loss  0.5042946374583697  and accuracy  0.8678777869529315
ep  260  training loss:  0.47159266005700223
valid loss  0.4991441600879887  and accuracy  0.8678777869529315
ep  261  training loss:  0.4729856687042418
valid loss  0.503557723283177  and accuracy  0.8670520231213873
ep  262  training loss:  0.471675955754395
valid loss  0.5013698344876409  and accuracy  0.8670520231213873
ep  263  training loss:  0.4699719085854353
valid loss  0.4993774979494506  and accuracy  0.8687035507844756
ep  264  training loss:  0.46950209897623385
valid loss  0.500646409074892  and accuracy  0.8678777869529315
ep  265  training loss:  0.4704810154319412
valid loss  0.49763674223157217  and accuracy  0.8678777869529315
ep  266  training loss:  0.4707072151220558
valid loss  0.49763295905930655  and accuracy  0.8670520231213873
ep  267  training loss:  0.4710543641811132
valid loss  0.4950854988210365  and accuracy  0.8670520231213873
ep  268  training loss:  0.47095989712862296
valid loss  0.49912088203784555  and accuracy  0.8687035507844756
ep  269  training loss:  0.47193652989327317
valid loss  0.4980723176191505  and accuracy  0.8687035507844756
ep  270  training loss:  0.47294234688957487
valid loss  0.49588561781569024  and accuracy  0.8670520231213873
ep  271  training loss:  0.46975675316157145
valid loss  0.49647701224642876  and accuracy  0.8678777869529315
ep  272  training loss:  0.47139641475354727
valid loss  0.4972703256063674  and accuracy  0.8687035507844756
ep  273  training loss:  0.46951087781391726
valid loss  0.4970624418321668  and accuracy  0.8687035507844756
ep  274  training loss:  0.47027762075987906
valid loss  0.4952940752838979  and accuracy  0.8678777869529315
ep  275  training loss:  0.46881089144790566
valid loss  0.49352766973336404  and accuracy  0.8695293146160198
ep  276  training loss:  0.4704795589728566
valid loss  0.4975710272001492  and accuracy  0.8670520231213873
ep  277  training loss:  0.46965125974075106
valid loss  0.4998429196123049  and accuracy  0.8687035507844756
ep  278  training loss:  0.47166113800178866
valid loss  0.49486397311374436  and accuracy  0.8670520231213873
ep  279  training loss:  0.469797178920613
valid loss  0.4999271884720548  and accuracy  0.8662262592898431
ep  280  training loss:  0.46898528887346314
valid loss  0.5010089941452004  and accuracy  0.8662262592898431
ep  281  training loss:  0.47221750470616264
valid loss  0.49946471920789126  and accuracy  0.8695293146160198
ep  282  training loss:  0.4703315166096337
valid loss  0.49789266995808784  and accuracy  0.8670520231213873
ep  283  training loss:  0.4700713846446678
valid loss  0.49373029257323897  and accuracy  0.8678777869529315
ep  284  training loss:  0.4707182930550632
valid loss  0.49973618432965583  and accuracy  0.8695293146160198
ep  285  training loss:  0.47090714701313585
valid loss  0.5015361242851475  and accuracy  0.8670520231213873
ep  286  training loss:  0.46901264405137777
valid loss  0.49742980044209  and accuracy  0.8662262592898431
ep  287  training loss:  0.4695765292483118
valid loss  0.4933887185831881  and accuracy  0.8662262592898431
ep  288  training loss:  0.47025107767216007
valid loss  0.49857212732293016  and accuracy  0.8670520231213873
ep  289  training loss:  0.4704247070523503
valid loss  0.49992336019024586  and accuracy  0.8670520231213873
ep  290  training loss:  0.46723282665301064
valid loss  0.4968693867682623  and accuracy  0.8670520231213873
ep  291  training loss:  0.46892549412768886
valid loss  0.4939750883817476  and accuracy  0.8678777869529315
ep  292  training loss:  0.46566483355564
valid loss  0.4973024877263533  and accuracy  0.8687035507844756
ep  293  training loss:  0.4688979910950621
valid loss  0.4985649915247491  and accuracy  0.8695293146160198
ep  294  training loss:  0.46890528739480597
valid loss  0.501457558866378  and accuracy  0.8670520231213873
ep  295  training loss:  0.4688319017206197
valid loss  0.49884012257826416  and accuracy  0.8687035507844756
ep  296  training loss:  0.46908951220300843
valid loss  0.4964076073872561  and accuracy  0.870355078447564
ep  297  training loss:  0.46878697045415985
valid loss  0.49473857407132815  and accuracy  0.8687035507844756
ep  298  training loss:  0.4691999834428855
valid loss  0.5017006520787908  and accuracy  0.8678777869529315
ep  299  training loss:  0.46778353415836693
valid loss  0.4991689849057343  and accuracy  0.8678777869529315
ep  300  training loss:  0.46704218259664254
valid loss  0.4972925211505197  and accuracy  0.8678777869529315
ep  301  training loss:  0.4676345814219347
valid loss  0.4953984206220909  and accuracy  0.8662262592898431
ep  302  training loss:  0.4690963072297313
valid loss  0.5021628067574159  and accuracy  0.8662262592898431
ep  303  training loss:  0.4683662740709031
valid loss  0.4998949669443016  and accuracy  0.8670520231213873
ep  304  training loss:  0.4666354764131986
valid loss  0.4998986479863955  and accuracy  0.8670520231213873
ep  305  training loss:  0.46735818903085713
valid loss  0.49973268790938064  and accuracy  0.8670520231213873
ep  306  training loss:  0.46649027012666505
valid loss  0.5058940125830798  and accuracy  0.8654004954582989
ep  307  training loss:  0.467363632618749
valid loss  0.5031898539731761  and accuracy  0.8678777869529315
ep  308  training loss:  0.4667784077194499
valid loss  0.4983870695682718  and accuracy  0.8678777869529315
ep  309  training loss:  0.46700880612465284
valid loss  0.4990504765244774  and accuracy  0.8678777869529315
ep  310  training loss:  0.46702151618566157
valid loss  0.499249491782153  and accuracy  0.8670520231213873
ep  311  training loss:  0.469029866731336
valid loss  0.49583816043509615  and accuracy  0.8678777869529315
ep  312  training loss:  0.4669816142834919
valid loss  0.4963053091806779  and accuracy  0.8670520231213873
ep  313  training loss:  0.4676348173614428
valid loss  0.49585959120883516  and accuracy  0.8695293146160198
ep  314  training loss:  0.46676750657337174
valid loss  0.49503596684442486  and accuracy  0.8687035507844756
ep  315  training loss:  0.46778059787373816
valid loss  0.4988189158510512  and accuracy  0.8678777869529315
ep  316  training loss:  0.46792856297744195
valid loss  0.4925415914330376  and accuracy  0.8678777869529315
ep  317  training loss:  0.4673362837746178
valid loss  0.49393669206381435  and accuracy  0.8678777869529315
ep  318  training loss:  0.4660829116121394
valid loss  0.49475258241794406  and accuracy  0.8678777869529315
ep  319  training loss:  0.46491916443410697
valid loss  0.49588749194125514  and accuracy  0.8670520231213873
ep  320  training loss:  0.467234318358652
valid loss  0.498350375723386  and accuracy  0.8678777869529315
ep  321  training loss:  0.4676342369234206
valid loss  0.4948891381357053  and accuracy  0.8670520231213873
ep  322  training loss:  0.46570185311971063
valid loss  0.49444570303160135  and accuracy  0.8670520231213873
ep  323  training loss:  0.46751298104894234
valid loss  0.49025006337563526  and accuracy  0.8687035507844756
ep  324  training loss:  0.4668312069788743
valid loss  0.4953734770380299  and accuracy  0.8670520231213873
ep  325  training loss:  0.468137297194188
valid loss  0.49695238793417995  and accuracy  0.8687035507844756
ep  326  training loss:  0.46667166675717886
valid loss  0.4944650154040925  and accuracy  0.8654004954582989
ep  327  training loss:  0.4686744098629498
valid loss  0.4942378011446763  and accuracy  0.8687035507844756
ep  328  training loss:  0.4664474821872335
valid loss  0.4978280375064847  and accuracy  0.8678777869529315
ep  329  training loss:  0.4658155830692767
valid loss  0.4974622290849095  and accuracy  0.8678777869529315
ep  330  training loss:  0.4653874778996438
valid loss  0.49511455785726144  and accuracy  0.8687035507844756
ep  331  training loss:  0.46512369714073365
valid loss  0.4956805976976746  and accuracy  0.8687035507844756
ep  332  training loss:  0.4652830589519514
valid loss  0.49464571414458464  and accuracy  0.8687035507844756
ep  333  training loss:  0.4675911866593878
valid loss  0.4983467122526035  and accuracy  0.8687035507844756
ep  334  training loss:  0.4670046094560133
valid loss  0.49370813320531615  and accuracy  0.8687035507844756
ep  335  training loss:  0.4640571288418136
valid loss  0.49359389827036837  and accuracy  0.8687035507844756
ep  336  training loss:  0.4662674810646837
valid loss  0.4969746172674228  and accuracy  0.8670520231213873
ep  337  training loss:  0.46737473711184274
valid loss  0.4946379776241167  and accuracy  0.8687035507844756
ep  338  training loss:  0.46638058679388894
valid loss  0.49484110449484614  and accuracy  0.8670520231213873
ep  339  training loss:  0.46693980030787163
valid loss  0.4922108111501626  and accuracy  0.8670520231213873
ep  340  training loss:  0.4669993417460579
valid loss  0.4927040502170607  and accuracy  0.8687035507844756
ep  341  training loss:  0.4663000500303286
valid loss  0.4983210081802135  and accuracy  0.8670520231213873
ep  342  training loss:  0.4661111096936442
valid loss  0.4972323426572278  and accuracy  0.8695293146160198
ep  343  training loss:  0.46506769030367523
valid loss  0.5002799649852844  and accuracy  0.8687035507844756
ep  344  training loss:  0.4663607504439381
valid loss  0.5008628468539083  and accuracy  0.8678777869529315
ep  345  training loss:  0.4675844812243472
valid loss  0.4987489331073745  and accuracy  0.8687035507844756
ep  346  training loss:  0.46710457273533673
valid loss  0.4981985375666599  and accuracy  0.8670520231213873
ep  347  training loss:  0.4654209627021761
valid loss  0.49654917166707535  and accuracy  0.870355078447564
ep  348  training loss:  0.46327134713596163
valid loss  0.4965472811022443  and accuracy  0.870355078447564
ep  349  training loss:  0.4658561017448722
valid loss  0.4948581544748522  and accuracy  0.8695293146160198
ep  350  training loss:  0.4656497855751107
valid loss  0.49577970429947316  and accuracy  0.8678777869529315
ep  351  training loss:  0.4649268040949088
valid loss  0.495137026790348  and accuracy  0.8678777869529315
ep  352  training loss:  0.4655005339307665
valid loss  0.494615321559024  and accuracy  0.8695293146160198
ep  353  training loss:  0.46635453154845913
valid loss  0.4985740059274568  and accuracy  0.8678777869529315
ep  354  training loss:  0.4676106849612701
valid loss  0.49797609460737763  and accuracy  0.8678777869529315
ep  355  training loss:  0.46575405496190686
valid loss  0.4948132818735502  and accuracy  0.8678777869529315
ep  356  training loss:  0.4637235816774118
valid loss  0.49334383886968863  and accuracy  0.8695293146160198
ep  357  training loss:  0.46564103815843727
valid loss  0.49557908111914045  and accuracy  0.870355078447564
ep  358  training loss:  0.4676323870940964
valid loss  0.49942321478582236  and accuracy  0.8695293146160198
ep  359  training loss:  0.46633050022633116
valid loss  0.49099360124617936  and accuracy  0.870355078447564
ep  360  training loss:  0.46458805470691383
valid loss  0.4918861230673818  and accuracy  0.8695293146160198
ep  361  training loss:  0.46594524206121213
valid loss  0.4947060588516547  and accuracy  0.8695293146160198
ep  362  training loss:  0.4654491649981016
valid loss  0.49263425864137766  and accuracy  0.870355078447564
ep  363  training loss:  0.4640473217681072
valid loss  0.49274436281694056  and accuracy  0.8687035507844756
ep  364  training loss:  0.4651149535253024
valid loss  0.49456957661839773  and accuracy  0.8687035507844756
ep  365  training loss:  0.46427602076040475
valid loss  0.4939071775270237  and accuracy  0.8695293146160198
ep  366  training loss:  0.46522507112812905
valid loss  0.4900574046014066  and accuracy  0.870355078447564
ep  367  training loss:  0.4658289827592337
valid loss  0.4958946214393581  and accuracy  0.8662262592898431
ep  368  training loss:  0.4638360554428051
valid loss  0.4977159073882414  and accuracy  0.8687035507844756
ep  369  training loss:  0.46244801808640185
valid loss  0.49631333646451414  and accuracy  0.8695293146160198
ep  370  training loss:  0.4625138597822057
valid loss  0.493812024691792  and accuracy  0.8687035507844756
ep  371  training loss:  0.4660179926508734
valid loss  0.4968259106678573  and accuracy  0.8687035507844756
ep  372  training loss:  0.4651694110499142
valid loss  0.49599868745768394  and accuracy  0.8695293146160198
ep  373  training loss:  0.4641675323787472
valid loss  0.49252735401952097  and accuracy  0.8687035507844756
ep  374  training loss:  0.4643107400528079
valid loss  0.4958042903048848  and accuracy  0.8670520231213873
ep  375  training loss:  0.46518309899749966
valid loss  0.49225694822142874  and accuracy  0.8662262592898431
ep  376  training loss:  0.46446897006116517
valid loss  0.4939454495660733  and accuracy  0.8687035507844756
ep  377  training loss:  0.46416929722454475
valid loss  0.49475513269446486  and accuracy  0.870355078447564
ep  378  training loss:  0.46803108974258306
valid loss  0.4932759947071973  and accuracy  0.8695293146160198
ep  379  training loss:  0.4650935945299317
valid loss  0.49477689547739573  and accuracy  0.870355078447564
ep  380  training loss:  0.46274407331676354
valid loss  0.4960262565992766  and accuracy  0.8687035507844756
ep  381  training loss:  0.4641952458358943
valid loss  0.4950323623630451  and accuracy  0.8678777869529315
ep  382  training loss:  0.4668404686375128
valid loss  0.49454829751411616  and accuracy  0.8678777869529315
ep  383  training loss:  0.46422614927413197
valid loss  0.4941497371277467  and accuracy  0.8695293146160198
ep  384  training loss:  0.46582604796204746
valid loss  0.4931720081781376  and accuracy  0.8687035507844756
ep  385  training loss:  0.4640211730471172
valid loss  0.4929780954668878  and accuracy  0.8695293146160198
ep  386  training loss:  0.4630542102878507
valid loss  0.49107651155315873  and accuracy  0.8695293146160198
ep  387  training loss:  0.46413988737312584
valid loss  0.48894918860727654  and accuracy  0.8687035507844756
ep  388  training loss:  0.46470733144664
valid loss  0.49540225894464923  and accuracy  0.8695293146160198
ep  389  training loss:  0.4626888200054937
valid loss  0.4939878814285398  and accuracy  0.870355078447564
ep  390  training loss:  0.4632836294162528
valid loss  0.5013058574410335  and accuracy  0.8678777869529315
ep  391  training loss:  0.4631257599182919
valid loss  0.49574686942466717  and accuracy  0.8695293146160198
ep  392  training loss:  0.46497651030978027
valid loss  0.4982827624981311  and accuracy  0.8687035507844756
ep  393  training loss:  0.46471553464403276
valid loss  0.49780592228832765  and accuracy  0.8678777869529315
ep  394  training loss:  0.4625425674003016
valid loss  0.49892045809748153  and accuracy  0.8695293146160198
ep  395  training loss:  0.46309554392158686
valid loss  0.5002456606172708  and accuracy  0.8678777869529315
ep  396  training loss:  0.4644646888562039
valid loss  0.4955346926886813  and accuracy  0.8687035507844756
ep  397  training loss:  0.4642651410072512
valid loss  0.4958956251451538  and accuracy  0.8678777869529315
ep  398  training loss:  0.46538909355397523
valid loss  0.4959827158984618  and accuracy  0.8695293146160198
ep  399  training loss:  0.4614620400357841
valid loss  0.4943757578422962  and accuracy  0.870355078447564
ep  400  training loss:  0.4620965557014003
valid loss  0.498295706033116  and accuracy  0.8678777869529315
ep  401  training loss:  0.4653072650388644
valid loss  0.5012433837997726  and accuracy  0.8670520231213873
ep  402  training loss:  0.46195999027523477
valid loss  0.49116441920018217  and accuracy  0.8687035507844756
ep  403  training loss:  0.4621150077915643
valid loss  0.49061382880002186  and accuracy  0.8687035507844756
ep  404  training loss:  0.4638885211796808
valid loss  0.49088445150783294  and accuracy  0.8695293146160198
ep  405  training loss:  0.46145577157346757
valid loss  0.49561778638699544  and accuracy  0.8670520231213873
ep  406  training loss:  0.4616722168482608
valid loss  0.49384562888980205  and accuracy  0.8678777869529315
ep  407  training loss:  0.46302276002487724
valid loss  0.4931695927380531  and accuracy  0.8670520231213873
ep  408  training loss:  0.46095462850442503
valid loss  0.49329085038970266  and accuracy  0.8687035507844756
ep  409  training loss:  0.4632651310452356
valid loss  0.4948579974650943  and accuracy  0.8670520231213873
ep  410  training loss:  0.46311095468256375
valid loss  0.49534352179109115  and accuracy  0.8687035507844756
ep  411  training loss:  0.46226747512156297
valid loss  0.49605925886912544  and accuracy  0.8687035507844756
ep  412  training loss:  0.4646694796973157
valid loss  0.49260486929402875  and accuracy  0.8678777869529315
ep  413  training loss:  0.4625087664384395
valid loss  0.4912532813582511  and accuracy  0.8695293146160198
ep  414  training loss:  0.4624383816554173
valid loss  0.49446813170325155  and accuracy  0.8695293146160198
ep  415  training loss:  0.4607709754623571
valid loss  0.4952089147908347  and accuracy  0.870355078447564
ep  416  training loss:  0.46245136998485653
valid loss  0.4922567885784351  and accuracy  0.8695293146160198
ep  417  training loss:  0.46259645222062057
valid loss  0.49207353774030577  and accuracy  0.8687035507844756
ep  418  training loss:  0.46205477428424613
valid loss  0.49398337940752357  and accuracy  0.8695293146160198
ep  419  training loss:  0.46124723518658883
valid loss  0.4888502541747987  and accuracy  0.870355078447564
ep  420  training loss:  0.4616976071096248
valid loss  0.49331334368637236  and accuracy  0.8695293146160198
ep  421  training loss:  0.4614829251834647
valid loss  0.49393395111149346  and accuracy  0.8695293146160198
ep  422  training loss:  0.4623143321789997
valid loss  0.49160727304825597  and accuracy  0.870355078447564
ep  423  training loss:  0.4620297353315501
valid loss  0.49280254891054576  and accuracy  0.8695293146160198
ep  424  training loss:  0.46098253271879563
valid loss  0.49078936750587604  and accuracy  0.870355078447564
ep  425  training loss:  0.4605340119035068
valid loss  0.4913549819925814  and accuracy  0.870355078447564
ep  426  training loss:  0.4626347900428579
valid loss  0.49619471960378814  and accuracy  0.870355078447564
ep  427  training loss:  0.46227579211317643
valid loss  0.49481324564810136  and accuracy  0.8687035507844756
ep  428  training loss:  0.4594323960734588
valid loss  0.49827992008009986  and accuracy  0.8695293146160198
ep  429  training loss:  0.46221780533445267
valid loss  0.4939685681454905  and accuracy  0.8687035507844756
ep  430  training loss:  0.46027488118185533
valid loss  0.49778422636989716  and accuracy  0.8662262592898431
ep  431  training loss:  0.4609225010467458
valid loss  0.4965359826751429  and accuracy  0.8662262592898431
ep  432  training loss:  0.461316254803532
valid loss  0.49454706213279365  and accuracy  0.8695293146160198
ep  433  training loss:  0.46015060168167365
valid loss  0.4948428690088967  and accuracy  0.8670520231213873
ep  434  training loss:  0.4631304759134225
valid loss  0.49024801461978157  and accuracy  0.8678777869529315
ep  435  training loss:  0.46298577836628857
valid loss  0.4900622588361619  and accuracy  0.8695293146160198
ep  436  training loss:  0.46131639761260546
valid loss  0.49098619826070344  and accuracy  0.8695293146160198
ep  437  training loss:  0.460592286745422
valid loss  0.4925666990406153  and accuracy  0.8687035507844756
ep  438  training loss:  0.46205513900420225
valid loss  0.4944309659677532  and accuracy  0.8678777869529315
ep  439  training loss:  0.4610909649519251
valid loss  0.49379558882173874  and accuracy  0.870355078447564
ep  440  training loss:  0.4621583818903251
valid loss  0.4934830789128972  and accuracy  0.8687035507844756
ep  441  training loss:  0.4604195051394674
valid loss  0.4908330606390483  and accuracy  0.8678777869529315
ep  442  training loss:  0.4630390854561199
valid loss  0.49178226317973495  and accuracy  0.8695293146160198
ep  443  training loss:  0.46117996139464923
valid loss  0.49203640852557246  and accuracy  0.870355078447564
ep  444  training loss:  0.4627051219147272
valid loss  0.49217555697155235  and accuracy  0.8695293146160198
ep  445  training loss:  0.46101849724371075
valid loss  0.49393959022277056  and accuracy  0.8695293146160198
ep  446  training loss:  0.46016720676670997
valid loss  0.49039875604612193  and accuracy  0.8678777869529315
ep  447  training loss:  0.4611565300660389
valid loss  0.4911881926924015  and accuracy  0.8720066061106524
ep  448  training loss:  0.4615468682122452
valid loss  0.4907873081924697  and accuracy  0.8687035507844756
ep  449  training loss:  0.46017365335270977
valid loss  0.49165332430556824  and accuracy  0.8695293146160198
ep  450  training loss:  0.46058818550157304
valid loss  0.4920259680657422  and accuracy  0.8670520231213873
ep  451  training loss:  0.46108667145854904
valid loss  0.4908009328495659  and accuracy  0.8687035507844756
ep  452  training loss:  0.4609698421014812
valid loss  0.49302704471088854  and accuracy  0.8711808422791082
ep  453  training loss:  0.4614431624689804
valid loss  0.49285020191348555  and accuracy  0.870355078447564
ep  454  training loss:  0.4616682583745979
valid loss  0.4945301382527875  and accuracy  0.8670520231213873
ep  455  training loss:  0.4611180848368345
valid loss  0.4939989983100324  and accuracy  0.870355078447564
ep  456  training loss:  0.4630032653881313
valid loss  0.49135828414404326  and accuracy  0.8711808422791082
ep  457  training loss:  0.46297888245595803
valid loss  0.4911102761324529  and accuracy  0.8695293146160198
ep  458  training loss:  0.46309178158132897
valid loss  0.4898041911257091  and accuracy  0.870355078447564
ep  459  training loss:  0.45938944451534
valid loss  0.4925144951664447  and accuracy  0.870355078447564
ep  460  training loss:  0.46082423713171855
valid loss  0.48980461345242626  and accuracy  0.870355078447564
ep  461  training loss:  0.4603080057705077
valid loss  0.4912683067244995  and accuracy  0.870355078447564
ep  462  training loss:  0.4618446645914021
valid loss  0.4922590267549951  and accuracy  0.8720066061106524
ep  463  training loss:  0.45969122754693365
valid loss  0.4911930283468484  and accuracy  0.8695293146160198
ep  464  training loss:  0.45910421062441326
valid loss  0.4939401577465993  and accuracy  0.8695293146160198
ep  465  training loss:  0.4619181925448211
valid loss  0.48881840225982826  and accuracy  0.8695293146160198
ep  466  training loss:  0.4605275778586898
valid loss  0.4898385866971965  and accuracy  0.8687035507844756
ep  467  training loss:  0.45938568786236267
valid loss  0.49313359625766734  and accuracy  0.870355078447564
ep  468  training loss:  0.4614976522992902
valid loss  0.49477053212784816  and accuracy  0.8711808422791082
ep  469  training loss:  0.45860299381029934
valid loss  0.49241525510635187  and accuracy  0.8670520231213873
ep  470  training loss:  0.46033197027438033
valid loss  0.49295644837406233  and accuracy  0.8695293146160198
ep  471  training loss:  0.460968713584605
valid loss  0.49139350677304383  and accuracy  0.8678777869529315
ep  472  training loss:  0.46188351597507515
valid loss  0.4919489961237478  and accuracy  0.8695293146160198
ep  473  training loss:  0.4606273117121428
valid loss  0.493888643288002  and accuracy  0.870355078447564
ep  474  training loss:  0.4604759508887583
valid loss  0.49112733177268536  and accuracy  0.8695293146160198
ep  475  training loss:  0.4601323513441423
valid loss  0.49130901947777494  and accuracy  0.870355078447564
ep  476  training loss:  0.460583162237742
valid loss  0.4903605966455773  and accuracy  0.8695293146160198
ep  477  training loss:  0.4584615455718282
valid loss  0.4856446001718696  and accuracy  0.8687035507844756
ep  478  training loss:  0.4597434714020922
valid loss  0.4861033742925926  and accuracy  0.870355078447564
ep  479  training loss:  0.4583321640754364
valid loss  0.48837421183542806  and accuracy  0.870355078447564
ep  480  training loss:  0.4614818360497912
valid loss  0.48868009809813745  and accuracy  0.8687035507844756
ep  481  training loss:  0.45866194473376887
valid loss  0.4905440879202795  and accuracy  0.8711808422791082
ep  482  training loss:  0.4617419012062236
valid loss  0.4922806896718448  and accuracy  0.8720066061106524
ep  483  training loss:  0.4588110182398471
valid loss  0.49101396531334207  and accuracy  0.8720066061106524
ep  484  training loss:  0.4611449726661137
valid loss  0.4899559710067132  and accuracy  0.8711808422791082
ep  485  training loss:  0.459388031707349
valid loss  0.4868462330590192  and accuracy  0.8687035507844756
ep  486  training loss:  0.45914330339182885
valid loss  0.49230296335173085  and accuracy  0.8695293146160198
ep  487  training loss:  0.45813415099990795
valid loss  0.4925380423726571  and accuracy  0.8687035507844756
ep  488  training loss:  0.4589714580921107
valid loss  0.491334817805239  and accuracy  0.8695293146160198
ep  489  training loss:  0.4605604962907128
valid loss  0.4973255370787409  and accuracy  0.8711808422791082
ep  490  training loss:  0.4605735928388934
valid loss  0.4915218541289439  and accuracy  0.870355078447564
ep  491  training loss:  0.459193918429275
valid loss  0.49205973631187866  and accuracy  0.8687035507844756
ep  492  training loss:  0.4589063718956303
valid loss  0.4945524817257258  and accuracy  0.870355078447564
ep  493  training loss:  0.45894443584857303
valid loss  0.48950589761371754  and accuracy  0.8678777869529315
ep  494  training loss:  0.4588484028787213
valid loss  0.4934734238217824  and accuracy  0.8695293146160198
ep  495  training loss:  0.4583292560424954
valid loss  0.49277849319451705  and accuracy  0.870355078447564
ep  496  training loss:  0.45948142604636555
valid loss  0.49567079135546693  and accuracy  0.870355078447564
ep  497  training loss:  0.4587236059089213
valid loss  0.49240638405502973  and accuracy  0.8720066061106524
ep  498  training loss:  0.46028316336925607
valid loss  0.4906845207208449  and accuracy  0.8720066061106524
ep  499  training loss:  0.45876610255264727
valid loss  0.48787153114177884  and accuracy  0.8711808422791082

True positive:  [  133   123   138    27 43585]
True negative:  [48404 48027 48694 49097   570]
False positive:  [  65   34   91   27 6039]
False negative:  [1660 2078 1339 1111   68]

[[  133     1    54     3  1602]
 [   13   123    12     5  2048]
 [   28     4   138     3  1304]
 [    1    18     7    27  1085]
 [   23    11    18    16 43585]]

              precision    recall  f1-score   support

           0       0.67      0.07      0.13      1793
           1       0.78      0.06      0.10      2201
           2       0.60      0.09      0.16      1477
           3       0.50      0.02      0.05      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.88     50262
   macro avg       0.69      0.25      0.28     50262
weighted avg       0.85      0.88      0.83     50262

Accuracy:  0.8755322112132427
Precision_weighted:  0.850114416500213
Recall_weighted:  0.8755322112132427
mcc:  0.2260333597912094
f2:  0.8703277793393858

- train_loop(model, epochs=1000, lr=0.003, wd=0.000001)

ep  0  training loss:  0.8857001388980055
valid loss  0.5802730950580888  and accuracy  0.8624550179928029
ep  1  training loss:  0.5724331490198439
valid loss  0.5704900785142638  and accuracy  0.8628548580567773
ep  2  training loss:  0.5605757577973717
valid loss  0.57039645958881  and accuracy  0.8624550179928029
ep  3  training loss:  0.5541447566239407
valid loss  0.563656478274207  and accuracy  0.8628548580567773
ep  4  training loss:  0.5488680899230582
valid loss  0.5562368221446925  and accuracy  0.8628548580567773
ep  5  training loss:  0.5431334210668228
valid loss  0.5528530309982941  and accuracy  0.8628548580567773
ep  6  training loss:  0.5389093542833854
valid loss  0.5509193132563335  and accuracy  0.8628548580567773
ep  7  training loss:  0.5363358605087211
valid loss  0.5480193094652398  and accuracy  0.8628548580567773
ep  8  training loss:  0.5345248978607938
valid loss  0.5520680931175961  and accuracy  0.8632546981207517
ep  9  training loss:  0.5322654461188204
valid loss  0.5487083532437473  and accuracy  0.8632546981207517
ep  10  training loss:  0.5272240873467229
valid loss  0.5442299544692087  and accuracy  0.8628548580567773
ep  11  training loss:  0.5271974846597762
valid loss  0.5468602254837811  and accuracy  0.8624550179928029
ep  12  training loss:  0.5265178762217493
valid loss  0.5414446299789143  and accuracy  0.8632546981207517
ep  13  training loss:  0.5235888619579468
valid loss  0.5461977788659392  and accuracy  0.8612554978008796
ep  14  training loss:  0.523667517723892
valid loss  0.5445493764040328  and accuracy  0.8640543782487006
ep  15  training loss:  0.5219658270788391
valid loss  0.5459941206384497  and accuracy  0.8632546981207517
ep  16  training loss:  0.5208173435014608
valid loss  0.543107904073764  and accuracy  0.8608556577369052
ep  17  training loss:  0.5204765542519793
valid loss  0.5443134501141483  and accuracy  0.8640543782487006
ep  18  training loss:  0.518665399379759
valid loss  0.5399614709799216  and accuracy  0.8636545381847262
ep  19  training loss:  0.5180894562564976
valid loss  0.539105652118387  and accuracy  0.8636545381847262
ep  20  training loss:  0.5158791325677804
valid loss  0.5413404666915125  and accuracy  0.8632546981207517
ep  21  training loss:  0.5148895121688065
valid loss  0.5339864646134878  and accuracy  0.8628548580567773
ep  22  training loss:  0.5159900144789056
valid loss  0.5315336546913141  and accuracy  0.8644542183126749
ep  23  training loss:  0.5149668684741192
valid loss  0.536897628224406  and accuracy  0.8636545381847262
ep  24  training loss:  0.5133614158519544
valid loss  0.5408611634358174  and accuracy  0.8636545381847262
ep  25  training loss:  0.512938697509637
valid loss  0.5379952660373382  and accuracy  0.8636545381847262
ep  26  training loss:  0.5109859192719326
valid loss  0.5383956383724586  and accuracy  0.8644542183126749
ep  27  training loss:  0.5105837603785334
valid loss  0.5338140492771016  and accuracy  0.8640543782487006
ep  28  training loss:  0.5090782167167289
valid loss  0.5327784916440376  and accuracy  0.8652538984406237
ep  29  training loss:  0.5086021292468722
valid loss  0.5356778840644986  and accuracy  0.8656537385045981
ep  30  training loss:  0.5078842571429798
valid loss  0.5311450990187269  and accuracy  0.8632546981207517
ep  31  training loss:  0.5079308321497549
valid loss  0.5277406994889422  and accuracy  0.8644542183126749
ep  32  training loss:  0.5066310848351664
valid loss  0.5304580562618054  and accuracy  0.8640543782487006
ep  33  training loss:  0.5064525836586099
valid loss  0.5300596362349987  and accuracy  0.8628548580567773
ep  34  training loss:  0.5063219194266658
valid loss  0.5276760530681526  and accuracy  0.8652538984406237
ep  35  training loss:  0.5068302101174154
valid loss  0.5274636503316459  and accuracy  0.8648540583766493
ep  36  training loss:  0.50481135172033
valid loss  0.5279612482094564  and accuracy  0.8644542183126749
ep  37  training loss:  0.5046770643988737
valid loss  0.5267404477294089  and accuracy  0.8644542183126749
ep  38  training loss:  0.5032989987326985
valid loss  0.53180086669899  and accuracy  0.8632546981207517
ep  39  training loss:  0.5050350402791255
valid loss  0.5291809897954728  and accuracy  0.8636545381847262
ep  40  training loss:  0.5030614299197868
valid loss  0.5263061060137102  and accuracy  0.8648540583766493
ep  41  training loss:  0.5019669519097189
valid loss  0.5262039974897873  and accuracy  0.8656537385045981
ep  42  training loss:  0.501430170967569
valid loss  0.5284233378892134  and accuracy  0.8652538984406237
ep  43  training loss:  0.5011662060234274
valid loss  0.5240679945482439  and accuracy  0.8656537385045981
ep  44  training loss:  0.5014232580680944
valid loss  0.5237385483323836  and accuracy  0.8676529388244703
ep  45  training loss:  0.501734123462019
valid loss  0.5217399241232196  and accuracy  0.868452618952419
ep  46  training loss:  0.5003336963716367
valid loss  0.5277875893976821  and accuracy  0.8668532586965214
ep  47  training loss:  0.502174699206394
valid loss  0.524661619274295  and accuracy  0.8640543782487006
ep  48  training loss:  0.4993910768572332
valid loss  0.5216484916586726  and accuracy  0.8680527788884447
ep  49  training loss:  0.4998529153948199
valid loss  0.5261987148762131  and accuracy  0.8648540583766493
ep  50  training loss:  0.5004342918886932
valid loss  0.5219392994793355  and accuracy  0.8672530987604958
ep  51  training loss:  0.49850004235475753
valid loss  0.5146948854025246  and accuracy  0.8680527788884447
ep  52  training loss:  0.4992415743188356
valid loss  0.5144106015497473  and accuracy  0.8692522990803678
ep  53  training loss:  0.4960837630631711
valid loss  0.5202725870711286  and accuracy  0.8700519792083167
ep  54  training loss:  0.49410302019148217
valid loss  0.520412082030553  and accuracy  0.8696521391443423
ep  55  training loss:  0.4951899785148603
valid loss  0.5169436645860531  and accuracy  0.868452618952419
ep  56  training loss:  0.49532330861752183
valid loss  0.5137530694194719  and accuracy  0.868452618952419
ep  57  training loss:  0.49505252315998177
valid loss  0.516729542788197  and accuracy  0.8680527788884447
ep  58  training loss:  0.49625283375103485
valid loss  0.5151703957317829  and accuracy  0.8692522990803678
ep  59  training loss:  0.49358585597811905
valid loss  0.5196867330652959  and accuracy  0.8668532586965214
ep  60  training loss:  0.49431041848060947
valid loss  0.5173911313827588  and accuracy  0.8696521391443423
ep  61  training loss:  0.49354349715939627
valid loss  0.5184989184772716  and accuracy  0.8688524590163934
ep  62  training loss:  0.4934009243043265
valid loss  0.511666509734302  and accuracy  0.868452618952419
ep  63  training loss:  0.4929010186010062
valid loss  0.5226889890177352  and accuracy  0.8676529388244703
ep  64  training loss:  0.49266774688135856
valid loss  0.5137469712279883  and accuracy  0.868452618952419
ep  65  training loss:  0.49286511561754653
valid loss  0.5119208921103037  and accuracy  0.8680527788884447
ep  66  training loss:  0.49080772779860427
valid loss  0.5118243195256534  and accuracy  0.8716513394642144
ep  67  training loss:  0.4940228012794376
valid loss  0.5139606268655678  and accuracy  0.8680527788884447
ep  68  training loss:  0.49021517463008774
valid loss  0.5170208005035748  and accuracy  0.8680527788884447
ep  69  training loss:  0.4913818708161642
valid loss  0.5131174381591472  and accuracy  0.8708516593362655
ep  70  training loss:  0.49170497453209244
valid loss  0.5193947997392535  and accuracy  0.868452618952419
ep  71  training loss:  0.48959261455789094
valid loss  0.5216243474638876  and accuracy  0.8676529388244703
ep  72  training loss:  0.49330318051345223
valid loss  0.5151514149412828  and accuracy  0.868452618952419
ep  73  training loss:  0.49033909920267466
valid loss  0.5140518278944068  and accuracy  0.8688524590163934
ep  74  training loss:  0.49024254830844377
valid loss  0.5136357589465815  and accuracy  0.8704518192722911
ep  75  training loss:  0.48932230782706543
valid loss  0.5145190161545246  and accuracy  0.8696521391443423
ep  76  training loss:  0.4896546291732884
valid loss  0.5122784787013692  and accuracy  0.8692522990803678
ep  77  training loss:  0.4887552109651367
valid loss  0.5140117962257427  and accuracy  0.8700519792083167
ep  78  training loss:  0.49123070033492494
valid loss  0.5106735284902343  and accuracy  0.8704518192722911
ep  79  training loss:  0.4883812254230712
valid loss  0.5171497707746354  and accuracy  0.8708516593362655
ep  80  training loss:  0.48957508413020556
valid loss  0.5184995512969968  and accuracy  0.8668532586965214
ep  81  training loss:  0.48784165644926253
valid loss  0.5132615643947042  and accuracy  0.8704518192722911
ep  82  training loss:  0.4887652690723427
valid loss  0.5126711352497804  and accuracy  0.8704518192722911
ep  83  training loss:  0.4875043543248556
valid loss  0.5105731440562813  and accuracy  0.8708516593362655
ep  84  training loss:  0.48983826644481826
valid loss  0.5156818151426334  and accuracy  0.8676529388244703
ep  85  training loss:  0.4869022444105861
valid loss  0.5084975515780855  and accuracy  0.8716513394642144
ep  86  training loss:  0.4891163216780326
valid loss  0.5109581605809443  and accuracy  0.8692522990803678
ep  87  training loss:  0.4864345774865139
valid loss  0.5064340054345389  and accuracy  0.8708516593362655
ep  88  training loss:  0.4878268684455361
valid loss  0.5094483141921035  and accuracy  0.8700519792083167
ep  89  training loss:  0.4858846469157914
valid loss  0.6685028669000959  and accuracy  0.8696521391443423
ep  90  training loss:  0.48547237668935417
valid loss  0.5080857828634827  and accuracy  0.8716513394642144
ep  91  training loss:  0.48421227830634195
valid loss  0.506852473522939  and accuracy  0.8708516593362655
ep  92  training loss:  0.4848996309288243
valid loss  0.5081406667059968  and accuracy  0.8692522990803678
ep  93  training loss:  0.48388041560936124
valid loss  0.5069476014039651  and accuracy  0.8720511795281887
ep  94  training loss:  0.4845144395600302
valid loss  0.5077014433675078  and accuracy  0.8704518192722911
ep  95  training loss:  0.48523649782375444
valid loss  0.5041416936853036  and accuracy  0.8696521391443423
ep  96  training loss:  0.48258755904375455
valid loss  0.5035001503758696  and accuracy  0.8712514994002399
ep  97  training loss:  0.482779230685174
valid loss  0.5091825523218219  and accuracy  0.8720511795281887
ep  98  training loss:  0.48546286604927535
valid loss  0.5030437997702073  and accuracy  0.8720511795281887
ep  99  training loss:  0.4836851045151712
valid loss  0.5100074536273214  and accuracy  0.8696521391443423
ep  100  training loss:  0.4823596021743559
valid loss  1.0669408205841504  and accuracy  0.8692522990803678
ep  101  training loss:  0.483389445353435
valid loss  0.502863146743027  and accuracy  0.8708516593362655
ep  102  training loss:  0.4819359686926904
valid loss  0.5067783531690778  and accuracy  0.8708516593362655
ep  103  training loss:  0.4822891227184286
valid loss  0.5154850582559792  and accuracy  0.866453418632547
ep  104  training loss:  0.4892810847253135
valid loss  0.5165642309384267  and accuracy  0.8688524590163934
ep  105  training loss:  0.48466408120921384
valid loss  0.5074329750864852  and accuracy  0.8704518192722911
ep  106  training loss:  0.48708142638530977
valid loss  0.9277612534059329  and accuracy  0.868452618952419
ep  107  training loss:  0.48335004625390726
valid loss  0.5088569069804787  and accuracy  0.8696521391443423
ep  108  training loss:  0.4858375147557288
valid loss  0.5070901194461486  and accuracy  0.8696521391443423
ep  109  training loss:  0.48409696851445033
valid loss  0.5057049543082547  and accuracy  0.868452618952419
ep  110  training loss:  0.4834462470073557
valid loss  0.5033335165708649  and accuracy  0.8704518192722911
ep  111  training loss:  0.481799310756801
valid loss  0.5092265970322762  and accuracy  0.8696521391443423
ep  112  training loss:  0.479258468779214
valid loss  0.50371941604742  and accuracy  0.8696521391443423
ep  113  training loss:  0.4827974031651368
valid loss  0.7000673570093371  and accuracy  0.8704518192722911
ep  114  training loss:  0.4825497238628728
valid loss  0.5054055573295851  and accuracy  0.8700519792083167
ep  115  training loss:  0.4816010571878017
valid loss  0.5066964472355436  and accuracy  0.8700519792083167
ep  116  training loss:  0.4806811878981615
valid loss  0.510619204612028  and accuracy  0.8692522990803678
ep  117  training loss:  0.4789890283463244
valid loss  0.5031842691857354  and accuracy  0.8708516593362655
ep  118  training loss:  0.48250003998051627
valid loss  0.5049968158326498  and accuracy  0.8708516593362655
ep  119  training loss:  0.480160284262765
valid loss  0.5048693096408936  and accuracy  0.8700519792083167
ep  120  training loss:  0.4787977775730047
valid loss  0.5113607069960788  and accuracy  0.8680527788884447
ep  121  training loss:  0.4806883745673933
valid loss  0.5063424597306997  and accuracy  0.8712514994002399
ep  122  training loss:  0.48109892732314596
valid loss  0.5058324293869108  and accuracy  0.8696521391443423
ep  123  training loss:  0.4801579717715345
valid loss  0.5036745991577201  and accuracy  0.8708516593362655
ep  124  training loss:  0.4787405121765263
valid loss  0.5026289423624547  and accuracy  0.8712514994002399
ep  125  training loss:  0.4791972730019378
valid loss  0.5038721528900761  and accuracy  0.8704518192722911
ep  126  training loss:  0.4809404648480051
valid loss  0.5047250409476141  and accuracy  0.8708516593362655
ep  127  training loss:  0.48002404011101113
valid loss  0.5087384843244785  and accuracy  0.8696521391443423
ep  128  training loss:  0.47932776784900843
valid loss  0.5020026612501056  and accuracy  0.8716513394642144
ep  129  training loss:  0.4778698504937095
valid loss  0.5091872476711601  and accuracy  0.8688524590163934
ep  130  training loss:  0.476740173125288
valid loss  0.5065259766049597  and accuracy  0.8712514994002399
ep  131  training loss:  0.47852197230933047
valid loss  0.5035327966930103  and accuracy  0.8732506997201119
ep  132  training loss:  0.47710344310062225
valid loss  0.5038784645834431  and accuracy  0.8712514994002399
ep  133  training loss:  0.4778488379681179
valid loss  0.5014956767966108  and accuracy  0.8732506997201119
ep  134  training loss:  0.4798132604171126
valid loss  0.5045494001801135  and accuracy  0.8720511795281887
ep  135  training loss:  0.47838984009927926
valid loss  0.5033602426405765  and accuracy  0.8700519792083167
ep  136  training loss:  0.47833777354502327
valid loss  0.5032819125805793  and accuracy  0.8708516593362655
ep  137  training loss:  0.47697027630854694
valid loss  0.5050017087162136  and accuracy  0.8696521391443423
ep  138  training loss:  0.47797028727055973
valid loss  0.502458684054054  and accuracy  0.8716513394642144
ep  139  training loss:  0.477225004092818
valid loss  0.5040864350556469  and accuracy  0.8716513394642144
ep  140  training loss:  0.4791714319871685
valid loss  0.5051100398959374  and accuracy  0.8708516593362655
ep  141  training loss:  0.47766651926370535
valid loss  0.501462509361852  and accuracy  0.8716513394642144
ep  142  training loss:  0.4754081092101854
valid loss  0.5015547182597145  and accuracy  0.8720511795281887
ep  143  training loss:  0.47676416863258514
valid loss  0.4977838484490695  and accuracy  0.8716513394642144
ep  144  training loss:  0.4752283219509056
valid loss  0.5038540446009935  and accuracy  0.8708516593362655
ep  145  training loss:  0.4785904018087869
valid loss  0.5060249594677357  and accuracy  0.8696521391443423
ep  146  training loss:  0.4762564399238507
valid loss  0.506064018729876  and accuracy  0.8716513394642144
ep  147  training loss:  0.4766761537467651
valid loss  0.5040723785477988  and accuracy  0.8700519792083167
ep  148  training loss:  0.4749628856476739
valid loss  0.5017162917233238  and accuracy  0.8704518192722911
ep  149  training loss:  0.47537418512514457
valid loss  0.5068406789648872  and accuracy  0.8708516593362655
ep  150  training loss:  0.47748838079585226
valid loss  0.5029007640899252  and accuracy  0.8708516593362655
ep  151  training loss:  0.4752121146132915
valid loss  0.5025909472970379  and accuracy  0.8708516593362655
ep  152  training loss:  0.47653334672898623
valid loss  0.5060604643888447  and accuracy  0.8708516593362655
ep  153  training loss:  0.4756276499232114
valid loss  0.5062973965935019  and accuracy  0.8692522990803678
ep  154  training loss:  0.47661492883986856
valid loss  0.5065155398459589  and accuracy  0.8716513394642144
ep  155  training loss:  0.47530415131371895
valid loss  0.5038211436950412  and accuracy  0.8712514994002399
ep  156  training loss:  0.4779528703884871
valid loss  0.5151835228623699  and accuracy  0.8704518192722911
ep  157  training loss:  0.47320970406538876
valid loss  0.5070671612074356  and accuracy  0.8724510195921631
ep  158  training loss:  0.47388079273720185
valid loss  0.5165691788269967  and accuracy  0.8696521391443423
ep  159  training loss:  0.4753578701252524
valid loss  0.5016235226347464  and accuracy  0.8704518192722911
ep  160  training loss:  0.47438368033641914
valid loss  0.5048824000101192  and accuracy  0.8712514994002399
ep  161  training loss:  0.47392771351781243
valid loss  0.5024410937128903  and accuracy  0.8712514994002399
ep  162  training loss:  0.47498697511110133
valid loss  0.5018156050491791  and accuracy  0.8716513394642144
ep  163  training loss:  0.47492118126942967
valid loss  0.5036412761455439  and accuracy  0.8724510195921631
ep  164  training loss:  0.47533197322878845
valid loss  0.5041035153493076  and accuracy  0.8712514994002399
ep  165  training loss:  0.47508936926988987
valid loss  0.5038527800816625  and accuracy  0.8708516593362655
ep  166  training loss:  0.4716226526481172
valid loss  0.5012955917496054  and accuracy  0.8720511795281887
ep  167  training loss:  0.4730496085617649
valid loss  0.503368886267362  and accuracy  0.8712514994002399
ep  168  training loss:  0.47303628763504935
valid loss  0.5062148563983868  and accuracy  0.8700519792083167
ep  169  training loss:  0.4755295130833993
valid loss  0.5035158582374316  and accuracy  0.8708516593362655
ep  170  training loss:  0.4740628085085393
valid loss  0.5005152427115854  and accuracy  0.8724510195921631
ep  171  training loss:  0.47383164924175236
valid loss  0.5030786224195167  and accuracy  0.8708516593362655
ep  172  training loss:  0.47303759306887644
valid loss  0.4985999664465269  and accuracy  0.8720511795281887
ep  173  training loss:  0.4743053520353322
valid loss  0.5012160835862874  and accuracy  0.8712514994002399
ep  174  training loss:  0.47423001815188526
valid loss  0.49885529091433495  and accuracy  0.8716513394642144
ep  175  training loss:  0.47262048985402066
valid loss  0.5031345945889832  and accuracy  0.8700519792083167
ep  176  training loss:  0.47259365675851644
valid loss  0.49863919310691784  and accuracy  0.8748500599760096
ep  177  training loss:  0.4736121304395925
valid loss  0.5029385779581752  and accuracy  0.8708516593362655
ep  178  training loss:  0.471832970904617
valid loss  0.5040473371255593  and accuracy  0.8700519792083167
ep  179  training loss:  0.474429704916927
valid loss  0.4998121969655055  and accuracy  0.8728508596561375
ep  180  training loss:  0.47233069237065606
valid loss  0.5015719417379838  and accuracy  0.8708516593362655
ep  181  training loss:  0.4704265313956939
valid loss  0.5008181952943996  and accuracy  0.8712514994002399
ep  182  training loss:  0.4719752705253455
valid loss  0.49987718496309286  and accuracy  0.8712514994002399
ep  183  training loss:  0.4728279259954297
valid loss  0.5031254486721165  and accuracy  0.8708516593362655
ep  184  training loss:  0.4724993903017198
valid loss  0.5098257045038507  and accuracy  0.8708516593362655
ep  185  training loss:  0.4716517238257703
valid loss  0.5069650896236163  and accuracy  0.8700519792083167
ep  186  training loss:  0.47046239152341646
valid loss  0.5304507658463485  and accuracy  0.8704518192722911
ep  187  training loss:  0.4720940917993671
valid loss  0.50327417126945  and accuracy  0.8712514994002399
ep  188  training loss:  0.4708632376144677
valid loss  0.5442276080338204  and accuracy  0.8704518192722911
ep  189  training loss:  0.4734916671198853
valid loss  0.5581401419706318  and accuracy  0.8712514994002399
ep  190  training loss:  0.4722550370579484
valid loss  0.5076750289054452  and accuracy  0.8704518192722911
ep  191  training loss:  0.4718027434620038
valid loss  0.5008703347136144  and accuracy  0.8716513394642144
ep  192  training loss:  0.47201837012628906
valid loss  0.5131190323391136  and accuracy  0.8720511795281887
ep  193  training loss:  0.47185962349132726
valid loss  0.5071824193000793  and accuracy  0.8700519792083167
ep  194  training loss:  0.472146126452071
valid loss  0.5080512524223099  and accuracy  0.8724510195921631
ep  195  training loss:  0.4716260303286902
valid loss  0.49922606646895457  and accuracy  0.8700519792083167
ep  196  training loss:  0.47224351154674094
valid loss  0.49810873144200113  and accuracy  0.8716513394642144
ep  197  training loss:  0.4710902262210267
valid loss  0.5014934176113642  and accuracy  0.8716513394642144
ep  198  training loss:  0.47209699809803346
valid loss  0.5019194032372783  and accuracy  0.8720511795281887
ep  199  training loss:  0.47115037292385575
valid loss  0.5003302304065976  and accuracy  0.8720511795281887
ep  200  training loss:  0.47176699997007987
valid loss  0.5114513380867822  and accuracy  0.8716513394642144
ep  201  training loss:  0.47278123040650377
valid loss  0.501321960608514  and accuracy  0.8712514994002399
ep  202  training loss:  0.4720530141934542
valid loss  0.4963329135656643  and accuracy  0.8728508596561375
ep  203  training loss:  0.4709465942072196
valid loss  0.5134491942992738  and accuracy  0.8724510195921631
ep  204  training loss:  0.471934976501228
valid loss  0.49929992364245096  and accuracy  0.8712514994002399
ep  205  training loss:  0.47089141720108785
valid loss  0.4961782542527651  and accuracy  0.8740503798480608
ep  206  training loss:  0.47033226505746256
valid loss  0.4989154377921683  and accuracy  0.8736505397840864
ep  207  training loss:  0.47206931889439063
valid loss  0.5022961296162001  and accuracy  0.8720511795281887
ep  208  training loss:  0.47166479704712416
valid loss  0.49861604455517367  and accuracy  0.8724510195921631
ep  209  training loss:  0.4704273110860516
valid loss  0.4972182855945451  and accuracy  0.8724510195921631
ep  210  training loss:  0.47165635860796484
valid loss  0.49792620957922523  and accuracy  0.8728508596561375
ep  211  training loss:  0.4682873000930495
valid loss  0.49409390977028605  and accuracy  0.8712514994002399
ep  212  training loss:  0.470028073462995
valid loss  0.4995119055477632  and accuracy  0.8704518192722911
ep  213  training loss:  0.4702831705893857
valid loss  0.4986352695793402  and accuracy  0.8732506997201119
ep  214  training loss:  0.46921083918942336
valid loss  0.5019477207939037  and accuracy  0.8700519792083167
ep  215  training loss:  0.46942827812046495
valid loss  0.498664543312676  and accuracy  0.8728508596561375
ep  216  training loss:  0.47180646343184074
valid loss  0.5003556270138925  and accuracy  0.8736505397840864
ep  217  training loss:  0.4702338813821836
valid loss  0.5067846955227309  and accuracy  0.8700519792083167
ep  218  training loss:  0.4728774615263958
valid loss  0.49677960409588073  and accuracy  0.8756497401039585
ep  219  training loss:  0.4690535531565594
valid loss  0.4984244018590531  and accuracy  0.8736505397840864
ep  220  training loss:  0.4693547804814207
valid loss  0.4983682858138407  and accuracy  0.8732506997201119
ep  221  training loss:  0.46799901554525497
valid loss  0.4930812276777674  and accuracy  0.8732506997201119
ep  222  training loss:  0.46924832379020703
valid loss  0.4941043663696974  and accuracy  0.8744502199120352
ep  223  training loss:  0.4708816814367194
valid loss  0.49404088135768487  and accuracy  0.8724510195921631
ep  224  training loss:  0.46912668980672095
valid loss  0.49953089608568424  and accuracy  0.8704518192722911
ep  225  training loss:  0.47114749211423124
valid loss  0.5005352030940553  and accuracy  0.8712514994002399
ep  226  training loss:  0.46946347670003175
valid loss  0.5012745862958528  and accuracy  0.8720511795281887
ep  227  training loss:  0.47013441189344146
valid loss  0.49913907670726876  and accuracy  0.8720511795281887
ep  228  training loss:  0.4695227751181131
valid loss  0.4972385837048924  and accuracy  0.8748500599760096
ep  229  training loss:  0.4722506157881331
valid loss  0.49502871557027517  and accuracy  0.8728508596561375
ep  230  training loss:  0.4680449372697473
valid loss  0.49788923839815424  and accuracy  0.8728508596561375
ep  231  training loss:  0.4702209585265821
valid loss  0.49540006280279025  and accuracy  0.8736505397840864
ep  232  training loss:  0.4677997759286221
valid loss  0.49896504472894987  and accuracy  0.8716513394642144
ep  233  training loss:  0.4680405614254158
valid loss  0.4949334704103779  and accuracy  0.8704518192722911
ep  234  training loss:  0.4669815815145573
valid loss  0.4947513444144361  and accuracy  0.8728508596561375
ep  235  training loss:  0.4707055467797252
valid loss  0.4997168021981881  and accuracy  0.8720511795281887
ep  236  training loss:  0.46921043275508795
valid loss  0.49731345133321947  and accuracy  0.8732506997201119
ep  237  training loss:  0.46936737514953975
valid loss  0.5012646680686628  and accuracy  0.8704518192722911
ep  238  training loss:  0.46775824912624087
valid loss  0.49805565926133516  and accuracy  0.8720511795281887
ep  239  training loss:  0.46994637064890044
valid loss  0.5009830752309443  and accuracy  0.8724510195921631
ep  240  training loss:  0.46847205958257965
valid loss  0.5004833523915415  and accuracy  0.8708516593362655
ep  241  training loss:  0.4678275129783992
valid loss  0.4960754905305639  and accuracy  0.8732506997201119
ep  242  training loss:  0.4677985689589025
valid loss  0.4996607879289195  and accuracy  0.8712514994002399
ep  243  training loss:  0.468898209726158
valid loss  0.5008473484719195  and accuracy  0.8692522990803678
ep  244  training loss:  0.47039150595390405
valid loss  0.505323346330375  and accuracy  0.8712514994002399
ep  245  training loss:  0.4685877831074365
valid loss  0.4952466869559206  and accuracy  0.8724510195921631
ep  246  training loss:  0.467215906203834
valid loss  0.4935780497299867  and accuracy  0.8728508596561375
ep  247  training loss:  0.46891299850991924
valid loss  0.49688816538861824  and accuracy  0.8724510195921631
ep  248  training loss:  0.46868482286633845
valid loss  0.497196721951993  and accuracy  0.8740503798480608
ep  249  training loss:  0.46578731741396606
valid loss  0.4989760159516706  and accuracy  0.8720511795281887
ep  250  training loss:  0.4662514456233326
valid loss  0.49661303264815443  and accuracy  0.8740503798480608
ep  251  training loss:  0.46804564479627553
valid loss  0.49871258348858105  and accuracy  0.8720511795281887
ep  252  training loss:  0.4675579528095405
valid loss  0.49886282308489643  and accuracy  0.8724510195921631
ep  253  training loss:  0.46741035477545667
valid loss  0.4992605923104887  and accuracy  0.8716513394642144
ep  254  training loss:  0.4679267607126021
valid loss  0.501860071472624  and accuracy  0.8704518192722911
ep  255  training loss:  0.4664331686681946
valid loss  0.49928414279439365  and accuracy  0.8736505397840864
ep  256  training loss:  0.4677518652738391
valid loss  0.49530844170539107  and accuracy  0.8740503798480608
ep  257  training loss:  0.4674844490363024
valid loss  0.5013982775877686  and accuracy  0.8712514994002399
ep  258  training loss:  0.46686579273885165
valid loss  0.5008993432885025  and accuracy  0.8704518192722911
ep  259  training loss:  0.46804068628215745
valid loss  0.5035092180512134  and accuracy  0.8712514994002399
ep  260  training loss:  0.4681344754637284
valid loss  0.5000873202802848  and accuracy  0.8724510195921631
ep  261  training loss:  0.4690842609932896
valid loss  0.5021785313799018  and accuracy  0.8720511795281887
ep  262  training loss:  0.4668890233989177
valid loss  0.5357271111616846  and accuracy  0.8732506997201119
ep  263  training loss:  0.4688162481963542
valid loss  0.5069456961764092  and accuracy  0.8700519792083167
ep  264  training loss:  0.4674560753845071
valid loss  0.4951037400629653  and accuracy  0.8736505397840864
ep  265  training loss:  0.46714803766049084
valid loss  0.4977005806173243  and accuracy  0.8732506997201119
ep  266  training loss:  0.4676193178181792
valid loss  0.5030251580064462  and accuracy  0.8708516593362655
ep  267  training loss:  0.4655634741936104
valid loss  0.4973643462165076  and accuracy  0.8720511795281887
ep  268  training loss:  0.4664380926448669
valid loss  0.5032768617125332  and accuracy  0.8728508596561375
ep  269  training loss:  0.46789494184669106
valid loss  0.4995127814810355  and accuracy  0.8740503798480608
ep  270  training loss:  0.46876474311865074
valid loss  0.5028457812479332  and accuracy  0.875249900039984
ep  271  training loss:  0.4672500787717752
valid loss  0.5006427253570046  and accuracy  0.8724510195921631
ep  272  training loss:  0.46661317349977893
valid loss  0.5003868632081127  and accuracy  0.8720511795281887
ep  273  training loss:  0.4675821548226321
valid loss  0.5019953406152607  and accuracy  0.8720511795281887
ep  274  training loss:  0.4665132448638203
valid loss  0.4999535817568038  and accuracy  0.8696521391443423
ep  275  training loss:  0.46541066030131256
valid loss  0.4956465728613721  and accuracy  0.8716513394642144
ep  276  training loss:  0.46710288831274976
valid loss  0.49935140640961556  and accuracy  0.8724510195921631
ep  277  training loss:  0.46716583399643047
valid loss  0.5004075763703155  and accuracy  0.8728508596561375
ep  278  training loss:  0.46532310245474295
valid loss  0.4982296097450188  and accuracy  0.8736505397840864
ep  279  training loss:  0.4672651273246047
valid loss  0.5052514034288018  and accuracy  0.8712514994002399
ep  280  training loss:  0.4659459177206357
valid loss  0.5003364428144986  and accuracy  0.8716513394642144
ep  281  training loss:  0.4655391784440463
valid loss  0.5018268697788981  and accuracy  0.8720511795281887
ep  282  training loss:  0.4662947501749932
valid loss  0.4990258847461229  and accuracy  0.8724510195921631
ep  283  training loss:  0.4682141254508239
valid loss  0.5043083078048077  and accuracy  0.8724510195921631
ep  284  training loss:  0.46749600880289505
valid loss  0.5049218760877073  and accuracy  0.8732506997201119
ep  285  training loss:  0.4666210072097669
valid loss  0.49572950637326246  and accuracy  0.8728508596561375
ep  286  training loss:  0.46683844750118925
valid loss  0.5128864846626123  and accuracy  0.8708516593362655
ep  287  training loss:  0.4662210966138525
valid loss  0.5079083841879432  and accuracy  0.8720511795281887
ep  288  training loss:  0.46572190880840353
valid loss  0.5031593520848191  and accuracy  0.8720511795281887
ep  289  training loss:  0.46673218996721805
valid loss  0.506648337493082  and accuracy  0.8736505397840864
ep  290  training loss:  0.46488828778421665
valid loss  0.511691427764679  and accuracy  0.8724510195921631
ep  291  training loss:  0.46646111075355656
valid loss  0.4969443526329016  and accuracy  0.8744502199120352
ep  292  training loss:  0.4672421719743966
valid loss  0.5065854804747488  and accuracy  0.8720511795281887
ep  293  training loss:  0.4671834946662179
valid loss  0.4952336670588799  and accuracy  0.8736505397840864
ep  294  training loss:  0.4661379366135886
valid loss  0.49703463521160063  and accuracy  0.8740503798480608
ep  295  training loss:  0.4666307810647957
valid loss  0.499795814101003  and accuracy  0.8728508596561375
ep  296  training loss:  0.4669999869790647
valid loss  0.4953697320462608  and accuracy  0.8712514994002399
ep  297  training loss:  0.4646755154158603
valid loss  0.5012286969991934  and accuracy  0.8704518192722911
ep  298  training loss:  0.4639311335240483
valid loss  0.49989490735916936  and accuracy  0.8720511795281887
ep  299  training loss:  0.46785997561266424
valid loss  0.5046338133338164  and accuracy  0.8708516593362655
ep  300  training loss:  0.46481566745595193
valid loss  0.4992864758479314  and accuracy  0.8720511795281887
ep  301  training loss:  0.4648287619616479
valid loss  0.498661997936955  and accuracy  0.8728508596561375
ep  302  training loss:  0.4658315500934907
valid loss  0.4996981134609145  and accuracy  0.8732506997201119
ep  303  training loss:  0.4663591099756887
valid loss  0.4978482829766577  and accuracy  0.8740503798480608
ep  304  training loss:  0.4635277180532109
valid loss  0.500837806926542  and accuracy  0.8716513394642144
ep  305  training loss:  0.46420724684406633
valid loss  0.49893063528449094  and accuracy  0.8724510195921631
ep  306  training loss:  0.46567473403347853
valid loss  0.5033696431343386  and accuracy  0.8720511795281887
ep  307  training loss:  0.46515574077200195
valid loss  0.5018314293316487  and accuracy  0.8720511795281887
ep  308  training loss:  0.46627765454870157
valid loss  0.5042639124731882  and accuracy  0.8696521391443423
ep  309  training loss:  0.4653619785041055
valid loss  0.49800990039899035  and accuracy  0.8708516593362655
ep  310  training loss:  0.46606570426216326
valid loss  0.4989432741026552  and accuracy  0.8720511795281887
ep  311  training loss:  0.465687651041939
valid loss  0.5040900806149021  and accuracy  0.8716513394642144
ep  312  training loss:  0.46294238687015876
valid loss  0.5020562821938866  and accuracy  0.8736505397840864
ep  313  training loss:  0.4659964915565762
valid loss  0.49960682662378925  and accuracy  0.8716513394642144
ep  314  training loss:  0.46560013105014725
valid loss  0.500234934483848  and accuracy  0.8716513394642144
ep  315  training loss:  0.4645316786410406
valid loss  0.5032831646594368  and accuracy  0.8712514994002399
ep  316  training loss:  0.4656237326773014
valid loss  0.4955069165571076  and accuracy  0.8724510195921631
ep  317  training loss:  0.46312814237554345
valid loss  0.4999614249034578  and accuracy  0.8720511795281887
ep  318  training loss:  0.4658518177619596
valid loss  0.4978080514620324  and accuracy  0.8732506997201119
ep  319  training loss:  0.466476988530827
valid loss  0.49957183314437437  and accuracy  0.8720511795281887
ep  320  training loss:  0.46352072868536276
valid loss  0.4983832396921374  and accuracy  0.8732506997201119
ep  321  training loss:  0.4633266245846054
valid loss  0.49799765617263075  and accuracy  0.8744502199120352
ep  322  training loss:  0.46377347908261046
valid loss  0.5037213572260381  and accuracy  0.8712514994002399
ep  323  training loss:  0.46419802174915037
valid loss  0.494590689114979  and accuracy  0.8732506997201119
ep  324  training loss:  0.4648793320883668
valid loss  0.5008809568881035  and accuracy  0.8724510195921631
ep  325  training loss:  0.46364448883534576
valid loss  0.4957350933732914  and accuracy  0.8732506997201119
ep  326  training loss:  0.46317507260019164
valid loss  0.4973976851129284  and accuracy  0.8732506997201119
ep  327  training loss:  0.4656902046637737
valid loss  0.5003088117003298  and accuracy  0.8700519792083167
ep  328  training loss:  0.4649429771986942
valid loss  0.5012554014887346  and accuracy  0.8696521391443423
ep  329  training loss:  0.4659633938379287
valid loss  0.5020559802383292  and accuracy  0.8712514994002399
ep  330  training loss:  0.4645098688237616
valid loss  0.49503092292021483  and accuracy  0.8732506997201119
ep  331  training loss:  0.4629318026987331
valid loss  0.5045381667541533  and accuracy  0.8708516593362655
ep  332  training loss:  0.46377634007186214
valid loss  0.5019074315693034  and accuracy  0.8724510195921631
ep  333  training loss:  0.4636088853590482
valid loss  0.5021659834582249  and accuracy  0.8724510195921631
ep  334  training loss:  0.4625615598498549
valid loss  0.5035980794130445  and accuracy  0.8704518192722911
ep  335  training loss:  0.4633632089078883
valid loss  0.5083701117951028  and accuracy  0.8716513394642144
ep  336  training loss:  0.4644140879249916
valid loss  0.5106383068878048  and accuracy  0.8692522990803678
ep  337  training loss:  0.46325579764251007
valid loss  0.5069953056751657  and accuracy  0.8712514994002399
ep  338  training loss:  0.46593503987171614
valid loss  0.5008469237798884  and accuracy  0.8724510195921631
ep  339  training loss:  0.46439379721117396
valid loss  0.4960428834175978  and accuracy  0.8732506997201119
ep  340  training loss:  0.46330920600517694
valid loss  0.5024363216711302  and accuracy  0.8716513394642144
ep  341  training loss:  0.46296239220480495
valid loss  0.49979195574291796  and accuracy  0.8728508596561375
ep  342  training loss:  0.46446832815922556
valid loss  0.500480714939633  and accuracy  0.8724510195921631
ep  343  training loss:  0.4645695339006427
valid loss  0.4966680710432959  and accuracy  0.8712514994002399
ep  344  training loss:  0.4638751470227403
valid loss  0.5004190943661522  and accuracy  0.8708516593362655
ep  345  training loss:  0.46375545834126125
valid loss  0.498551991475863  and accuracy  0.8712514994002399
ep  346  training loss:  0.4631549211369422
valid loss  0.5001516801888635  and accuracy  0.8720511795281887
ep  347  training loss:  0.4634967296860057
valid loss  0.49943294362133384  and accuracy  0.8724510195921631
ep  348  training loss:  0.4628337341450413
valid loss  0.4979512711040309  and accuracy  0.8720511795281887
ep  349  training loss:  0.46156152307305776
valid loss  0.4978449953789236  and accuracy  0.8724510195921631
ep  350  training loss:  0.46153838333385044
valid loss  0.5006653521953226  and accuracy  0.8728508596561375
ep  351  training loss:  0.4612618036303889
valid loss  0.4991099864828353  and accuracy  0.8736505397840864
ep  352  training loss:  0.4635959677228157
valid loss  0.49658644425778425  and accuracy  0.8716513394642144
ep  353  training loss:  0.46224130736837493
valid loss  0.5079759853165515  and accuracy  0.8696521391443423
ep  354  training loss:  0.4643215087696288
valid loss  0.4964892484578358  and accuracy  0.8748500599760096
ep  355  training loss:  0.46243262651892303
valid loss  0.5004350858085492  and accuracy  0.8712514994002399
ep  356  training loss:  0.4641038685733786
valid loss  0.49853791007515147  and accuracy  0.8716513394642144
ep  357  training loss:  0.46312169237743694
valid loss  0.49872988591142675  and accuracy  0.8728508596561375
ep  358  training loss:  0.46251640219131085
valid loss  0.5039272423936385  and accuracy  0.8724510195921631
ep  359  training loss:  0.463475142902523
valid loss  0.5032328116755541  and accuracy  0.8736505397840864
ep  360  training loss:  0.4645401801294785
valid loss  0.4993845376549888  and accuracy  0.8732506997201119
ep  361  training loss:  0.46347130394804653
valid loss  0.5032207796760484  and accuracy  0.8704518192722911
ep  362  training loss:  0.46221501594402714
valid loss  0.5036974564188721  and accuracy  0.8720511795281887
ep  363  training loss:  0.46334187354647716
valid loss  0.494705611565074  and accuracy  0.8740503798480608
ep  364  training loss:  0.4630661260478136
valid loss  0.50020035217162  and accuracy  0.8728508596561375
ep  365  training loss:  0.4637471062113948
valid loss  0.5002524986690352  and accuracy  0.8716513394642144
ep  366  training loss:  0.46436340115748187
valid loss  0.5076240823775089  and accuracy  0.8692522990803678
ep  367  training loss:  0.46382049677225107
valid loss  0.4992912125225212  and accuracy  0.8712514994002399
ep  368  training loss:  0.4616718566396727
valid loss  0.49309048356889773  and accuracy  0.8720511795281887
ep  369  training loss:  0.46350308184528066
valid loss  0.5015022327951411  and accuracy  0.8728508596561375
ep  370  training loss:  0.4609158150394626
valid loss  0.49464238643264924  and accuracy  0.8720511795281887
ep  371  training loss:  0.46161098122831373
valid loss  0.49909901854897537  and accuracy  0.8712514994002399
ep  372  training loss:  0.4624796402622115
valid loss  0.5030257358449977  and accuracy  0.8704518192722911
ep  373  training loss:  0.46337735614233855
valid loss  0.49916113508505516  and accuracy  0.8724510195921631
ep  374  training loss:  0.46370797445288936
valid loss  0.4999574724076891  and accuracy  0.8712514994002399
ep  375  training loss:  0.46394264586701994
valid loss  0.49377317328016457  and accuracy  0.8724510195921631
ep  376  training loss:  0.4622908587097108
valid loss  0.49719829613663874  and accuracy  0.8712514994002399
ep  377  training loss:  0.4637224154584377
valid loss  0.5032318938808029  and accuracy  0.8696521391443423
ep  378  training loss:  0.4625750201169987
valid loss  0.4987147925425319  and accuracy  0.8744502199120352
ep  379  training loss:  0.46168610226085094
valid loss  0.49760888129031833  and accuracy  0.8708516593362655
ep  380  training loss:  0.4632514234555966
valid loss  0.5023093991925934  and accuracy  0.8724510195921631
ep  381  training loss:  0.46274751826164146
valid loss  0.5014390421957552  and accuracy  0.8700519792083167
ep  382  training loss:  0.46315731066565974
valid loss  0.5007816029400504  and accuracy  0.8724510195921631
ep  383  training loss:  0.46061137505944305
valid loss  0.49878826209517874  and accuracy  0.8720511795281887
ep  384  training loss:  0.46289196297687984
valid loss  0.5017149663838993  and accuracy  0.8736505397840864
ep  385  training loss:  0.4628730806602376
valid loss  0.4961791300907082  and accuracy  0.8756497401039585
ep  386  training loss:  0.46326431749983193
valid loss  0.49767324478566194  and accuracy  0.8720511795281887
ep  387  training loss:  0.46178440130047976
valid loss  0.5016892082640668  and accuracy  0.8712514994002399
ep  388  training loss:  0.4615081550238385
valid loss  0.5000712841165299  and accuracy  0.8724510195921631
ep  389  training loss:  0.4621594960682855
valid loss  0.49516440481722046  and accuracy  0.8736505397840864
ep  390  training loss:  0.463525230016704
valid loss  0.4957967558559157  and accuracy  0.8728508596561375
ep  391  training loss:  0.46166450189852204
valid loss  0.49904285201498244  and accuracy  0.8708516593362655
ep  392  training loss:  0.4608121853242383
valid loss  0.4990111600394632  and accuracy  0.8720511795281887
ep  393  training loss:  0.46229096426027866
valid loss  0.5040484219968248  and accuracy  0.8708516593362655
ep  394  training loss:  0.46219162211565323
valid loss  0.4945966808236346  and accuracy  0.8712514994002399
ep  395  training loss:  0.4610209527042744
valid loss  0.4975170040907549  and accuracy  0.8704518192722911
ep  396  training loss:  0.4604183085635008
valid loss  0.498693420905106  and accuracy  0.8724510195921631
ep  397  training loss:  0.4613327961345629
valid loss  0.4997395827311699  and accuracy  0.8716513394642144
ep  398  training loss:  0.4600722107352501
valid loss  0.4994149614171665  and accuracy  0.8708516593362655
ep  399  training loss:  0.4619934351965732
valid loss  0.49445001783965825  and accuracy  0.8736505397840864
ep  400  training loss:  0.46217202689960285
valid loss  0.49777288093227523  and accuracy  0.8716513394642144
ep  401  training loss:  0.4606573281687924
valid loss  0.4967789055823517  and accuracy  0.8720511795281887
ep  402  training loss:  0.4626373494783185
valid loss  0.4966846267398192  and accuracy  0.8720511795281887
ep  403  training loss:  0.46249797835417533
valid loss  0.49545773862600806  and accuracy  0.8724510195921631
ep  404  training loss:  0.4610634597620626
valid loss  0.49272095597014337  and accuracy  0.8740503798480608
ep  405  training loss:  0.46280826704651573
valid loss  0.5004157273471951  and accuracy  0.8716513394642144
ep  406  training loss:  0.45975413518243713
valid loss  0.49831593401286184  and accuracy  0.8728508596561375
ep  407  training loss:  0.4610010634088562
valid loss  0.49765060622184004  and accuracy  0.8740503798480608
ep  408  training loss:  0.4600720347868517
valid loss  0.4972743282838613  and accuracy  0.8724510195921631
ep  409  training loss:  0.4603636499398058
valid loss  0.4997947926833028  and accuracy  0.8716513394642144
ep  410  training loss:  0.4613627689663222
valid loss  0.49574717677912206  and accuracy  0.8736505397840864
ep  411  training loss:  0.46026241775957644
valid loss  0.49661282682027974  and accuracy  0.8704518192722911
ep  412  training loss:  0.4612567456374941
valid loss  0.49860969222769264  and accuracy  0.8712514994002399
ep  413  training loss:  0.4610640613086919
valid loss  0.4956296478114763  and accuracy  0.8720511795281887
ep  414  training loss:  0.4614405507712694
valid loss  0.5056652912422449  and accuracy  0.8724510195921631
ep  415  training loss:  0.4618603778489995
valid loss  0.496730610591228  and accuracy  0.8708516593362655
ep  416  training loss:  0.4638030056782896
valid loss  0.5011758613901013  and accuracy  0.8712514994002399
ep  417  training loss:  0.4617647813055312
valid loss  0.4987489103746624  and accuracy  0.8712514994002399
ep  418  training loss:  0.4603346557734305
valid loss  0.49670387945714733  and accuracy  0.8724510195921631
ep  419  training loss:  0.4624112726912795
valid loss  0.5013631001823857  and accuracy  0.8716513394642144
ep  420  training loss:  0.4622104199676289
valid loss  0.4942889270044622  and accuracy  0.8740503798480608
ep  421  training loss:  0.4599236459135285
valid loss  0.4982153635199477  and accuracy  0.8720511795281887
ep  422  training loss:  0.46148824937389826
valid loss  0.49677889752702586  and accuracy  0.8724510195921631
ep  423  training loss:  0.4611241749867187
valid loss  0.4993387475008013  and accuracy  0.8724510195921631
ep  424  training loss:  0.4619227961417773
valid loss  0.4960781199986436  and accuracy  0.8716513394642144
ep  425  training loss:  0.46168739462129443
valid loss  0.49901686413962093  and accuracy  0.8712514994002399
ep  426  training loss:  0.46109025958190536
valid loss  0.4984067145799075  and accuracy  0.8716513394642144
ep  427  training loss:  0.46058787831140274
valid loss  0.49353319039491594  and accuracy  0.8740503798480608
ep  428  training loss:  0.46043330649144637
valid loss  0.508821464559642  and accuracy  0.8708516593362655
ep  429  training loss:  0.46297820172590837
valid loss  0.5006118204916825  and accuracy  0.8704518192722911
ep  430  training loss:  0.46079367182303616
valid loss  0.49660701002897334  and accuracy  0.8736505397840864
ep  431  training loss:  0.4621551745805246
valid loss  0.49952924566143087  and accuracy  0.8724510195921631
ep  432  training loss:  0.46289616509185594
valid loss  0.5028469594704156  and accuracy  0.8724510195921631
ep  433  training loss:  0.45867686687968945
valid loss  0.4984464672792916  and accuracy  0.8736505397840864
ep  434  training loss:  0.46151866439304495
valid loss  0.4952148883903279  and accuracy  0.8736505397840864
ep  435  training loss:  0.4625795181609612
valid loss  0.5017777637975877  and accuracy  0.8720511795281887
ep  436  training loss:  0.46188126229786575
valid loss  0.4950175025805336  and accuracy  0.8720511795281887
ep  437  training loss:  0.46050454204573427
valid loss  0.5011430079581403  and accuracy  0.8724510195921631
ep  438  training loss:  0.46053848055580304
valid loss  0.4998400767223209  and accuracy  0.8716513394642144
ep  439  training loss:  0.46123442274575915
valid loss  0.49578004564251343  and accuracy  0.8736505397840864
ep  440  training loss:  0.45932779768572723
valid loss  0.49854092642527875  and accuracy  0.8720511795281887
ep  441  training loss:  0.462177362202989
valid loss  0.493340213565243  and accuracy  0.8728508596561375
ep  442  training loss:  0.46291668508598877
valid loss  0.49526482956355117  and accuracy  0.8720511795281887
ep  443  training loss:  0.46192017537454516
valid loss  0.5031088569649502  and accuracy  0.8704518192722911
ep  444  training loss:  0.46114442138082995
valid loss  0.5053795445971087  and accuracy  0.8704518192722911
ep  445  training loss:  0.46233149157970654
valid loss  0.49457409608511865  and accuracy  0.8724510195921631
ep  446  training loss:  0.4610498878263819
valid loss  0.49463938865981927  and accuracy  0.8728508596561375
ep  447  training loss:  0.45843884692459064
valid loss  0.4926349978263928  and accuracy  0.8724510195921631
ep  448  training loss:  0.4623656577438739
valid loss  0.4989437037756423  and accuracy  0.8724510195921631
ep  449  training loss:  0.4594026392885746
valid loss  0.5037550793581608  and accuracy  0.8708516593362655
ep  450  training loss:  0.461464240512405
valid loss  0.49766221948739575  and accuracy  0.8720511795281887
ep  451  training loss:  0.46014619810449214
valid loss  0.49879356025934507  and accuracy  0.8716513394642144
ep  452  training loss:  0.46039822480352965
valid loss  0.49791879591489974  and accuracy  0.8716513394642144
ep  453  training loss:  0.4601342417725646
valid loss  0.5009233501971793  and accuracy  0.8720511795281887
ep  454  training loss:  0.4606392709005967
valid loss  0.49893993440745876  and accuracy  0.8724510195921631
ep  455  training loss:  0.46016519819079166
valid loss  0.5011478241492824  and accuracy  0.8708516593362655
ep  456  training loss:  0.4607778805023292
valid loss  0.49537726820873673  and accuracy  0.8736505397840864
ep  457  training loss:  0.4599632219149692
valid loss  0.49988073141562467  and accuracy  0.8708516593362655
ep  458  training loss:  0.4593663960202285
valid loss  0.4963586688256178  and accuracy  0.8708516593362655
ep  459  training loss:  0.45986782891316313
valid loss  0.5009963564613445  and accuracy  0.8708516593362655
ep  460  training loss:  0.46104940215798146
valid loss  0.5045386253476667  and accuracy  0.8688524590163934
ep  461  training loss:  0.4632463910815414
valid loss  0.49846021942451735  and accuracy  0.8708516593362655
ep  462  training loss:  0.45929914400392735
valid loss  0.5007192119270837  and accuracy  0.8688524590163934
ep  463  training loss:  0.4593200182100003
valid loss  0.502822889799311  and accuracy  0.8700519792083167
ep  464  training loss:  0.4615875391388734
valid loss  0.501690294875092  and accuracy  0.8724510195921631
ep  465  training loss:  0.4591102972916859
valid loss  0.49435536225573246  and accuracy  0.8728508596561375
ep  466  training loss:  0.4610709087289235
valid loss  0.4942533778386419  and accuracy  0.8728508596561375
ep  467  training loss:  0.458836764641366
valid loss  0.5005427323928217  and accuracy  0.8716513394642144
ep  468  training loss:  0.46138737848707717
valid loss  0.4926990907652671  and accuracy  0.8732506997201119
ep  469  training loss:  0.4589314399424937
valid loss  0.4931826324450498  and accuracy  0.8708516593362655
ep  470  training loss:  0.4589570850112679
valid loss  0.49774374804798005  and accuracy  0.8716513394642144
ep  471  training loss:  0.4609610297678503
valid loss  0.4981136551526774  and accuracy  0.8708516593362655
ep  472  training loss:  0.45967856603050367
valid loss  0.4962610198182614  and accuracy  0.8728508596561375
ep  473  training loss:  0.45917396989792875
valid loss  0.5001697924221958  and accuracy  0.8716513394642144
ep  474  training loss:  0.46075857371819456
valid loss  0.4994007320177169  and accuracy  0.8708516593362655
ep  475  training loss:  0.4598094079980619
valid loss  0.4940248271886085  and accuracy  0.8712514994002399
ep  476  training loss:  0.4594613015102558
valid loss  0.5011781507709034  and accuracy  0.8696521391443423
ep  477  training loss:  0.4591361378147213
valid loss  0.49449442256121384  and accuracy  0.8736505397840864
ep  478  training loss:  0.4581188277583899
valid loss  0.4947562603200259  and accuracy  0.8712514994002399
ep  479  training loss:  0.4601641474888799
valid loss  0.49632858480514885  and accuracy  0.8724510195921631
ep  480  training loss:  0.46088347297361976
valid loss  0.49767091018302306  and accuracy  0.8712514994002399
ep  481  training loss:  0.4603653446397139
valid loss  0.49783739483342176  and accuracy  0.8712514994002399
ep  482  training loss:  0.4589617435202496
valid loss  0.4942817330980053  and accuracy  0.8736505397840864
ep  483  training loss:  0.45803499365078554
valid loss  0.49721326650690434  and accuracy  0.8708516593362655
ep  484  training loss:  0.46078949060425684
valid loss  0.49214801807157615  and accuracy  0.8744502199120352
ep  485  training loss:  0.4598472144886389
valid loss  0.49098168852709617  and accuracy  0.8720511795281887
ep  486  training loss:  0.45874457494802895
valid loss  0.532629408964106  and accuracy  0.8712514994002399
ep  487  training loss:  0.45819312856371985
valid loss  0.4959857642340784  and accuracy  0.8720511795281887
ep  488  training loss:  0.4575248498444162
valid loss  0.4967014125612725  and accuracy  0.8720511795281887
ep  489  training loss:  0.4599405705312263
valid loss  0.49326372680450525  and accuracy  0.8736505397840864
ep  490  training loss:  0.4590903628054184
valid loss  0.49619202055439193  and accuracy  0.8724510195921631
ep  491  training loss:  0.45768272323726017
valid loss  0.4923676571003297  and accuracy  0.8724510195921631
ep  492  training loss:  0.4573815815589741
valid loss  0.4999036137602988  and accuracy  0.8696521391443423
ep  493  training loss:  0.46088840278744864
valid loss  0.4930799833205832  and accuracy  0.8724510195921631
ep  494  training loss:  0.4580997024652691
valid loss  0.49561887891327844  and accuracy  0.8712514994002399
ep  495  training loss:  0.4587890480717271
valid loss  0.49512169844672377  and accuracy  0.8728508596561375
ep  496  training loss:  0.45732837978696955
valid loss  0.4985453927340578  and accuracy  0.8720511795281887
ep  497  training loss:  0.4586777203251476
valid loss  0.498937803094552  and accuracy  0.8716513394642144
ep  498  training loss:  0.45779525789864484
valid loss  0.4976000050838734  and accuracy  0.8724510195921631
ep  499  training loss:  0.4550637111558882
valid loss  0.49666553015996817  and accuracy  0.8736505397840864
ep  500  training loss:  0.45820140113627483
valid loss  0.49709600025489303  and accuracy  0.8724510195921631
ep  501  training loss:  0.45863182595429236
valid loss  0.4938001104208623  and accuracy  0.8732506997201119
ep  502  training loss:  0.46053329333173126
valid loss  0.5067564393224262  and accuracy  0.8716513394642144
ep  503  training loss:  0.4595842397419161
valid loss  0.49625693984338637  and accuracy  0.8728508596561375
ep  504  training loss:  0.4573499583170599
valid loss  0.49887714973214814  and accuracy  0.8704518192722911
ep  505  training loss:  0.4571713600417688
valid loss  0.500475515870274  and accuracy  0.8716513394642144
ep  506  training loss:  0.45872328054655825
valid loss  0.4974950649699227  and accuracy  0.8732506997201119
ep  507  training loss:  0.4588961819642113
valid loss  0.4934118260197142  and accuracy  0.8724510195921631
ep  508  training loss:  0.45865772732628407
valid loss  0.5015378084386743  and accuracy  0.8708516593362655
ep  509  training loss:  0.45755392910323006
valid loss  0.4961243569970083  and accuracy  0.8716513394642144
ep  510  training loss:  0.45942684103333425
valid loss  0.49849170560791034  and accuracy  0.8712514994002399
ep  511  training loss:  0.4601834369637298
valid loss  0.4987056559679366  and accuracy  0.8712514994002399
ep  512  training loss:  0.45752357995022364
valid loss  0.4951472975692002  and accuracy  0.8728508596561375
ep  513  training loss:  0.4577770308424537
valid loss  0.4914304038039783  and accuracy  0.8720511795281887
ep  514  training loss:  0.4580914524113834
valid loss  0.4946175046035739  and accuracy  0.8716513394642144
ep  515  training loss:  0.46164548888673357
valid loss  0.49690302937138514  and accuracy  0.8712514994002399
ep  516  training loss:  0.45820417184616846
valid loss  0.4955962089742007  and accuracy  0.8724510195921631
ep  517  training loss:  0.4586180567122358
valid loss  0.48955701056216916  and accuracy  0.8732506997201119
ep  518  training loss:  0.4589005391713268
valid loss  0.49220105782836404  and accuracy  0.8740503798480608
ep  519  training loss:  0.4566004211701232
valid loss  0.4937245878444963  and accuracy  0.8748500599760096
ep  520  training loss:  0.46043593814047623
valid loss  0.49074742777115915  and accuracy  0.8732506997201119
ep  521  training loss:  0.45922477116581384
valid loss  0.4943973967810718  and accuracy  0.8724510195921631
ep  522  training loss:  0.45811793187279726
valid loss  0.4958170011061661  and accuracy  0.8712514994002399
ep  523  training loss:  0.4581363477673062
valid loss  0.4928836426535686  and accuracy  0.8744502199120352
ep  524  training loss:  0.4587530767509768
valid loss  0.496049119526746  and accuracy  0.8712514994002399
ep  525  training loss:  0.45896547600434123
valid loss  0.4980070957037412  and accuracy  0.8716513394642144
ep  526  training loss:  0.46004316790468647
valid loss  0.49694388737825335  and accuracy  0.8724510195921631
ep  527  training loss:  0.45656159677279445
valid loss  0.5095633295406012  and accuracy  0.8708516593362655
ep  528  training loss:  0.4606258051382636
valid loss  0.4982313660086274  and accuracy  0.8712514994002399
ep  529  training loss:  0.4589430103724743
valid loss  0.5076019952698547  and accuracy  0.8712514994002399
ep  530  training loss:  0.4557434074570373
valid loss  0.49788635179120416  and accuracy  0.8704518192722911
ep  531  training loss:  0.4577079196711911
valid loss  0.4988395095538826  and accuracy  0.8704518192722911
ep  532  training loss:  0.45897207000935625
valid loss  0.49761216899529787  and accuracy  0.8712514994002399
ep  533  training loss:  0.45890611963528893
valid loss  0.498584478676486  and accuracy  0.8712514994002399
ep  534  training loss:  0.45753640245840155
valid loss  0.4942641301852901  and accuracy  0.8712514994002399
ep  535  training loss:  0.4584556906071234
valid loss  0.5052666277086577  and accuracy  0.8704518192722911
ep  536  training loss:  0.4582005659985459
valid loss  0.4984346582431023  and accuracy  0.8716513394642144
ep  537  training loss:  0.45866891549926464
valid loss  0.49927381280420874  and accuracy  0.8720511795281887
ep  538  training loss:  0.4575857390655116
valid loss  0.4939875719738884  and accuracy  0.8732506997201119
ep  539  training loss:  0.4595375527758071
valid loss  0.498126609260585  and accuracy  0.8712514994002399
ep  540  training loss:  0.45951835951232406
valid loss  0.4962025787629208  and accuracy  0.8716513394642144
ep  541  training loss:  0.45755849141457344
valid loss  0.49453285869146907  and accuracy  0.8712514994002399
ep  542  training loss:  0.4566032794348693
valid loss  0.5001278581356153  and accuracy  0.8720511795281887
ep  543  training loss:  0.45929579787170666
valid loss  0.49805597022551146  and accuracy  0.8704518192722911
ep  544  training loss:  0.459541867833585
valid loss  0.4992917670053942  and accuracy  0.8712514994002399
ep  545  training loss:  0.4579924833805019
valid loss  0.4981973781818297  and accuracy  0.8728508596561375
ep  546  training loss:  0.4598212014443661
valid loss  0.4920307780470385  and accuracy  0.8744502199120352
ep  547  training loss:  0.457078293682897
valid loss  0.49781659468323264  and accuracy  0.8716513394642144
ep  548  training loss:  0.4581022936673094
valid loss  0.497726894411646  and accuracy  0.8720511795281887
ep  549  training loss:  0.4566884174174792
valid loss  0.4931579637365406  and accuracy  0.8728508596561375
ep  550  training loss:  0.45716047315817393
valid loss  0.4987166666808199  and accuracy  0.8712514994002399
ep  551  training loss:  0.45578035263316485
valid loss  0.49662301275597626  and accuracy  0.8720511795281887
ep  552  training loss:  0.45663273357342593
valid loss  0.4990105914716862  and accuracy  0.8720511795281887
ep  553  training loss:  0.4539503503499246
valid loss  0.4936637627106102  and accuracy  0.8724510195921631
ep  554  training loss:  0.45726040353267416
valid loss  0.496200004216434  and accuracy  0.8716513394642144
ep  555  training loss:  0.4555595867487747
valid loss  0.4940170360679199  and accuracy  0.8724510195921631
ep  556  training loss:  0.45906770632836014
valid loss  0.5015631305222891  and accuracy  0.8712514994002399
ep  557  training loss:  0.45730759477319816
valid loss  0.49588040865120625  and accuracy  0.8724510195921631
ep  558  training loss:  0.45670698348803607
valid loss  0.4969346332983797  and accuracy  0.8716513394642144
ep  559  training loss:  0.4598520853630367
valid loss  0.49504276198513364  and accuracy  0.8724510195921631
ep  560  training loss:  0.45638624257620836
valid loss  0.49884469272708093  and accuracy  0.8708516593362655
ep  561  training loss:  0.45944688182898746
valid loss  0.4977720905570496  and accuracy  0.8712514994002399
ep  562  training loss:  0.45705935909518536
valid loss  0.49254941595930524  and accuracy  0.8740503798480608
ep  563  training loss:  0.456727317554317
valid loss  0.49891777438480633  and accuracy  0.8728508596561375
ep  564  training loss:  0.45713861925707094
valid loss  0.49532897574383944  and accuracy  0.8720511795281887
ep  565  training loss:  0.457045164950065
valid loss  0.49705039897188097  and accuracy  0.8748500599760096
ep  566  training loss:  0.45750235722833676
valid loss  0.4974788891129949  and accuracy  0.8704518192722911
ep  567  training loss:  0.4562390809772232
valid loss  0.4940901600518545  and accuracy  0.8736505397840864
ep  568  training loss:  0.458714657387665
valid loss  0.4981520409728946  and accuracy  0.8700519792083167
ep  569  training loss:  0.45663075854023566
valid loss  0.49851175751842436  and accuracy  0.8724510195921631
ep  570  training loss:  0.4581724308065156
valid loss  0.49936794101453125  and accuracy  0.8712514994002399
ep  571  training loss:  0.45714648094722
valid loss  0.499906653644847  and accuracy  0.8712514994002399
ep  572  training loss:  0.4598611973379269
valid loss  0.4984830082392321  and accuracy  0.8716513394642144
ep  573  training loss:  0.4579521696246236
valid loss  0.4945089994717102  and accuracy  0.8732506997201119
ep  574  training loss:  0.4587758640167856
valid loss  0.5023652367451724  and accuracy  0.8700519792083167
ep  575  training loss:  0.45753166628291275
valid loss  0.4931470951358112  and accuracy  0.8732506997201119
ep  576  training loss:  0.45659364167532385
valid loss  0.4985959983525015  and accuracy  0.8724510195921631
ep  577  training loss:  0.4568377335862132
valid loss  0.4952410611330343  and accuracy  0.8716513394642144
ep  578  training loss:  0.45717851604627174
valid loss  0.4954792297825438  and accuracy  0.8732506997201119
ep  579  training loss:  0.45649584849014857
valid loss  0.4990423019887542  and accuracy  0.8724510195921631
ep  580  training loss:  0.45662393309576904
valid loss  0.4970644437661413  and accuracy  0.8720511795281887
ep  581  training loss:  0.4591466801350947
valid loss  0.5005437166678434  and accuracy  0.8712514994002399
ep  582  training loss:  0.45606850473728705
valid loss  0.49668717820469926  and accuracy  0.8704518192722911
ep  583  training loss:  0.45697880050855355
valid loss  0.49654293955206535  and accuracy  0.8720511795281887
ep  584  training loss:  0.45809132652624385
valid loss  0.49986513169323715  and accuracy  0.8708516593362655
ep  585  training loss:  0.45652531589054524
valid loss  0.4969743088167794  and accuracy  0.8716513394642144
ep  586  training loss:  0.45920093174660453
valid loss  0.49817567989855754  and accuracy  0.8716513394642144
ep  587  training loss:  0.45503603413384675
valid loss  0.4934207889519897  and accuracy  0.8728508596561375
ep  588  training loss:  0.45827088738821714
valid loss  0.49674041444709044  and accuracy  0.8732506997201119
ep  589  training loss:  0.45579284019439065
valid loss  0.49991414760027536  and accuracy  0.8720511795281887
ep  590  training loss:  0.4566445129528942
valid loss  0.5020547244177966  and accuracy  0.8708516593362655
ep  591  training loss:  0.4597248092641323
valid loss  0.5004879553787044  and accuracy  0.8720511795281887
ep  592  training loss:  0.4561866899247375
valid loss  0.49873382594288945  and accuracy  0.8716513394642144
ep  593  training loss:  0.4554115198962907
valid loss  0.4970811301829671  and accuracy  0.8720511795281887
ep  594  training loss:  0.4563904606725379
valid loss  0.49728510461774456  and accuracy  0.8728508596561375
ep  595  training loss:  0.4548744276516042
valid loss  0.4958138672984252  and accuracy  0.8732506997201119
ep  596  training loss:  0.4573092251183614
valid loss  0.49380101930232395  and accuracy  0.8716513394642144
ep  597  training loss:  0.4568669624082796
valid loss  0.4979907372554556  and accuracy  0.8708516593362655
ep  598  training loss:  0.45549084041539867
valid loss  0.4951795241633495  and accuracy  0.8716513394642144
ep  599  training loss:  0.45678661874932225
valid loss  0.49423743122484814  and accuracy  0.8716513394642144
ep  600  training loss:  0.4557921558205439
valid loss  0.5038220454387214  and accuracy  0.8712514994002399
ep  601  training loss:  0.4568654759227543
valid loss  0.49697106959866505  and accuracy  0.8720511795281887
ep  602  training loss:  0.45700478442070985
valid loss  0.49328605273588805  and accuracy  0.8740503798480608
ep  603  training loss:  0.4561774174199291
valid loss  0.49611953300077977  and accuracy  0.8712514994002399
ep  604  training loss:  0.4556910961565597
valid loss  0.49165745760526053  and accuracy  0.8712514994002399
ep  605  training loss:  0.4561149095279836
valid loss  0.4929914323509526  and accuracy  0.8732506997201119
ep  606  training loss:  0.45813433362878625
valid loss  0.4982318834441512  and accuracy  0.8716513394642144
ep  607  training loss:  0.4563248773816093
valid loss  0.4998184762588266  and accuracy  0.8716513394642144
ep  608  training loss:  0.4550315869618832
valid loss  0.49309081489779577  and accuracy  0.8712514994002399
ep  609  training loss:  0.4565399271283968
valid loss  0.500178926708888  and accuracy  0.8708516593362655
ep  610  training loss:  0.45564788939880063
valid loss  0.49410172045731343  and accuracy  0.8720511795281887
ep  611  training loss:  0.45837999180092215
valid loss  0.4948606797453405  and accuracy  0.8724510195921631
ep  612  training loss:  0.4558375136422634
valid loss  0.497141051892994  and accuracy  0.8720511795281887
ep  613  training loss:  0.45712930014986225
valid loss  0.4922564168278573  and accuracy  0.8724510195921631
ep  614  training loss:  0.45534894461932107
valid loss  0.5001533084967192  and accuracy  0.8720511795281887
ep  615  training loss:  0.45676485613475837
valid loss  0.4956065254752896  and accuracy  0.8716513394642144
ep  616  training loss:  0.45862308575273425
valid loss  0.4945087079308787  and accuracy  0.8720511795281887
ep  617  training loss:  0.4566100005244055
valid loss  0.5005737310502587  and accuracy  0.8724510195921631
ep  618  training loss:  0.4545629066115793
valid loss  0.4933584962354475  and accuracy  0.8736505397840864
ep  619  training loss:  0.45634582213278346
valid loss  0.4965017867870018  and accuracy  0.8708516593362655
ep  620  training loss:  0.4552455063570913
valid loss  0.4986981306015039  and accuracy  0.8704518192722911
ep  621  training loss:  0.45544529013849594
valid loss  0.4962451075468479  and accuracy  0.8708516593362655
ep  622  training loss:  0.4548133646649598
valid loss  0.49414404659498123  and accuracy  0.8736505397840864
ep  623  training loss:  0.455656596294744
valid loss  0.4950638785451853  and accuracy  0.8716513394642144
ep  624  training loss:  0.4571533311231586
valid loss  0.4904650771727518  and accuracy  0.8724510195921631
ep  625  training loss:  0.4550156692543183
valid loss  0.4947727145265932  and accuracy  0.8716513394642144
ep  626  training loss:  0.456874317435235
valid loss  0.4930061365975613  and accuracy  0.8720511795281887
ep  627  training loss:  0.4551484011726127
valid loss  0.5018377879746958  and accuracy  0.8716513394642144
ep  628  training loss:  0.45704577685930087
valid loss  0.4930285496837566  and accuracy  0.8736505397840864
ep  629  training loss:  0.45544167312077477
valid loss  0.4916736759313914  and accuracy  0.8712514994002399
ep  630  training loss:  0.4564662504669929
valid loss  0.4925265807430537  and accuracy  0.8740503798480608
ep  631  training loss:  0.45647736553760615
valid loss  0.4991374220885262  and accuracy  0.8724510195921631
ep  632  training loss:  0.45671517804587647
valid loss  0.49631807907539954  and accuracy  0.8720511795281887
ep  633  training loss:  0.4538136912845448
valid loss  0.49602573094774083  and accuracy  0.8720511795281887
ep  634  training loss:  0.4554276417246156
valid loss  0.49753082637451307  and accuracy  0.8708516593362655
ep  635  training loss:  0.45553522727223783
valid loss  0.49731638511625686  and accuracy  0.8724510195921631
ep  636  training loss:  0.456600909691579
valid loss  0.4990392752000686  and accuracy  0.8716513394642144
ep  637  training loss:  0.4544991356198565
valid loss  0.4931527344239611  and accuracy  0.8736505397840864
ep  638  training loss:  0.45562560864392276
valid loss  0.4961708320445511  and accuracy  0.8748500599760096
ep  639  training loss:  0.45705530295654967
valid loss  0.49753797389134935  and accuracy  0.8732506997201119
ep  640  training loss:  0.45532193597354825
valid loss  0.49455298668668063  and accuracy  0.8736505397840864
ep  641  training loss:  0.4571941155545441
valid loss  0.49658153733698096  and accuracy  0.8732506997201119
ep  642  training loss:  0.4548872318058748
valid loss  0.49620728088063937  and accuracy  0.8712514994002399
ep  643  training loss:  0.45440188245465996
valid loss  0.49606020141486784  and accuracy  0.8728508596561375
ep  644  training loss:  0.4562863506031344
valid loss  0.49430745978300117  and accuracy  0.8736505397840864
ep  645  training loss:  0.4556826865624641
valid loss  0.4923327865122986  and accuracy  0.8728508596561375
ep  646  training loss:  0.4552396226754974
valid loss  0.49646564812862315  and accuracy  0.8720511795281887
ep  647  training loss:  0.4556173626849078
valid loss  0.4984310400910208  and accuracy  0.8724510195921631
ep  648  training loss:  0.4548486771458352
valid loss  0.4984282524239679  and accuracy  0.8740503798480608
ep  649  training loss:  0.45649204336671834
valid loss  0.49470494285386735  and accuracy  0.8748500599760096
ep  650  training loss:  0.4563306205995249
valid loss  0.5016882127759362  and accuracy  0.8724510195921631
ep  651  training loss:  0.4550732081222686
valid loss  0.49441282630491046  and accuracy  0.8716513394642144
ep  652  training loss:  0.45510360310013664
valid loss  0.49477003977709605  and accuracy  0.8732506997201119
ep  653  training loss:  0.45476396881139697
valid loss  0.4970792861091571  and accuracy  0.8736505397840864
ep  654  training loss:  0.4573499570265703
valid loss  0.49388409761942087  and accuracy  0.8740503798480608
ep  655  training loss:  0.4570337857708938
valid loss  0.49728800219948033  and accuracy  0.8708516593362655
ep  656  training loss:  0.45613725623761375
valid loss  0.4935364724873829  and accuracy  0.8712514994002399
ep  657  training loss:  0.45592497451365593
valid loss  0.4960301199087092  and accuracy  0.8716513394642144
ep  658  training loss:  0.45655719060944555
valid loss  0.495185637381114  and accuracy  0.8732506997201119
ep  659  training loss:  0.454174020111753
valid loss  0.49448619905065316  and accuracy  0.8740503798480608
ep  660  training loss:  0.4530792341639066
valid loss  0.49900873572003696  and accuracy  0.8724510195921631
ep  661  training loss:  0.4529129601018134
valid loss  0.49399765490627634  and accuracy  0.8724510195921631
ep  662  training loss:  0.4580936566525016
valid loss  0.4985958018668982  and accuracy  0.8716513394642144
ep  663  training loss:  0.4535027140798049
valid loss  0.4953041621586267  and accuracy  0.8708516593362655
ep  664  training loss:  0.4555672350026759
valid loss  0.49390487397303345  and accuracy  0.8740503798480608
ep  665  training loss:  0.4575380319661194
valid loss  0.49374151606409133  and accuracy  0.8724510195921631
ep  666  training loss:  0.4562058156284686
valid loss  0.49826877833794997  and accuracy  0.8716513394642144
ep  667  training loss:  0.4539829225460086
valid loss  0.49592758304640944  and accuracy  0.8728508596561375
ep  668  training loss:  0.45379907811844716
valid loss  0.4930721837870839  and accuracy  0.8736505397840864
ep  669  training loss:  0.4550289854509027
valid loss  0.49742922454965155  and accuracy  0.8720511795281887
ep  670  training loss:  0.4547351909330769
valid loss  0.4930186895121102  and accuracy  0.8728508596561375
ep  671  training loss:  0.45603970843142433
valid loss  0.49628014516372865  and accuracy  0.8712514994002399
ep  672  training loss:  0.45595317127726687
valid loss  0.4969037104873169  and accuracy  0.8724510195921631
ep  673  training loss:  0.4574114360937726
valid loss  0.49715402039562595  and accuracy  0.8724510195921631
ep  674  training loss:  0.455362747697416
valid loss  0.4984809926799277  and accuracy  0.8728508596561375
ep  675  training loss:  0.45525234108584434
valid loss  0.49490543593410874  and accuracy  0.8716513394642144
ep  676  training loss:  0.45586886555561745
valid loss  0.49947082194410675  and accuracy  0.8708516593362655
ep  677  training loss:  0.4565252284492658
valid loss  0.5051510657657866  and accuracy  0.8700519792083167
ep  678  training loss:  0.4539523968218239
valid loss  0.5025317953830241  and accuracy  0.8716513394642144
ep  679  training loss:  0.45552776500983944
valid loss  0.49315603705989225  and accuracy  0.8744502199120352
ep  680  training loss:  0.45628541470861966
valid loss  0.4975673658568494  and accuracy  0.8720511795281887
ep  681  training loss:  0.455608371887491
valid loss  0.5012569299391487  and accuracy  0.8712514994002399
ep  682  training loss:  0.4551945831878605
valid loss  0.5015772121851562  and accuracy  0.8708516593362655
ep  683  training loss:  0.4536476566806611
valid loss  0.4948880652197358  and accuracy  0.8712514994002399
ep  684  training loss:  0.4539745214287073
valid loss  0.4997952746682432  and accuracy  0.8716513394642144
ep  685  training loss:  0.45563275776415035
valid loss  0.49492301203545835  and accuracy  0.8732506997201119
ep  686  training loss:  0.45513680639056897
valid loss  0.49993626725096935  and accuracy  0.8720511795281887
ep  687  training loss:  0.45370856780763036
valid loss  0.49367218573347943  and accuracy  0.8732506997201119
ep  688  training loss:  0.4574889499736075
valid loss  0.4964289700732332  and accuracy  0.8724510195921631
ep  689  training loss:  0.45561671450592484
valid loss  0.5041280432206924  and accuracy  0.8712514994002399
ep  690  training loss:  0.4559273453602275
valid loss  0.4947639283586721  and accuracy  0.8728508596561375
ep  691  training loss:  0.4552822768940309
valid loss  0.49549933537536983  and accuracy  0.8716513394642144
ep  692  training loss:  0.4531139706246362
valid loss  0.49226281890531676  and accuracy  0.8732506997201119
ep  693  training loss:  0.4565986254001673
valid loss  0.5009517609143629  and accuracy  0.8716513394642144
ep  694  training loss:  0.4557829263699135
valid loss  0.5021163655943224  and accuracy  0.8700519792083167
ep  695  training loss:  0.4555024025732661
valid loss  0.49312781992076826  and accuracy  0.8728508596561375
ep  696  training loss:  0.45328482461687536
valid loss  0.4953442094374637  and accuracy  0.8716513394642144
ep  697  training loss:  0.4545353923846771
valid loss  0.49663233652156813  and accuracy  0.8728508596561375
ep  698  training loss:  0.45559055756831657
valid loss  0.4957928086628393  and accuracy  0.8728508596561375
ep  699  training loss:  0.45574797292845776
valid loss  0.4926932676226461  and accuracy  0.875249900039984
ep  700  training loss:  0.4549859547913087
valid loss  0.4950018773575584  and accuracy  0.8720511795281887
ep  701  training loss:  0.4546927177125752
valid loss  0.49898018693027857  and accuracy  0.8720511795281887
ep  702  training loss:  0.45382698537307026
valid loss  0.4924845925954188  and accuracy  0.8744502199120352
ep  703  training loss:  0.4561639951498416
valid loss  0.498902144609857  and accuracy  0.8708516593362655
ep  704  training loss:  0.4547299829538386
valid loss  0.49592188409403387  and accuracy  0.8724510195921631
ep  705  training loss:  0.4565180161861555
valid loss  0.49956875869103884  and accuracy  0.8736505397840864
ep  706  training loss:  0.45497206350438
valid loss  0.49907527367671173  and accuracy  0.8704518192722911
ep  707  training loss:  0.4550390950092055
valid loss  0.49239832131875033  and accuracy  0.8748500599760096
ep  708  training loss:  0.4553897240684681
valid loss  0.498015591879932  and accuracy  0.8724510195921631
ep  709  training loss:  0.4536791238694331
valid loss  0.4976578910128682  and accuracy  0.8708516593362655
ep  710  training loss:  0.4563494527418313
valid loss  0.4960597713248151  and accuracy  0.8708516593362655
ep  711  training loss:  0.4574024817358385
valid loss  0.4936905418429933  and accuracy  0.8728508596561375
ep  712  training loss:  0.4565561958728481
valid loss  0.4955584750276525  and accuracy  0.8704518192722911
ep  713  training loss:  0.454740743977255
valid loss  0.49818875331108403  and accuracy  0.8716513394642144
ep  714  training loss:  0.45308804123379376
valid loss  0.49155691333982  and accuracy  0.8720511795281887
ep  715  training loss:  0.4531802675987519
valid loss  0.49068193403971955  and accuracy  0.8740503798480608
ep  716  training loss:  0.45444754958452305
valid loss  0.4951065330207944  and accuracy  0.8708516593362655
ep  717  training loss:  0.45506837860094884
valid loss  0.4929758031003144  and accuracy  0.8720511795281887
ep  718  training loss:  0.45568892030995706
valid loss  0.4941410282310225  and accuracy  0.8724510195921631
ep  719  training loss:  0.4555779781901281
valid loss  0.489219442683189  and accuracy  0.8760495801679328
ep  720  training loss:  0.45516879013587225
valid loss  0.5003354186203326  and accuracy  0.8696521391443423
ep  721  training loss:  0.45634893425206174
valid loss  0.4910602840672775  and accuracy  0.8724510195921631
ep  722  training loss:  0.45741455709421536
valid loss  0.4982636005389409  and accuracy  0.8720511795281887
ep  723  training loss:  0.45387723947211356
valid loss  0.5022820671383547  and accuracy  0.8708516593362655
ep  724  training loss:  0.4522254238291808
valid loss  0.4912875640730722  and accuracy  0.8720511795281887
ep  725  training loss:  0.4535130976262746
valid loss  0.4966098017475215  and accuracy  0.8740503798480608
ep  726  training loss:  0.45493409752601144
valid loss  0.4961723903330361  and accuracy  0.8716513394642144
ep  727  training loss:  0.45405890440999763
valid loss  0.49602362437278735  and accuracy  0.8728508596561375
ep  728  training loss:  0.4543064103598191
valid loss  0.4949761596692652  and accuracy  0.8740503798480608
ep  729  training loss:  0.4555648307132034
valid loss  0.4923341440491941  and accuracy  0.8708516593362655
ep  730  training loss:  0.45538312657264124
valid loss  0.4959717309269987  and accuracy  0.8732506997201119
ep  731  training loss:  0.4564994934992633
valid loss  0.4940913750476143  and accuracy  0.8736505397840864
ep  732  training loss:  0.4541720088749626
valid loss  0.49639580219757645  and accuracy  0.8708516593362655
ep  733  training loss:  0.45539649450737524
valid loss  0.4974439406051773  and accuracy  0.8712514994002399
ep  734  training loss:  0.453253560486768
valid loss  0.4935295741851689  and accuracy  0.8708516593362655
ep  735  training loss:  0.4547984788760682
valid loss  0.5001299816672681  and accuracy  0.8708516593362655
ep  736  training loss:  0.455844178649168
valid loss  0.49755192737205656  and accuracy  0.8712514994002399
ep  737  training loss:  0.4558916291420651
valid loss  0.49975230705971624  and accuracy  0.8704518192722911
ep  738  training loss:  0.4542397197601638
valid loss  0.5027648099705202  and accuracy  0.8712514994002399
ep  739  training loss:  0.4551871952621565
valid loss  0.5003521912386779  and accuracy  0.8732506997201119
ep  740  training loss:  0.4576571064207231
valid loss  0.5023677376378588  and accuracy  0.8724510195921631
ep  741  training loss:  0.4553761783818952
valid loss  0.49920617811968687  and accuracy  0.8696521391443423
ep  742  training loss:  0.4557153344244218
valid loss  0.500392352996088  and accuracy  0.8712514994002399
ep  743  training loss:  0.45303679812298464
valid loss  0.4985693639419118  and accuracy  0.8716513394642144
ep  744  training loss:  0.4558192714583889
valid loss  0.5006684891846789  and accuracy  0.8716513394642144
ep  745  training loss:  0.4547097918011694
valid loss  0.4951770555396311  and accuracy  0.8728508596561375
ep  746  training loss:  0.4547522971516174
valid loss  0.4981918535629114  and accuracy  0.8716513394642144
ep  747  training loss:  0.4555244629364816
valid loss  0.4992234076156182  and accuracy  0.8728508596561375
ep  748  training loss:  0.4539702694301924
valid loss  0.49859284183255487  and accuracy  0.8716513394642144
ep  749  training loss:  0.45435911650643473
valid loss  0.4952533459577595  and accuracy  0.8728508596561375
ep  750  training loss:  0.4541088035545076
valid loss  0.49796056244574466  and accuracy  0.8704518192722911
ep  751  training loss:  0.4512111742500784
valid loss  0.48873186223462123  and accuracy  0.8728508596561375
ep  752  training loss:  0.45415720468894577
valid loss  0.49224733847563196  and accuracy  0.8724510195921631
ep  753  training loss:  0.45392046840283046
valid loss  0.4970150470900469  and accuracy  0.8724510195921631
ep  754  training loss:  0.45270580680352657
valid loss  0.49695900566384393  and accuracy  0.8716513394642144
ep  755  training loss:  0.45378926386510293
valid loss  0.5002125277132189  and accuracy  0.8728508596561375
ep  756  training loss:  0.45565060485352366
valid loss  0.5002464604277651  and accuracy  0.8700519792083167
ep  757  training loss:  0.45568667598253737
valid loss  0.49213570427865994  and accuracy  0.8712514994002399
ep  758  training loss:  0.4540271201551677
valid loss  0.5002263259859097  and accuracy  0.8704518192722911
ep  759  training loss:  0.45429206012536283
valid loss  0.5001090909184955  and accuracy  0.8716513394642144
ep  760  training loss:  0.4552157808999589
valid loss  0.5015745299761413  and accuracy  0.8712514994002399
ep  761  training loss:  0.4531496341810878
valid loss  0.5009403895230734  and accuracy  0.8716513394642144
ep  762  training loss:  0.4538377118964826
valid loss  0.4984700332780401  and accuracy  0.8724510195921631
ep  763  training loss:  0.4545539254753617
valid loss  0.4950063930945795  and accuracy  0.8724510195921631
ep  764  training loss:  0.45618159924151935
valid loss  0.4929562071975066  and accuracy  0.8720511795281887
ep  765  training loss:  0.45396184262540845
valid loss  0.4978493836249222  and accuracy  0.8704518192722911
ep  766  training loss:  0.4566366686132157
valid loss  0.49779467055293286  and accuracy  0.8704518192722911
ep  767  training loss:  0.453481064299462
valid loss  0.5011194122881472  and accuracy  0.8716513394642144
ep  768  training loss:  0.45400577894661553
valid loss  0.497043273845514  and accuracy  0.8704518192722911
ep  769  training loss:  0.45453225783658957
valid loss  0.4985661862826929  and accuracy  0.8700519792083167
ep  770  training loss:  0.4513368333842248
valid loss  0.4969848959220023  and accuracy  0.8716513394642144
ep  771  training loss:  0.4526507136080801
valid loss  0.49474610565233973  and accuracy  0.8716513394642144
ep  772  training loss:  0.4531969359868675
valid loss  0.4943137093335807  and accuracy  0.8732506997201119
ep  773  training loss:  0.4520618193295539
valid loss  0.49250652405415857  and accuracy  0.8740503798480608
ep  774  training loss:  0.4539746666512221
valid loss  0.49168386168834544  and accuracy  0.8704518192722911
ep  775  training loss:  0.45098876951876943
valid loss  0.49879727946048447  and accuracy  0.8720511795281887
ep  776  training loss:  0.45334694800906467
valid loss  0.5025786367334018  and accuracy  0.8708516593362655
ep  777  training loss:  0.4551189785686798
valid loss  0.49944582530280585  and accuracy  0.8708516593362655
ep  778  training loss:  0.4531237804436385
valid loss  0.4959524045558702  and accuracy  0.8704518192722911
ep  779  training loss:  0.4557284274886135
valid loss  0.491549375556746  and accuracy  0.8744502199120352
ep  780  training loss:  0.4531195002827464
valid loss  0.497030586945634  and accuracy  0.8712514994002399
ep  781  training loss:  0.4553416674048953
valid loss  0.49593512830091735  and accuracy  0.8708516593362655
ep  782  training loss:  0.4531447836974242
valid loss  0.4972057909381147  and accuracy  0.8708516593362655
ep  783  training loss:  0.45062041733326724
valid loss  0.49461989397289563  and accuracy  0.8732506997201119
ep  784  training loss:  0.4535119129855198
valid loss  0.4974138686438648  and accuracy  0.8728508596561375
ep  785  training loss:  0.4548699020031454
valid loss  0.4942159858621249  and accuracy  0.8732506997201119
ep  786  training loss:  0.45483887718772115
valid loss  0.49322475002604166  and accuracy  0.8712514994002399
ep  787  training loss:  0.4531614556362583
valid loss  0.4977578625279586  and accuracy  0.8716513394642144
ep  788  training loss:  0.453950037950347
valid loss  0.49300673631418707  and accuracy  0.8720511795281887
ep  789  training loss:  0.45445352126652533
valid loss  0.4933689361164828  and accuracy  0.8724510195921631
ep  790  training loss:  0.4520463372595801
valid loss  0.4908936979745875  and accuracy  0.8708516593362655
ep  791  training loss:  0.4512830221254605
valid loss  0.49791955250780473  and accuracy  0.8740503798480608
ep  792  training loss:  0.45210153764553956
valid loss  0.4968246583793698  and accuracy  0.8732506997201119
ep  793  training loss:  0.4538190799785063
valid loss  0.4978826624638841  and accuracy  0.8712514994002399
ep  794  training loss:  0.45305693435745825
valid loss  0.49844514577687143  and accuracy  0.8712514994002399
ep  795  training loss:  0.45248040919343036
valid loss  0.49662815371021085  and accuracy  0.8720511795281887
ep  796  training loss:  0.45047164588843397
valid loss  0.4977526954534959  and accuracy  0.8724510195921631
ep  797  training loss:  0.45225838761148135
valid loss  0.4964463671461576  and accuracy  0.8716513394642144
ep  798  training loss:  0.4554955047565447
valid loss  0.4887394274844498  and accuracy  0.8720511795281887
ep  799  training loss:  0.4530385366171677
valid loss  0.49648630211516315  and accuracy  0.8708516593362655
ep  800  training loss:  0.45357025450555677
valid loss  0.49160120447651473  and accuracy  0.8720511795281887
ep  801  training loss:  0.4544994186102493
valid loss  0.5003825065876093  and accuracy  0.8704518192722911
ep  802  training loss:  0.4543322753160412
valid loss  0.49847199150868676  and accuracy  0.8716513394642144
ep  803  training loss:  0.4530297116102238
valid loss  0.5004190105716976  and accuracy  0.8708516593362655
ep  804  training loss:  0.4516849639183203
valid loss  0.49451249818809506  and accuracy  0.8728508596561375
ep  805  training loss:  0.45444767188025437
valid loss  0.5003226020249402  and accuracy  0.8712514994002399
ep  806  training loss:  0.4514424135983602
valid loss  0.4959337510904375  and accuracy  0.8712514994002399
ep  807  training loss:  0.4553041671272838
valid loss  0.4945826290345869  and accuracy  0.8732506997201119
ep  808  training loss:  0.45140665871917596
valid loss  0.49058577725764324  and accuracy  0.8740503798480608
ep  809  training loss:  0.4534578279083241
valid loss  0.4960638490832839  and accuracy  0.8732506997201119
ep  810  training loss:  0.4537478055610115
valid loss  0.4965149510840996  and accuracy  0.8720511795281887
ep  811  training loss:  0.4516445032712767
valid loss  0.4930325561954898  and accuracy  0.8740503798480608
ep  812  training loss:  0.4541433662958663
valid loss  0.4945123024174615  and accuracy  0.8708516593362655
ep  813  training loss:  0.45395791831515364
valid loss  0.49384534952403164  and accuracy  0.8724510195921631
ep  814  training loss:  0.453011507589397
valid loss  0.496529297726672  and accuracy  0.8708516593362655
ep  815  training loss:  0.4519626039799007
valid loss  0.4967299989584397  and accuracy  0.8720511795281887
ep  816  training loss:  0.4527476079647473
valid loss  0.4972717638780288  and accuracy  0.8700519792083167
ep  817  training loss:  0.45250754765696627
valid loss  0.4991693753497403  and accuracy  0.8708516593362655
ep  818  training loss:  0.45326412674485
valid loss  0.49288408628038194  and accuracy  0.8724510195921631
ep  819  training loss:  0.45212396141262556
valid loss  0.4919517639635659  and accuracy  0.8724510195921631
ep  820  training loss:  0.4533265324934198
valid loss  0.4949485762316625  and accuracy  0.8716513394642144
ep  821  training loss:  0.451513397831669
valid loss  0.4971371585609721  and accuracy  0.8712514994002399
ep  822  training loss:  0.45539047911846187
valid loss  0.49614666252792095  and accuracy  0.8712514994002399
ep  823  training loss:  0.4528527718661863
valid loss  0.4927939390454565  and accuracy  0.8724510195921631
ep  824  training loss:  0.4537014004508127
valid loss  0.493338814119204  and accuracy  0.8716513394642144
ep  825  training loss:  0.4535722934329593
valid loss  0.49549156078239864  and accuracy  0.8716513394642144
ep  826  training loss:  0.4535705638062121
valid loss  0.4979837944940394  and accuracy  0.8740503798480608
ep  827  training loss:  0.4527340525945101
valid loss  0.49459742219960007  and accuracy  0.8708516593362655
ep  828  training loss:  0.4503083082443356
valid loss  0.4929312647509127  and accuracy  0.8728508596561375
ep  829  training loss:  0.453124582328171
valid loss  0.491043704812501  and accuracy  0.8728508596561375
ep  830  training loss:  0.4522329951042536
valid loss  0.4949975276484293  and accuracy  0.8744502199120352
ep  831  training loss:  0.45279653342361553
valid loss  0.4981972377736871  and accuracy  0.8704518192722911
ep  832  training loss:  0.4536487296279179
valid loss  0.4934078696631089  and accuracy  0.8720511795281887
ep  833  training loss:  0.45421255809901634
valid loss  0.49861009555404445  and accuracy  0.8720511795281887
ep  834  training loss:  0.45094347894728404
valid loss  0.4945353107278893  and accuracy  0.8716513394642144
ep  835  training loss:  0.4533244511608283
valid loss  0.5033721218867953  and accuracy  0.8712514994002399
ep  836  training loss:  0.4518554926085819
valid loss  0.49276291251611537  and accuracy  0.8740503798480608
ep  837  training loss:  0.45394574087455575
valid loss  0.49396731735753424  and accuracy  0.8728508596561375
ep  838  training loss:  0.45354903776934513
valid loss  0.49011110329999774  and accuracy  0.8736505397840864
ep  839  training loss:  0.45261127201350343
valid loss  0.49211487077513205  and accuracy  0.8724510195921631
ep  840  training loss:  0.4531504238696198
valid loss  0.5001198959464981  and accuracy  0.8700519792083167
ep  841  training loss:  0.45065798569855825
valid loss  0.48996673563107257  and accuracy  0.8748500599760096
ep  842  training loss:  0.4534554241729884
valid loss  0.49786750995507484  and accuracy  0.8716513394642144
ep  843  training loss:  0.4520770102493467
valid loss  0.5012592048418136  and accuracy  0.8716513394642144
ep  844  training loss:  0.4521624594933074
valid loss  0.5022211594254624  and accuracy  0.8712514994002399
ep  845  training loss:  0.45206303840291784
valid loss  0.5022833960287955  and accuracy  0.8728508596561375
ep  846  training loss:  0.45125399222974627
valid loss  0.5007456852311566  and accuracy  0.8712514994002399
ep  847  training loss:  0.4535050219257721
valid loss  0.49535499432715163  and accuracy  0.8724510195921631
ep  848  training loss:  0.45212742782746373
valid loss  0.502305167667964  and accuracy  0.8716513394642144
ep  849  training loss:  0.4530265602495778
valid loss  0.5023548470550134  and accuracy  0.8712514994002399
ep  850  training loss:  0.45368038371303304
valid loss  0.49212720823354694  and accuracy  0.8732506997201119
ep  851  training loss:  0.4528246777501656
valid loss  0.49010490888550207  and accuracy  0.8744502199120352
ep  852  training loss:  0.44862657631904795
valid loss  0.4935146661126008  and accuracy  0.8716513394642144
ep  853  training loss:  0.45326127460680943
valid loss  0.4949171255465175  and accuracy  0.8732506997201119
ep  854  training loss:  0.4524372911636961
valid loss  0.4989282067705755  and accuracy  0.8704518192722911
ep  855  training loss:  0.4532134030398834
valid loss  0.4946198290417262  and accuracy  0.8712514994002399
ep  856  training loss:  0.45130965632955816
valid loss  0.4985230834138055  and accuracy  0.8720511795281887
ep  857  training loss:  0.4520815866412547
valid loss  0.4929893202945644  and accuracy  0.8728508596561375
ep  858  training loss:  0.45279696767461663
valid loss  0.4969337320075136  and accuracy  0.8736505397840864
ep  859  training loss:  0.45311960809853535
valid loss  0.4961215023611222  and accuracy  0.8724510195921631
ep  860  training loss:  0.4529307793720248
valid loss  0.5071925646254941  and accuracy  0.8704518192722911
ep  861  training loss:  0.4539140492866145
valid loss  0.4968163724162015  and accuracy  0.8712514994002399
ep  862  training loss:  0.4521107559697533
valid loss  0.4903069951447522  and accuracy  0.8732506997201119
ep  863  training loss:  0.4517457601667534
valid loss  0.4927189201414466  and accuracy  0.8728508596561375
ep  864  training loss:  0.4529359869106994
valid loss  0.4935806521126291  and accuracy  0.8724510195921631
ep  865  training loss:  0.4537614736618811
valid loss  0.49519156642267104  and accuracy  0.8712514994002399
ep  866  training loss:  0.45238092055701745
valid loss  0.4924298409627276  and accuracy  0.8736505397840864
ep  867  training loss:  0.4519471945505135
valid loss  0.4940662480077473  and accuracy  0.8736505397840864
ep  868  training loss:  0.45340110567567676
valid loss  0.49751821221088893  and accuracy  0.8704518192722911
ep  869  training loss:  0.4534859458147828
valid loss  0.4964085902180113  and accuracy  0.8724510195921631
ep  870  training loss:  0.45349990663588746
valid loss  0.4979710969530263  and accuracy  0.8716513394642144
ep  871  training loss:  0.4511350645776959
valid loss  0.4932487966393719  and accuracy  0.8712514994002399
ep  872  training loss:  0.45198049979586513
valid loss  0.4958236533038762  and accuracy  0.8704518192722911
ep  873  training loss:  0.45284998640053
valid loss  0.4979141937261198  and accuracy  0.8728508596561375
ep  874  training loss:  0.4520270414770566
valid loss  0.4983193728624464  and accuracy  0.8712514994002399
ep  875  training loss:  0.45190725660667563
valid loss  0.4962153033655389  and accuracy  0.8712514994002399
ep  876  training loss:  0.454323737597794
valid loss  0.49339975058579627  and accuracy  0.8744502199120352
ep  877  training loss:  0.4522322957387616
valid loss  0.4952135630389873  and accuracy  0.8728508596561375
ep  878  training loss:  0.4506306219709504
valid loss  0.4928135249029394  and accuracy  0.8724510195921631
ep  879  training loss:  0.4538902237270639
valid loss  0.4908495928134407  and accuracy  0.8732506997201119
ep  880  training loss:  0.45272913716077917
valid loss  0.49349868289235593  and accuracy  0.8724510195921631
ep  881  training loss:  0.45229665465208946
valid loss  0.4938777777277341  and accuracy  0.8720511795281887
ep  882  training loss:  0.45332937245293276
valid loss  0.4946707936107326  and accuracy  0.8712514994002399
ep  883  training loss:  0.45367069349271366
valid loss  0.4941374928128572  and accuracy  0.8728508596561375
ep  884  training loss:  0.45470640219417596
valid loss  0.49437813911853623  and accuracy  0.8728508596561375
ep  885  training loss:  0.45469475444464774
valid loss  0.49788112289569036  and accuracy  0.8712514994002399
ep  886  training loss:  0.4510763023156128
valid loss  0.49337217377357984  and accuracy  0.8728508596561375
ep  887  training loss:  0.45240835531397205
valid loss  0.49812059839074013  and accuracy  0.8736505397840864
ep  888  training loss:  0.4515902065762503
valid loss  0.4973734791209249  and accuracy  0.8724510195921631
ep  889  training loss:  0.4547209393724015
valid loss  0.4947154879546175  and accuracy  0.8732506997201119
ep  890  training loss:  0.45132944125257307
valid loss  0.4979805481738922  and accuracy  0.8712514994002399
ep  891  training loss:  0.45226091138980784
valid loss  0.4945121809083526  and accuracy  0.8716513394642144
ep  892  training loss:  0.45220746725412003
valid loss  0.4965392956253244  and accuracy  0.8728508596561375
ep  893  training loss:  0.45146905562651907
valid loss  0.49397692926785125  and accuracy  0.8720511795281887
ep  894  training loss:  0.45244682687097393
valid loss  0.4998920317985019  and accuracy  0.8716513394642144
ep  895  training loss:  0.45335795645703025
valid loss  0.4942847777013539  and accuracy  0.8724510195921631
ep  896  training loss:  0.4532244181749314
valid loss  0.4959649545628755  and accuracy  0.8708516593362655
ep  897  training loss:  0.4506156755190628
valid loss  0.5040333127317692  and accuracy  0.8720511795281887
ep  898  training loss:  0.4543973441883579
valid loss  0.49580590613409786  and accuracy  0.8724510195921631
ep  899  training loss:  0.4524112178570532
valid loss  0.49753113430006796  and accuracy  0.8724510195921631
ep  900  training loss:  0.4530638089516069
valid loss  0.4983453142409418  and accuracy  0.8724510195921631
ep  901  training loss:  0.45318981720188206
valid loss  0.4966729920490033  and accuracy  0.8728508596561375
ep  902  training loss:  0.45401390554532617
valid loss  0.5007878403003957  and accuracy  0.8708516593362655
ep  903  training loss:  0.45237024531122216
valid loss  0.49407171583423515  and accuracy  0.8732506997201119
ep  904  training loss:  0.4515190081054419
valid loss  0.49797169053282847  and accuracy  0.8720511795281887
ep  905  training loss:  0.45235600361767525
valid loss  0.4938607264737614  and accuracy  0.8724510195921631
ep  906  training loss:  0.4532934312040849
valid loss  0.5130094873671626  and accuracy  0.8712514994002399
ep  907  training loss:  0.4531378793708443
valid loss  0.5030699305561055  and accuracy  0.8708516593362655
ep  908  training loss:  0.4519990513115963
valid loss  0.4952636765318363  and accuracy  0.8728508596561375
ep  909  training loss:  0.45301519593100603
valid loss  0.5054781869000218  and accuracy  0.8704518192722911
ep  910  training loss:  0.4525416396466865
valid loss  0.4962872983884068  and accuracy  0.8708516593362655
ep  911  training loss:  0.44987782476145927
valid loss  0.4936331378751066  and accuracy  0.8736505397840864
ep  912  training loss:  0.4528716917755147
valid loss  0.4939809094806139  and accuracy  0.8724510195921631
ep  913  training loss:  0.4515398502539558
valid loss  0.4968271955448549  and accuracy  0.8724510195921631
ep  914  training loss:  0.45306357806754016
valid loss  0.4970057009220695  and accuracy  0.8712514994002399
ep  915  training loss:  0.45107171312489347
valid loss  0.5003258942223129  and accuracy  0.8720511795281887
ep  916  training loss:  0.4516938365435819
valid loss  0.4983736388161868  and accuracy  0.8732506997201119
ep  917  training loss:  0.45280035596116713
valid loss  0.49530721780825787  and accuracy  0.8720511795281887
ep  918  training loss:  0.45106380730599055
valid loss  0.49447896640475203  and accuracy  0.8724510195921631
ep  919  training loss:  0.4538106407917616
valid loss  0.49559637171323184  and accuracy  0.8720511795281887
ep  920  training loss:  0.4521225371501659
valid loss  0.49509759818683  and accuracy  0.8720511795281887
ep  921  training loss:  0.45116400188015676
valid loss  0.5004761056011555  and accuracy  0.8720511795281887
ep  922  training loss:  0.4513117749328375
valid loss  0.49683144608767976  and accuracy  0.8712514994002399
ep  923  training loss:  0.4499867993801142
valid loss  0.49726202343998316  and accuracy  0.8720511795281887
ep  924  training loss:  0.4526089924746879
valid loss  0.4968487786703327  and accuracy  0.8720511795281887
ep  925  training loss:  0.4523412964517654
valid loss  0.4967704297827988  and accuracy  0.8712514994002399
ep  926  training loss:  0.45076886795539495
valid loss  0.49559209788622544  and accuracy  0.8716513394642144
ep  927  training loss:  0.451696938981692
valid loss  0.4982057560066946  and accuracy  0.8728508596561375
ep  928  training loss:  0.45160140340200117
valid loss  0.4985731359197349  and accuracy  0.8732506997201119
ep  929  training loss:  0.4543204604955348
valid loss  0.4953589111816783  and accuracy  0.8728508596561375
ep  930  training loss:  0.45433175285620636
valid loss  0.49604312474372053  and accuracy  0.8724510195921631
ep  931  training loss:  0.4529430458114756
valid loss  0.492639745892834  and accuracy  0.8728508596561375
ep  932  training loss:  0.4528258957452094
valid loss  0.4982756731129798  and accuracy  0.8740503798480608
ep  933  training loss:  0.45232426488318134
valid loss  0.49280029609173787  and accuracy  0.8728508596561375
ep  934  training loss:  0.45093312972733873
valid loss  0.49443342191417045  and accuracy  0.875249900039984
ep  935  training loss:  0.44947661644220605
valid loss  0.4990979416615388  and accuracy  0.8716513394642144
ep  936  training loss:  0.45207180062641905
valid loss  0.4975516849973115  and accuracy  0.8712514994002399
ep  937  training loss:  0.4537767482226236
valid loss  0.4925152005218878  and accuracy  0.8720511795281887
ep  938  training loss:  0.45400979416400045
valid loss  0.4941175469442731  and accuracy  0.8732506997201119
ep  939  training loss:  0.4505811453519582
valid loss  0.49491881451955655  and accuracy  0.8728508596561375
ep  940  training loss:  0.45261096562331404
valid loss  0.49334816108556423  and accuracy  0.8720511795281887
ep  941  training loss:  0.4547321768635516
valid loss  0.4962422393980335  and accuracy  0.8724510195921631
ep  942  training loss:  0.4530553744737764
valid loss  0.49376563889319686  and accuracy  0.8728508596561375
ep  943  training loss:  0.45192817836117516
valid loss  0.4956590243693782  and accuracy  0.8712514994002399
ep  944  training loss:  0.4522391486379578
valid loss  0.4946694613241854  and accuracy  0.8732506997201119
ep  945  training loss:  0.45222290182309344
valid loss  0.491536107183885  and accuracy  0.8736505397840864
ep  946  training loss:  0.4524571434779754
valid loss  0.49493485421049555  and accuracy  0.8736505397840864
ep  947  training loss:  0.4529413819797295
valid loss  0.4871569528478663  and accuracy  0.8736505397840864
ep  948  training loss:  0.4506715060932821
valid loss  0.495780827068701  and accuracy  0.8724510195921631
ep  949  training loss:  0.4523563045076831
valid loss  0.4919205046281582  and accuracy  0.8736505397840864
ep  950  training loss:  0.45383071882814924
valid loss  0.49446690213627836  and accuracy  0.8724510195921631
ep  951  training loss:  0.45194151719873826
valid loss  0.49620368185399866  and accuracy  0.8712514994002399
ep  952  training loss:  0.454331211678032
valid loss  0.49533637696769134  and accuracy  0.8720511795281887
ep  953  training loss:  0.4521781270368409
valid loss  0.4963519965730062  and accuracy  0.8720511795281887
ep  954  training loss:  0.45146655031192884
valid loss  0.4937729237676382  and accuracy  0.8720511795281887
ep  955  training loss:  0.4530945052999843
valid loss  0.49469742971341735  and accuracy  0.8724510195921631
ep  956  training loss:  0.45191103498427193
valid loss  0.49464672480950783  and accuracy  0.875249900039984
ep  957  training loss:  0.4523096542032107
valid loss  0.49336495701192334  and accuracy  0.8744502199120352
ep  958  training loss:  0.45204340490275846
valid loss  0.4974981004693231  and accuracy  0.8724510195921631
ep  959  training loss:  0.4516471150262985
valid loss  0.4938795216223661  and accuracy  0.8716513394642144
ep  960  training loss:  0.4520886566547189
valid loss  0.4944986319456135  and accuracy  0.8724510195921631
ep  961  training loss:  0.45159340756583183
valid loss  0.49350324812911595  and accuracy  0.8724510195921631
ep  962  training loss:  0.45342354560508424
valid loss  0.4922146806951429  and accuracy  0.8732506997201119
ep  963  training loss:  0.4482929976003754
valid loss  0.4904267010999555  and accuracy  0.8724510195921631
ep  964  training loss:  0.45160687006200306
valid loss  0.48937836452657246  and accuracy  0.8724510195921631
ep  965  training loss:  0.45093792382259706
valid loss  0.4968083176742502  and accuracy  0.8708516593362655
ep  966  training loss:  0.4505640882057203
valid loss  0.49805783291236727  and accuracy  0.8720511795281887
ep  967  training loss:  0.453879417979571
valid loss  0.49781306115973717  and accuracy  0.8716513394642144
ep  968  training loss:  0.45362896637614236
valid loss  0.4947487948084774  and accuracy  0.8720511795281887
ep  969  training loss:  0.45313752894924936
valid loss  0.4981413720989647  and accuracy  0.8720511795281887
ep  970  training loss:  0.4515051566789166
valid loss  0.49287070972068175  and accuracy  0.8716513394642144
ep  971  training loss:  0.4503820360273496
valid loss  0.49851898489738167  and accuracy  0.8712514994002399
ep  972  training loss:  0.4513851300751206
valid loss  0.49400544962564596  and accuracy  0.8724510195921631
ep  973  training loss:  0.4495633367123459
valid loss  0.4925604450945757  and accuracy  0.8728508596561375
ep  974  training loss:  0.45392632960694096
valid loss  0.4984589330151957  and accuracy  0.8724510195921631
ep  975  training loss:  0.4518545545337138
valid loss  0.49966603334023446  and accuracy  0.8720511795281887
ep  976  training loss:  0.45527461808657066
valid loss  0.5029651883815298  and accuracy  0.8716513394642144
ep  977  training loss:  0.4544457165264066
valid loss  0.49907493641356476  and accuracy  0.8712514994002399
ep  978  training loss:  0.45504170385376136
valid loss  0.49520750247874484  and accuracy  0.8708516593362655
ep  979  training loss:  0.4547481478805683
valid loss  0.49766753905298994  and accuracy  0.8732506997201119
ep  980  training loss:  0.4528093298848707
valid loss  0.4967136418805128  and accuracy  0.8728508596561375
ep  981  training loss:  0.452587126343887
valid loss  0.4940452629306325  and accuracy  0.8760495801679328
ep  982  training loss:  0.45260681373384404
valid loss  0.4991143301623862  and accuracy  0.8736505397840864
ep  983  training loss:  0.45036085441891405
valid loss  0.4967421641925581  and accuracy  0.8728508596561375
ep  984  training loss:  0.4525074327934067
valid loss  0.4981141747211943  and accuracy  0.8732506997201119
ep  985  training loss:  0.45262729364167537
valid loss  0.4964714226890497  and accuracy  0.8716513394642144
ep  986  training loss:  0.45121439035611066
valid loss  0.4989158342309782  and accuracy  0.8724510195921631
ep  987  training loss:  0.449856389958713
valid loss  0.49531635897057574  and accuracy  0.8724510195921631
ep  988  training loss:  0.4521197961065566
valid loss  0.49359723843368997  and accuracy  0.8728508596561375
ep  989  training loss:  0.45253664997236137
valid loss  0.49789040762393394  and accuracy  0.8712514994002399
ep  990  training loss:  0.4534478821685663
valid loss  0.49733707982032405  and accuracy  0.8720511795281887
ep  991  training loss:  0.4521472401228366
valid loss  0.4950279589297056  and accuracy  0.8720511795281887
ep  992  training loss:  0.4516650945768169
valid loss  0.4976675627065725  and accuracy  0.8732506997201119
ep  993  training loss:  0.4514028481117889
valid loss  0.49879605383550774  and accuracy  0.8720511795281887
ep  994  training loss:  0.4521916524412604
valid loss  0.4997271736923669  and accuracy  0.8720511795281887
ep  995  training loss:  0.451091259510764
valid loss  0.4937821536457858  and accuracy  0.8720511795281887
ep  996  training loss:  0.4520116823555472
valid loss  0.49314497222379894  and accuracy  0.8724510195921631
ep  997  training loss:  0.4532108924223044
valid loss  0.49431864677167614  and accuracy  0.8720511795281887
ep  998  training loss:  0.4514178116083734
valid loss  0.49596426508179764  and accuracy  0.8708516593362655
ep  999  training loss:  0.4508143516073395
valid loss  0.49109368689391003  and accuracy  0.8724510195921631

True positive:  [  167   139   145    27 43596]
True negative:  [48431 48020 48679 49095   635]
False positive:  [  38   41  106   29 5974]
False negative:  [1626 2062 1332 1111   57]

[[  167     7    60     5  1554]
 [    8   139    20     5  2029]
 [   18     3   145     3  1308]
 [    5    17     6    27  1083]
 [    7    14    20    16 43596]]

              precision    recall  f1-score   support

           0       0.81      0.09      0.17      1793
           1       0.77      0.06      0.12      2201
           2       0.58      0.10      0.17      1477
           3       0.48      0.02      0.05      1138
           4       0.88      1.00      0.94     43653

    accuracy                           0.88     50262
   macro avg       0.71      0.26      0.29     50262
weighted avg       0.85      0.88      0.83     50262

Accuracy:  0.8768851219609247
Precision_weighted:  0.8546082826513526
Recall_weighted:  0.8768851219609247
mcc:  0.24481424834684323
f2:  0.8723373264761065

time = 8 min 7 sec

- train_loop(model, epochs=500, lr=0.00001, wd=0.00000001)

[[    1     0     0     0  1792]
 [    0    13     0     1  2187]
 [    0     0    17     1  1459]
 [    0     0     0     1  1137]
 [    0     0     0     0 43653]]

Accuracy:  0.8691456766543313
Precision_weighted:  0.8712152540115878
Recall_weighted:  0.8691456766543313
mcc:  0.06650185840960089
f2:  0.8695588051448265

time = 4 min 36 sec

- train_loop(model, epochs=500, lr=0.01)

True positive:  [  174    93   122    33 43580]
True negative:  [48426 48035 48673 49076   578]
False positive:  [  43   26  112   48 6031]
False negative:  [1619 2108 1355 1105   73]

[[  174     1    64     5  1549]
 [    9    93    19     9  2071]
 [   16     9   122     4  1326]
 [    3    11     6    33  1085]
 [   15     5    23    30 43580]]

              precision    recall  f1-score   support

           0       0.80      0.10      0.17      1793
           1       0.78      0.04      0.08      2201
           2       0.52      0.08      0.14      1477
           3       0.41      0.03      0.05      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.88     50262
   macro avg       0.68      0.25      0.28     50262
weighted avg       0.85      0.88      0.83     50262

Accuracy:  0.8754526282280848
Precision_weighted:  0.8503002805174904
Recall_weighted:  0.8754526282280848
mcc:  0.22568199910722478
f2:  0.8703038155827281

- train_loop(model, epochs=1000, lr=0.01)

True positive:  [  187   113   155    31 43566]
True negative:  [48420 48041 48638 49091   648]
False positive:  [  49   20  147   33 5961]
False negative:  [1606 2088 1322 1107   87]

[[  187     2    70     3  1531]
 [   10   113    28     5  2045]
 [   22     3   155     1  1296]
 [    2    10     6    31  1089]
 [   15     5    43    24 43566]]

              precision    recall  f1-score   support

           0       0.79      0.10      0.18      1793
           1       0.85      0.05      0.10      2201
           2       0.51      0.10      0.17      1477
           3       0.48      0.03      0.05      1138
           4       0.88      1.00      0.94     43653

    accuracy                           0.88     50262
   macro avg       0.70      0.26      0.29     50262
weighted avg       0.86      0.88      0.83     50262

Accuracy:  0.876447415542557
Precision_weighted:  0.8554974964712859
Recall_weighted:  0.876447415542557
mcc:  0.2411079499438726
f2:  0.8721757463960568

time = 7 min 58 sec

- train_loop(model, epochs=1000, lr=0.001)

True positive:  [  188   133   123    26 43613]
True negative:  [48435 48042 48698 49090   604]
False positive:  [  34   19   87   34 6005]
False negative:  [1605 2068 1354 1112   40]

[[  188     2    52     6  1545]
 [    4   133    16     7  2041]
 [   20     6   123     1  1327]
 [    3    11     6    26  1092]
 [    7     0    13    20 43613]]

              precision    recall  f1-score   support

           0       0.85      0.10      0.19      1793
           1       0.88      0.06      0.11      2201
           2       0.59      0.08      0.15      1477
           3       0.43      0.02      0.04      1138
           4       0.88      1.00      0.94     43653

    accuracy                           0.88     50262
   macro avg       0.72      0.25      0.28     50262
weighted avg       0.86      0.88      0.83     50262

Accuracy:  0.8770641836775297
Precision_weighted:  0.8589474481059045
Recall_weighted:  0.8770641836775297
mcc:  0.24582421935328685
f2:  0.8733799550940724

time = 7 min 58 sec

- train_loop(model, epochs=100, lr=0.001, wd = 0.1)

True positive:  [    0     0     0     0 43653]
True negative:  [48469 48061 48785 49124     0]
False positive:  [   0    0    0    0 6609]
False negative:  [1793 2201 1477 1138    0]

- train_loop(model, epochs=100, lr=0.001, wd = 0.01)

True positive:  [    0     3     5     0 43605]
True negative:  [48469 48057 48717 49124    32]
False positive:  [   0    4   68    0 6577]
False negative:  [1793 2198 1472 1138   48]

[[    0     1     2     0  1790]
 [    0     3    13     0  2185]
 [    0     3     5     0  1469]
 [    0     0     5     0  1133]
 [    0     0    48     0 43605]]

- train_loop(model, epochs=100, lr=0.001, wd = 0.001)

[[  118     0     9     1  1665]
 [    4    38     3     0  2156]
 [    2     3    65     0  1407]
 [    1     5     2     2  1128]
 [   11     5    15     7 43615]]


           0       0.87      0.07      0.12      1793
           1       0.75      0.02      0.03      2201
           2       0.69      0.04      0.08      1477
           3       0.20      0.00      0.00      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.68      0.23      0.23     50262
weighted avg       0.85      0.87      0.82     50262

Accuracy:  0.8721897258366161
Precision_weighted:  0.8464683459067615
Recall_weighted:  0.8721897258366161
mcc:  0.16134343238879115
f2:  0.8669211511977624

- train_loop(model, epochs=100, lr=0.001, wd = 0.0001)   best wd

True positive:  [  112    55    94    10 43607]
True negative:  [48462 48054 48690 49115   343]
False positive:  [   7    7   95    9 6266]
False negative:  [1681 2146 1383 1128   46]

[[  112     1    35     1  1644]
 [    1    55    18     4  2123]
 [    2     2    94     0  1379]
 [    0     1     7    10  1120]
 [    4     3    35     4 43607]]

              precision    recall  f1-score   support

           0       0.94      0.06      0.12      1793
           1       0.89      0.02      0.05      2201
           2       0.50      0.06      0.11      1477
           3       0.53      0.01      0.02      1138
           4       0.87      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.75      0.23      0.25     50262
weighted avg       0.86      0.87      0.82     50262

Accuracy:  0.8729855556881939
Precision_weighted:  0.8583431761601349
Recall_weighted:  0.8729855556881939
mcc:  0.179556216746809
f2:  0.8700172504390333

time = 44 sec

- train_loop(model, epochs=500, lr=0.001, wd = 0.0001)

True positive:  [  149    98    84    25 43589]
True negative:  [48407 48040 48728 49091   465]
False positive:  [  62   21   57   33 6144]
False negative:  [1644 2103 1393 1113   64]


print(cm)
[[  149     3    28     4  1609]
 [   10    98     7     6  2080]
 [   31     5    84     1  1356]
 [    3     9     2    25  1099]
 [   18     4    20    22 43589]]

              precision    recall  f1-score   support

           0       0.71      0.08      0.15      1793
           1       0.82      0.04      0.08      2201
           2       0.60      0.06      0.10      1477
           3       0.43      0.02      0.04      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.69      0.24      0.26     50262
weighted avg       0.85      0.87      0.82     50262

Accuracy:  0.8743185706895865
Precision_weighted:  0.8497331717387846
Recall_weighted:  0.8743185706895865
mcc:  0.20546972184258616
f2:  0.8692883326870716

- train_loop(model, epochs=1000, lr=0.001, wd = 0.0001)

True positive:  [  112   103   105    28 43606]
True negative:  [48447 48047 48715 49092   439]
False positive:  [  22   14   70   32 6170]
False negative:  [1681 2098 1372 1110   47]

[[  112     1    38     3  1639]
 [    2   103    10     7  2079]
 [   12     3   105     0  1357]
 [    1    10     4    28  1095]
 [    7     0    18    22 43606]]

              precision    recall  f1-score   support

           0       0.84      0.06      0.12      1793
           1       0.88      0.05      0.09      2201
           2       0.60      0.07      0.13      1477
           3       0.47      0.02      0.05      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.87     50262
   macro avg       0.73      0.24      0.26     50262
weighted avg       0.86      0.87      0.82     50262

Accuracy:  0.8744976324061916
Precision_weighted:  0.8574172224405583
Recall_weighted:  0.8744976324061916
mcc:  0.20653864473363426
f2:  0.8710273256700228

- train_loop(model, epochs=1500, lr=0.001, wd = 0.0001)

True positive:  [  191    89   123    19 43572]
True negative:  [48405 48022 48678 49105   570]
False positive:  [  64   39  107   19 6039]
False negative:  [1602 2112 1354 1119   81]

[[  191     3    52     3  1544]
 [   15    89    17     7  2073]
 [   24     8   123     0  1322]
 [    1    13     5    19  1100]
 [   24    15    33     9 43572]]

              precision    recall  f1-score   support

           0       0.75      0.11      0.19      1793
           1       0.70      0.04      0.08      2201
           2       0.53      0.08      0.14      1477
           3       0.50      0.02      0.03      1138
           4       0.88      1.00      0.93     43653

    accuracy                           0.88     50262
   macro avg       0.67      0.25      0.27     50262
weighted avg       0.85      0.88      0.83     50262

Accuracy:  0.8752934622577693
Precision_weighted:  0.846991734609373
Recall_weighted:  0.8752934622577693
mcc:  0.22351584061052596
f2:  0.8694828115226649

time = 12 min 13 sec

- train con 2000 epoche le prestazioni diminuiscono