{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CTU_TabNet_PT(NoEmbed).ipynb","provenance":[],"mount_file_id":"1PwlcDmIyUfwqM9IC6cdL5eMMecEWpfrZ","authorship_tag":"ABX9TyNvad+lyLmCFkh/aYmScTXe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vUXlA8uDMVBr"},"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as torch_optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8txsRgSNCZ6","executionInfo":{"status":"ok","timestamp":1624966629494,"user_tz":-120,"elapsed":49,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8169e479-4d80-4038-a204-e9755d5dd68f"},"source":["if torch.cuda.is_available():\n","  print(torch.cuda.device_count())            # Numero di GPU disponibili\n","  print(torch.cuda.get_device_name(0))        # Nome della prima GPU disponibile\n","  print(torch.cuda.current_device())        # Device in uso al momento\n","  print(torch.cuda.set_device(0))             # Imposta la prima GPU come default\n","  print(torch.cuda.get_device_capability(0))  # Verifica le capacità della prima GPU"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","Tesla T4\n","0\n","None\n","(7, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QnmjI_Lj04E1"},"source":["### ***DATASET & PRE-ELABORAZIONE***"]},{"cell_type":"code","metadata":{"id":"3ykUAwuEN3d6"},"source":["# Carico il dataset dal drive\n","path = './drive/MyDrive/Materiale_Pellegrino_personal/CTU_Shuffled/CTU_Shuffled.csv'\n","dataset = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uykMb0MqKssU"},"source":["# Caratteristica temporale inutile \n","dataset = dataset.drop('StartTime', axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"6n5u-zIVgKJD","executionInfo":{"status":"ok","timestamp":1624966629498,"user_tz":-120,"elapsed":43,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"d1a68a0a-88d3-448d-9526-e40214cbead8"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","      <th>multilabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000540</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>131</td>\n","      <td>71</td>\n","      <td>flow=Background-UDP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.014909</td>\n","      <td>2</td>\n","      <td>-&gt;</td>\n","      <td>SRPA_FSPA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11</td>\n","      <td>2882</td>\n","      <td>1504</td>\n","      <td>flow=Background-TCP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000798</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>244</td>\n","      <td>182</td>\n","      <td>flow=Background-UDP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15.302759</td>\n","      <td>1</td>\n","      <td>-&gt;</td>\n","      <td>INT</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>336</td>\n","      <td>336</td>\n","      <td>flow=Background-UDP-Attempt</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.843942</td>\n","      <td>2</td>\n","      <td>-&gt;</td>\n","      <td>FSPA_SRPA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>93</td>\n","      <td>11846</td>\n","      <td>4562</td>\n","      <td>flow=Background-TCP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31468</th>\n","      <td>8.917003</td>\n","      <td>2</td>\n","      <td>-&gt;</td>\n","      <td>S_SA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8</td>\n","      <td>512</td>\n","      <td>194</td>\n","      <td>flow=Background-TCP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31469</th>\n","      <td>0.038780</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>191</td>\n","      <td>68</td>\n","      <td>flow=To-Background-UDP-CVUT-DNS-Server</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31470</th>\n","      <td>603.106201</td>\n","      <td>1</td>\n","      <td>-&gt;</td>\n","      <td>INT</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>6</td>\n","      <td>552</td>\n","      <td>552</td>\n","      <td>flow=Background-Attempt-cmpgw-CVUT</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31471</th>\n","      <td>0.000550</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>128</td>\n","      <td>60</td>\n","      <td>flow=Background-UDP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31472</th>\n","      <td>0.000186</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>flow=To-Background-UDP-CVUT-DNS-Server</td>\n","      <td>normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31473 rows × 11 columns</p>\n","</div>"],"text/plain":["              Dur  Proto  ...                                 Details multilabel\n","0        0.000540      1  ...         flow=Background-UDP-Established     normal\n","1        0.014909      2  ...         flow=Background-TCP-Established     normal\n","2        0.000798      1  ...         flow=Background-UDP-Established     normal\n","3       15.302759      1  ...             flow=Background-UDP-Attempt     normal\n","4        7.843942      2  ...         flow=Background-TCP-Established     normal\n","...           ...    ...  ...                                     ...        ...\n","31468    8.917003      2  ...         flow=Background-TCP-Established     normal\n","31469    0.038780      1  ...  flow=To-Background-UDP-CVUT-DNS-Server     normal\n","31470  603.106201      1  ...      flow=Background-Attempt-cmpgw-CVUT     normal\n","31471    0.000550      1  ...         flow=Background-UDP-Established     normal\n","31472    0.000186      1  ...  flow=To-Background-UDP-CVUT-DNS-Server     normal\n","\n","[31473 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mss4AVU-pWtg","executionInfo":{"status":"ok","timestamp":1624966629500,"user_tz":-120,"elapsed":42,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"2ab164ee-e289-4e02-d388-3f17a67e3222"},"source":["print(Counter(dataset['multilabel']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counter({'normal': 31406, 'BotNet': 67})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bqGImh5iNT_u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624966629501,"user_tz":-120,"elapsed":37,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"bbe43eef-9aa0-432b-94e8-ff6e63573da1"},"source":["dep_var = 'multilabel'\n","\n","cont_names = [col for col in dataset.columns if col != dep_var]\n","\n","print(cont_names, 'len: ', len(cont_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Dur', 'Proto', 'Dir', 'State', 'sTos', 'dTos', 'TotPkts', 'TotBytes', 'SrcBytes', 'Details'] len:  10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5pqCKXOoU-mA"},"source":["# LabelEncoding della variabile target \n","target_index = dataset.columns.get_loc(dep_var)\n","dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[dep_var])\n","\n","#LabelEncoding delle variabili categoriali\n","for col in [\"Dir\", \"State\", \"Details\"]:\n","  target_index = dataset.columns.get_loc(col)\n","  dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[col])\n","\n","# Fill NaN\n","\"\"\" Eliminiamo dalle colonne i valori nan \"\"\" \n","for col in dataset.columns:\n","  dataset[col] = dataset[col].fillna(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"fgf6kpUyNHjP","executionInfo":{"status":"ok","timestamp":1624966629505,"user_tz":-120,"elapsed":34,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"b0c47ece-7501-448c-ebf5-01354dd2b365"},"source":["dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","      <th>multilabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000540</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>131</td>\n","      <td>71</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.014909</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>89</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11</td>\n","      <td>2882</td>\n","      <td>1504</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000798</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>244</td>\n","      <td>182</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15.302759</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>336</td>\n","      <td>336</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.843942</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>43</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>93</td>\n","      <td>11846</td>\n","      <td>4562</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Dur  Proto  Dir  State  ...  TotBytes  SrcBytes  Details  multilabel\n","0   0.000540      1    3      4  ...       131        71        6           1\n","1   0.014909      2    0     89  ...      2882      1504        4           1\n","2   0.000798      1    3      4  ...       244       182        6           1\n","3  15.302759      1    0     53  ...       336       336        5           1\n","4   7.843942      2    0     43  ...     11846      4562        4           1\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"GL5xD37fnaia"},"source":["from sklearn.model_selection import train_test_split\n","\n","# train 50% e test 50%\n","train, test = train_test_split(dataset, test_size=0.50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ0TyYvfQLgW"},"source":["y_train = train[dep_var]\n","train = train.drop(dep_var, axis=1)\n","y_test = test[dep_var]\n","test = test.drop(dep_var, axis=1)\n","\n","# validation di 2500 righe da train\n","train, validation, y_train, y_val = train_test_split(train, y_train, test_size=(2500/len(train)), random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNTwI2TaaXlY","executionInfo":{"status":"ok","timestamp":1624966629925,"user_tz":-120,"elapsed":449,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"586f0884-424f-4e15-fc19-a6a1b4d3f073"},"source":["\"\"\"Visto che nel dataset la variabile target è molto squilibrata lo amplio con una generazione\n"," randomica di dati mediante la tecnica chiamata Synthetic Minority Over-sampling Technique (SMOTE)\"\"\"\n"," \n","from imblearn.over_sampling import SMOTE\n","sm = SMOTE(random_state=0)\n","x_sm, y_train = sm.fit_resample(train, y_train)\n","train = pd.DataFrame(x_sm,columns=train.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"k5xEsk56PSsW"},"source":["#y_train = y_train.values\n","y_test = y_test.values\n","y_val = y_val.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"IKp_gSloQTZO","executionInfo":{"status":"ok","timestamp":1624966629927,"user_tz":-120,"elapsed":21,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"66c33f71-2c30-491b-99e8-ad4518e36e71"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>30212</th>\n","      <td>316.277405</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>79</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32</td>\n","      <td>2880</td>\n","      <td>1238</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3445</th>\n","      <td>1.860478</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>26</td>\n","      <td>20385</td>\n","      <td>20013</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>19724</th>\n","      <td>0.000267</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>70</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>10197</th>\n","      <td>0.000260</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>9543</th>\n","      <td>0.000329</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11155</th>\n","      <td>0.583620</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>67</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>140</td>\n","      <td>140</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>29942</th>\n","      <td>0.000365</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>372</td>\n","      <td>87</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>13993</th>\n","      <td>0.000265</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>24409</th>\n","      <td>0.000317</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>4469</th>\n","      <td>15.137527</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>37</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>24</td>\n","      <td>12339</td>\n","      <td>3295</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15737 rows × 10 columns</p>\n","</div>"],"text/plain":["              Dur  Proto  Dir  State  ...  TotPkts  TotBytes  SrcBytes  Details\n","30212  316.277405      2    0     79  ...       32      2880      1238        2\n","3445     1.860478      1    3      4  ...       26     20385     20013        6\n","19724    0.000267      1    3      4  ...        2       214        70       46\n","10197    0.000260      1    3      4  ...        2       214        81       46\n","9543     0.000329      1    3      4  ...        2       214        81       46\n","...           ...    ...  ...    ...  ...      ...       ...       ...      ...\n","11155    0.583620      3    0     67  ...        2       140       140       42\n","29942    0.000365      1    3      4  ...        2       372        87       46\n","13993    0.000265      1    3      4  ...        2       214        81       46\n","24409    0.000317      1    3      4  ...        2       214        81       46\n","4469    15.137527      2    0     37  ...       24     12339      3295        2\n","\n","[15737 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"79A_sfwnQVqX","executionInfo":{"status":"ok","timestamp":1624966629927,"user_tz":-120,"elapsed":19,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"935855e0-110f-4a41-c81f-ece1e719050b"},"source":["train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000295</td>\n","      <td>1.0</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>409.000000</td>\n","      <td>85.000000</td>\n","      <td>46.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.002029</td>\n","      <td>1.0</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>550.000000</td>\n","      <td>490.000000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000755</td>\n","      <td>1.0</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>137.000000</td>\n","      <td>77.000000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000361</td>\n","      <td>1.0</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>314.000000</td>\n","      <td>85.000000</td>\n","      <td>46.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000223</td>\n","      <td>1.0</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>214.000000</td>\n","      <td>81.000000</td>\n","      <td>46.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>26403</th>\n","      <td>58.621110</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>87.277184</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.171739</td>\n","      <td>1993.572960</td>\n","      <td>832.865276</td>\n","      <td>27.085869</td>\n","    </tr>\n","    <tr>\n","      <th>26404</th>\n","      <td>4.308502</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>37.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.955859</td>\n","      <td>962.162899</td>\n","      <td>520.162899</td>\n","      <td>28.000000</td>\n","    </tr>\n","    <tr>\n","      <th>26405</th>\n","      <td>133.266316</td>\n","      <td>2.0</td>\n","      <td>1.505258</td>\n","      <td>58.508004</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.065845</td>\n","      <td>22105.731709</td>\n","      <td>4268.825780</td>\n","      <td>16.839507</td>\n","    </tr>\n","    <tr>\n","      <th>26406</th>\n","      <td>15.127515</td>\n","      <td>2.0</td>\n","      <td>0.763596</td>\n","      <td>21.749171</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.654828</td>\n","      <td>1503.361819</td>\n","      <td>1360.100978</td>\n","      <td>6.382909</td>\n","    </tr>\n","    <tr>\n","      <th>26407</th>\n","      <td>74.926363</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>92.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>89.469903</td>\n","      <td>69661.231481</td>\n","      <td>4241.499685</td>\n","      <td>27.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>26408 rows × 10 columns</p>\n","</div>"],"text/plain":["              Dur  Proto       Dir  ...      TotBytes     SrcBytes    Details\n","0        0.000295    1.0  3.000000  ...    409.000000    85.000000  46.000000\n","1        0.002029    1.0  3.000000  ...    550.000000   490.000000   6.000000\n","2        0.000755    1.0  3.000000  ...    137.000000    77.000000   6.000000\n","3        0.000361    1.0  3.000000  ...    314.000000    85.000000  46.000000\n","4        0.000223    1.0  3.000000  ...    214.000000    81.000000  46.000000\n","...           ...    ...       ...  ...           ...          ...        ...\n","26403   58.621110    2.0  0.000000  ...   1993.572960   832.865276  27.085869\n","26404    4.308502    2.0  0.000000  ...    962.162899   520.162899  28.000000\n","26405  133.266316    2.0  1.505258  ...  22105.731709  4268.825780  16.839507\n","26406   15.127515    2.0  0.763596  ...   1503.361819  1360.100978   6.382909\n","26407   74.926363    2.0  0.000000  ...  69661.231481  4241.499685  27.000000\n","\n","[26408 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"va0f2YIwqfM4","executionInfo":{"status":"ok","timestamp":1624966629929,"user_tz":-120,"elapsed":20,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"75fcfb4d-1a64-4837-bbcf-7e069d72efa8"},"source":["validation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17177</th>\n","      <td>0.000324</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>328</td>\n","      <td>85</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>26319</th>\n","      <td>304.167114</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10</td>\n","      <td>670</td>\n","      <td>370</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>10767</th>\n","      <td>0.004151</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>240</td>\n","      <td>73</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>22412</th>\n","      <td>0.000152</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>21934</th>\n","      <td>0.000342</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>284</td>\n","      <td>77</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>25712</th>\n","      <td>483.011200</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10</td>\n","      <td>4190</td>\n","      <td>666</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2375</th>\n","      <td>0.003502</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>37</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9</td>\n","      <td>1908</td>\n","      <td>1049</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>23265</th>\n","      <td>0.003895</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>2989</td>\n","      <td>60</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>15918</th>\n","      <td>0.000307</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>27471</th>\n","      <td>0.033345</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>89</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12</td>\n","      <td>5663</td>\n","      <td>1066</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2500 rows × 10 columns</p>\n","</div>"],"text/plain":["              Dur  Proto  Dir  State  ...  TotPkts  TotBytes  SrcBytes  Details\n","17177    0.000324      1    3      4  ...        2       328        85       46\n","26319  304.167114      1    3      4  ...       10       670       370        6\n","10767    0.004151      1    3      4  ...        2       240        73       46\n","22412    0.000152      1    3      4  ...        2       214        81       46\n","21934    0.000342      1    3      4  ...        2       284        77       46\n","...           ...    ...  ...    ...  ...      ...       ...       ...      ...\n","25712  483.011200      1    3      4  ...       10      4190       666        6\n","2375     0.003502      2    0     37  ...        9      1908      1049        2\n","23265    0.003895      1    3      4  ...        4      2989        60        6\n","15918    0.000307      1    3      4  ...        2       214        81       46\n","27471    0.033345      2    0     89  ...       12      5663      1066        2\n","\n","[2500 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"s8278UqL0KrR"},"source":["### ***GPU/CPU***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR0qF0wEZ-hQ","executionInfo":{"status":"ok","timestamp":1624966629929,"user_tz":-120,"elapsed":18,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"aacd78bc-6a4d-4f59-e96c-b49d6cdef763"},"source":["\"\"\" Making device (GPU/CPU) compatible\n","\n","(borrowed from https://jovian.ml/aakashns/04-feedforward-nn)\n","\n","In order to make use of a GPU if available, we'll have to move our data and model to it. \"\"\" \n","\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"6yqANTackmiK"},"source":["### ***MODEL***"]},{"cell_type":"code","metadata":{"id":"XkHbqCF12vWx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624966632202,"user_tz":-120,"elapsed":2287,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"f2104372-0118-41e5-ac65-12ece63c7c35"},"source":["\"\"\" Ghost Batch Normalization (GBN):\n"," Questa tenica ci consente di operare su grandi batch di dati e al tempo stesso ottenere buone generalizzazioni.\n"," In pratica: viene diviso il batch di in input in sotto-batch di dimensioni uguali (dimensione del batch \n"," virtuale) e viene applicato lo stesso livello di Batch Normalization. \n"," Tutti i layer di normalizzazione del modello, eccetto il primo, adottano questa tecnica. \"\"\"\n","\n","class GBN(nn.Module):\n","  def __init__(self,inp,vbs=128,momentum=1.0):\n","        super().__init__()\n","        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n","        self.vbs = vbs\n","        \n","  def forward(self,x):\n","        chunk = torch.chunk(x,x.size(0)//self.vbs,0)\n","        res = [self.bn(y) for y in chunk]\n","        return torch.cat(res,0)\n","\n","\"\"\" SparseMax: \n","  essa è una funzione di normalizzazione lineare come Softmax ma con una distribuzione più sparsa.\n","  Ovvero rispetto a Softmax alcuni numeri nella distribuzione della probabilità di output sono molto vicini\n","  a 1 mentre altri molto più vicini a 0; ciò consente al modello di selezionare le caratteristiche rilevanti in \n","  ogni fase deciionale in modo più efficace. \n","  Useremo Sparsemax per progettare la maschera per il passaggio di selezione delle features su uno spazio più ristretto. \"\"\"\n","\n","!pip install -U sparsemax\n","\n","from sparsemax import Sparsemax\n","\n","\"\"\" Attention Transformer: \n","  è la fase in cui modelli apprendono la relazione tra le caratteristiche rilevanti e decidono quali trasferire al Feature Transformer.\n","  Ciascun Attention Transformer è costituito da: \n","    - un livello completamente connesso;\n","    - un livello di GBN;\n","    - un livello Sparsemax.\n","  L'attention transformer in ogni fase decisionale riceve le caratteristiche di input, quelle elaborate nella fase precedente e le informazioni preliminari\n","  sulle caratteristiche utilizzate. \n","  Tutte queste info sono rappresentate da una matrice di dim batch_size x input_features. Essa viene aggiornata in ogni fase decisionale.\n","  Esiste anche un parametro di \"rilassamento\" che limita il numero di volte in cui una determinata funzione può essere utilizzata in un passaggio in avanti. \"\"\"\n","\n","class AttentionTransformer(nn.Module):\n","\n","    def __init__(self,d_a,inp_dim,relax,vbs=128):\n","        super().__init__()\n","        self.fc = nn.Linear(d_a,inp_dim)\n","        #self.bn = GBN(out_dim,vbs=vbs)\n","        self.bn = GBN(inp_dim, vbs=vbs)\n","        self.smax = Sparsemax()\n","        self.r = relax\n","    \n","    #a:feature from previous decision step\n","    \n","    def forward(self,a,priors): \n","        a = self.bn(self.fc(a)) \n","        mask = self.smax(a*priors) \n","        priors =priors*(self.r-mask)  #updating the prior\n","        return mask\n","\n","\"\"\" Feautre Transformer: \n"," Il trasformatore di caratteristiche è dove tutte le caratteristiche selezionate vengono elaborate per generare l'output finale. \n"," \n"," Ogni trasformatore di caratteristiche è composto da più Gated Linear Unit Blocks.\n"," Una GLU controlla quali informazioni devono essere autorizzate a fluire ulteriormente attraverso la rete. \n"," Per implementare un blocco GLU, prima raddoppiamo la dimensione delle caratteristiche di input alla GLU utilizzando uno strato completamente connesso.\n"," Normalizziamo la matrice risultante utilizzando un GBN Layer. Quindi, applichiamo un sigmoide alla seconda metà delle caratteristiche risultanti \n"," e moltiplichiamo i risultati per la prima metà. Il risultato viene moltiplicato per un fattore di scala (sqrt (0,5) in questo caso) e aggiunto all'input. \n"," Questo risultato sommato è l'input per il blocco GLU successivo nella sequenza.\n","\n"," Un certo numero di blocchi GLU è condiviso tra tutte le fasi decisionali per promuovere la capacità e l'efficienza del modello (opzionale). \n"," Il primo blocco GLU condiviso (o il primo blocco indipendente se non ci sono blocchi condivisi) è unico in quanto riduce la dimensione \n"," delle features di input ad una dimensione uguale n_a + n_d. \n"," n_a è la dimensione delle caratteristiche in ingresso al trasformatore di attenzione del passaggio successivo e \n"," n_d è la dimensione delle caratteristiche utilizzate per calcolare i risultati finali. \n"," Queste caratteristiche vengono elaborate insieme fino a raggiungere lo splitter. \n"," L'attivazione di ReLU viene applicata al vettore dimensionato n_d. \n"," Gli output di tutte le fasi decisionali vengono sommati e passati attraverso un livello completamente connesso per mapparli alla dimensione di output. \"\"\"\n","\n","class GLU(nn.Module):\n","\n","  def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n","      super().__init__()\n","      if fc:\n","          self.fc = fc\n","      else:\n","          self.fc = nn.Linear(inp_dim,out_dim*2)\n","      self.bn = GBN(out_dim*2,vbs=vbs) \n","      self.od = out_dim\n","\n","  def forward(self,x):\n","      x = self.bn(self.fc(x))\n","      return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n","\n","class FeatureTransformer(nn.Module):\n","\n","  def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n","      super().__init__()\n","      first = True\n","      self.shared = nn.ModuleList()\n","      if shared:\n","          self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n","          first= False    \n","          for fc in shared[1:]:\n","              self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n","      else:\n","          self.shared = None\n","      self.independ = nn.ModuleList()\n","      if first:\n","          self.independ.append(GLU(inp,out_dim,vbs=vbs))\n","      for x in range(first, n_ind):\n","          self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n","      self.scale = torch.sqrt(torch.tensor([.5],device=device))\n","\n","  def forward(self,x):\n","      if self.shared:\n","          x = self.shared[0](x)\n","          for glu in self.shared[1:]:\n","              x = torch.add(x, glu(x))\n","              x = x*self.scale\n","      for glu in self.independ:\n","          x = torch.add(x, glu(x))\n","          x = x*self.scale\n","      return x\n","      \n","\"\"\" Combiniamo Attention Transformer e Feature Transformer in un DecisionStep \"\"\"\n","\n","class DecisionStep(nn.Module):\n","  \n","    def __init__(self,inp_dim,n_d,n_a,shared,n_ind,relax,vbs=128):\n","        super().__init__()\n","        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n","        self.atten_tran =  AttentionTransformer(n_a,inp_dim,relax,vbs)\n","    \n","    def forward(self,x,a,priors):\n","        mask = self.atten_tran(a,priors)\n","        sparse_loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n","        x = self.fea_tran(x*mask)\n","        return x,sparse_loss"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: sparsemax in /usr/local/lib/python3.7/dist-packages (0.1.9)\n","Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from sparsemax) (1.9.0+cu102)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->sparsemax) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4ky4DHJWEQMG"},"source":["\"\"\" Creiamo ora il modello completo mediante gli elementi definiti \"\"\"\n","\n","class TabNet(nn.Module):\n","    def __init__(self,inp_dim, final_out_dim, n_d=64, n_a=64, n_shared=3, n_ind=2, n_steps=5, relax=1.2, vbs=128):\n","        super().__init__()\n","        if n_shared>0:\n","            self.shared = nn.ModuleList()\n","            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n","            for x in range(n_shared-1):\n","                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n","        else:\n","            self.shared=None\n","        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind) \n","        self.steps = nn.ModuleList()\n","        for x in range(n_steps-1):\n","            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n","        self.fc = nn.Linear(n_d,final_out_dim)\n","        self.bn = nn.BatchNorm1d(inp_dim, momentum=1.0)\n","        self.n_d = n_d\n","\n","    def forward(self,x):\n","        x = self.bn(x)\n","        x_a = self.first_step(x)[:,self.n_d:]\n","        sparse_loss = torch.zeros(1).to(x.device)\n","        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n","        priors = torch.ones(x.shape).to(x.device)\n","        for step in self.steps:\n","            x_te,l = step(x,x_a,priors)\n","            out += F.relu(x_te[:,:self.n_d])\n","            x_a = x_te[:,self.n_d:]\n","            sparse_loss += l\n","        return self.fc(out),sparse_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Roh-BHj_YYUE"},"source":["\"\"\" Pytorch Dataset e DataLoader\n","Estendiamo la Datasetclasse (astratta) fornita da Pytorch per un accesso più facile al nostro set di dati durante l'addestramento \n","e per utilizzare efficacemente  il DataLoader modulo per gestire i batch. Ciò comporta la sovrascrittura dei metodi __len__e __getitem__\n","secondo il nostro particolare set di dati.\n","Poiché abbiamo solo bisogno di incorporare colonne categoriali, dividiamo il nostro input in due parti: numerico e categoriale. \"\"\" \n","\n","class CTU_Dataset(Dataset):\n","    def __init__(self, X, Y):\n","        X = X.copy()\n","        self.X = X.copy().values.astype(np.float32) #numerical columns\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","        \n","#creating train and valid datasets\n","train_ds = CTU_Dataset(train, y_train)\n","valid_ds = CTU_Dataset(validation, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQc6Y5qBapPL"},"source":["\"\"\" Fase di preparazione per l'addestramento \"\"\"\n","\n","# Optimizer\n","def get_optimizer(model, lr = 0.001, wd = 0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim\n","\n","# Training function\n","def train_model(model, optim, train_dl):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x, y in train_dl:\n","        batch = y.shape[0]\n","        output, _ = model(x)\n","        loss = F.cross_entropy(output, y)\n","        optim.zero_grad()\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","# Evaluation function\n","def val_loss(model, valid_dl):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x, y in valid_dl:\n","        current_batch_size = y.shape[0]\n","        out,_ = model(x)\n","        loss = F.cross_entropy(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.max(out, 1)[1]\n","        correct += (pred == y).float().sum().item()\n","    #print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n","    print('valid loss ', sum_loss/total, ' and accuracy ', correct/total)\n","    return sum_loss/total, correct/total\n","\n","# Funzione per l'addestramento \n","def train_loop(model, epochs, lr=0.01, wd=0.0):\n","    optim = get_optimizer(model, lr = lr, wd = wd)\n","    for i in range(epochs): \n","        loss = train_model(model, optim, train_dl)\n","        print(\"ep \", i, \" training loss: \", loss)\n","        val_loss(model, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCc89DBck8kz"},"source":["### ***TRAINING***"]},{"cell_type":"code","metadata":{"id":"8hzAjp9VPPVp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624966632208,"user_tz":-120,"elapsed":17,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"74508b7d-38cf-4de3-9b1b-59719f0197ba"},"source":["print('Lunghezza train: ', len(train))\n","print('Lunghezza validation: ', len(validation))\n","print('Lunghezza test: ', len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Lunghezza train:  26408\n","Lunghezza validation:  2500\n","Lunghezza test:  15737\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eDi3nHXlYe8-"},"source":["\"\"\" Ora addestriamo il modello sul set di addestramento. Ho usato l'ottimizzatore Adam per ottimizzare la perdita di entropia incrociata. \n","L'addestramento è piuttosto semplice: iterare attraverso ogni batch, eseguire un passaggio in avanti, calcolare i gradienti, \n","eseguire una discesa del gradiente e ripetere questo processo per tutte le epoche necessarie. \"\"\"\n","\n","# Per TabNet ogni singolo batch deve essere di lunghezza >= a 128 (che sarebbe il vbs)\n","\n","batch_size = 4096\n","train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6OhDp4-Yjn0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624966635677,"user_tz":-120,"elapsed":3481,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"40a93f27-71a2-4bbf-e6fe-491d02715a63"},"source":["model = TabNet(inp_dim=len(cont_names), final_out_dim=2)\n","to_device(model, device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TabNet(\n","  (shared): ModuleList(\n","    (0): Linear(in_features=10, out_features=256, bias=True)\n","    (1): Linear(in_features=128, out_features=256, bias=True)\n","    (2): Linear(in_features=128, out_features=256, bias=True)\n","  )\n","  (first_step): FeatureTransformer(\n","    (shared): ModuleList(\n","      (0): GLU(\n","        (fc): Linear(in_features=10, out_features=256, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): GLU(\n","        (fc): Linear(in_features=128, out_features=256, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (2): GLU(\n","        (fc): Linear(in_features=128, out_features=256, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (independ): ModuleList(\n","      (0): GLU(\n","        (fc): Linear(in_features=128, out_features=256, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): GLU(\n","        (fc): Linear(in_features=128, out_features=256, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","  )\n","  (steps): ModuleList(\n","    (0): DecisionStep(\n","      (fea_tran): FeatureTransformer(\n","        (shared): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=10, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (2): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","        (independ): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","      )\n","      (atten_tran): AttentionTransformer(\n","        (fc): Linear(in_features=64, out_features=10, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(10, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","        (smax): Sparsemax(dim=-1)\n","      )\n","    )\n","    (1): DecisionStep(\n","      (fea_tran): FeatureTransformer(\n","        (shared): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=10, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (2): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","        (independ): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","      )\n","      (atten_tran): AttentionTransformer(\n","        (fc): Linear(in_features=64, out_features=10, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(10, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","        (smax): Sparsemax(dim=-1)\n","      )\n","    )\n","    (2): DecisionStep(\n","      (fea_tran): FeatureTransformer(\n","        (shared): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=10, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (2): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","        (independ): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","      )\n","      (atten_tran): AttentionTransformer(\n","        (fc): Linear(in_features=64, out_features=10, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(10, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","        (smax): Sparsemax(dim=-1)\n","      )\n","    )\n","    (3): DecisionStep(\n","      (fea_tran): FeatureTransformer(\n","        (shared): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=10, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (2): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","        (independ): ModuleList(\n","          (0): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): GLU(\n","            (fc): Linear(in_features=128, out_features=256, bias=True)\n","            (bn): GBN(\n","              (bn): BatchNorm1d(256, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","            )\n","          )\n","        )\n","      )\n","      (atten_tran): AttentionTransformer(\n","        (fc): Linear(in_features=64, out_features=10, bias=True)\n","        (bn): GBN(\n","          (bn): BatchNorm1d(10, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","        )\n","        (smax): Sparsemax(dim=-1)\n","      )\n","    )\n","  )\n","  (fc): Linear(in_features=64, out_features=2, bias=True)\n","  (bn): BatchNorm1d(10, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",")"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7sw7S9oAbb7l","executionInfo":{"status":"ok","timestamp":1624967549939,"user_tz":-120,"elapsed":914286,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"eff8a058-7522-4c7c-df38-867efec91700"},"source":["train_loop(model, epochs=1000, lr=0.00008)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ep  0  training loss:  0.6786936944775493\n","valid loss  0.6494379043579102  and accuracy  0.562\n","ep  1  training loss:  0.5949883987346731\n","valid loss  0.8538815379142761  and accuracy  0.3416\n","ep  2  training loss:  0.5308690767185646\n","valid loss  0.5280598402023315  and accuracy  0.7968\n","ep  3  training loss:  0.5025102018912754\n","valid loss  0.4260927140712738  and accuracy  0.862\n","ep  4  training loss:  0.4964028070259008\n","valid loss  0.43863779306411743  and accuracy  0.8848\n","ep  5  training loss:  0.44943575928045815\n","valid loss  0.45115843415260315  and accuracy  0.9284\n","ep  6  training loss:  0.41467953707838884\n","valid loss  0.3086109757423401  and accuracy  0.9564\n","ep  7  training loss:  0.37554018782218707\n","valid loss  0.5307931303977966  and accuracy  0.5448\n","ep  8  training loss:  0.34903092396616825\n","valid loss  0.15909171104431152  and accuracy  0.9436\n","ep  9  training loss:  0.3284750308765855\n","valid loss  0.37448373436927795  and accuracy  0.948\n","ep  10  training loss:  0.31346167091780164\n","valid loss  0.8718302845954895  and accuracy  0.5216\n","ep  11  training loss:  0.2854707785643653\n","valid loss  0.19043928384780884  and accuracy  0.9328\n","ep  12  training loss:  0.2608288041626602\n","valid loss  0.22734566032886505  and accuracy  0.9488\n","ep  13  training loss:  0.24948206538505463\n","valid loss  0.4011361598968506  and accuracy  0.954\n","ep  14  training loss:  0.22734074178102412\n","valid loss  0.1867293417453766  and accuracy  0.9584\n","ep  15  training loss:  0.22358468761772\n","valid loss  0.14467227458953857  and accuracy  0.9772\n","ep  16  training loss:  0.2301514992042947\n","valid loss  0.12948861718177795  and accuracy  0.9848\n","ep  17  training loss:  0.21377467188247223\n","valid loss  0.3102871775627136  and accuracy  0.9596\n","ep  18  training loss:  0.22011327079952359\n","valid loss  0.08376460522413254  and accuracy  0.9832\n","ep  19  training loss:  0.2060807566865218\n","valid loss  0.4495331943035126  and accuracy  0.8256\n","ep  20  training loss:  0.23120776310865535\n","valid loss  0.09827475249767303  and accuracy  0.9672\n","ep  21  training loss:  0.18175543584685658\n","valid loss  0.11169887334108353  and accuracy  0.9808\n","ep  22  training loss:  0.16954052124609192\n","valid loss  0.16565808653831482  and accuracy  0.962\n","ep  23  training loss:  0.17132651400056906\n","valid loss  0.0689823254942894  and accuracy  0.9816\n","ep  24  training loss:  0.18800929223487176\n","valid loss  0.429044246673584  and accuracy  0.9544\n","ep  25  training loss:  0.16677737964604414\n","valid loss  0.29059723019599915  and accuracy  0.9132\n","ep  26  training loss:  0.17366605568694116\n","valid loss  0.19665205478668213  and accuracy  0.9812\n","ep  27  training loss:  0.1555589956520038\n","valid loss  0.31228265166282654  and accuracy  0.9588\n","ep  28  training loss:  0.1515552191364516\n","valid loss  0.22182397544384003  and accuracy  0.9852\n","ep  29  training loss:  0.15675192808191257\n","valid loss  0.4711586534976959  and accuracy  0.9436\n","ep  30  training loss:  0.14917746680554242\n","valid loss  0.10686822235584259  and accuracy  0.9688\n","ep  31  training loss:  0.14467881926440065\n","valid loss  0.2217736542224884  and accuracy  0.9652\n","ep  32  training loss:  0.13788366841388738\n","valid loss  0.06896933168172836  and accuracy  0.9848\n","ep  33  training loss:  0.13327799260760756\n","valid loss  0.1280374825000763  and accuracy  0.9752\n","ep  34  training loss:  0.12993259578988395\n","valid loss  0.2672419846057892  and accuracy  0.9816\n","ep  35  training loss:  0.11905630922378897\n","valid loss  0.15852032601833344  and accuracy  0.9836\n","ep  36  training loss:  0.11456866837277335\n","valid loss  0.04828770086169243  and accuracy  0.9936\n","ep  37  training loss:  0.11446382862371013\n","valid loss  0.0606146864593029  and accuracy  0.9868\n","ep  38  training loss:  0.11076500374226814\n","valid loss  0.07553219795227051  and accuracy  0.9796\n","ep  39  training loss:  0.10520210394946708\n","valid loss  0.1379772275686264  and accuracy  0.984\n","ep  40  training loss:  0.1104941812564806\n","valid loss  0.05157240480184555  and accuracy  0.9884\n","ep  41  training loss:  0.1016741539539261\n","valid loss  0.09765114635229111  and accuracy  0.9768\n","ep  42  training loss:  0.10666990845321128\n","valid loss  0.06074374541640282  and accuracy  0.9892\n","ep  43  training loss:  0.10024494268801384\n","valid loss  0.06353166699409485  and accuracy  0.9844\n","ep  44  training loss:  0.11371960254401303\n","valid loss  0.07232799381017685  and accuracy  0.9836\n","ep  45  training loss:  0.09503700481702328\n","valid loss  0.07083410024642944  and accuracy  0.9844\n","ep  46  training loss:  0.09463200526539393\n","valid loss  0.17200541496276855  and accuracy  0.9796\n","ep  47  training loss:  0.0826302822120657\n","valid loss  0.08315882831811905  and accuracy  0.9792\n","ep  48  training loss:  0.1045509905336265\n","valid loss  0.04798879101872444  and accuracy  0.9844\n","ep  49  training loss:  0.09341475335495719\n","valid loss  0.09138857573270798  and accuracy  0.986\n","ep  50  training loss:  0.0948475721531729\n","valid loss  0.06201319396495819  and accuracy  0.988\n","ep  51  training loss:  0.09117409185148376\n","valid loss  0.08778855949640274  and accuracy  0.9812\n","ep  52  training loss:  0.1072675206243396\n","valid loss  0.054856933653354645  and accuracy  0.9876\n","ep  53  training loss:  0.10952440068527483\n","valid loss  0.05374576896429062  and accuracy  0.9868\n","ep  54  training loss:  0.10225238716717887\n","valid loss  0.1725485473871231  and accuracy  0.9864\n","ep  55  training loss:  0.08459553476428668\n","valid loss  0.044524095952510834  and accuracy  0.988\n","ep  56  training loss:  0.08859324389354853\n","valid loss  0.06621850281953812  and accuracy  0.97\n","ep  57  training loss:  0.09409201978255821\n","valid loss  0.13854017853736877  and accuracy  0.96\n","ep  58  training loss:  0.08029133077373435\n","valid loss  0.051676683127880096  and accuracy  0.9876\n","ep  59  training loss:  0.07327206080265386\n","valid loss  0.21091963350772858  and accuracy  0.986\n","ep  60  training loss:  0.07149560508927372\n","valid loss  1.1439014673233032  and accuracy  0.9604\n","ep  61  training loss:  0.08023382211879902\n","valid loss  0.13807176053524017  and accuracy  0.9908\n","ep  62  training loss:  0.07429508262242522\n","valid loss  0.1853291094303131  and accuracy  0.9228\n","ep  63  training loss:  0.07596506724815158\n","valid loss  0.03522730991244316  and accuracy  0.9876\n","ep  64  training loss:  0.07009468527589195\n","valid loss  0.1177004873752594  and accuracy  0.9756\n","ep  65  training loss:  0.06647366935359604\n","valid loss  0.05383208766579628  and accuracy  0.9896\n","ep  66  training loss:  0.07072468684301778\n","valid loss  0.42836087942123413  and accuracy  0.9916\n","ep  67  training loss:  0.06239923400766594\n","valid loss  0.0952632799744606  and accuracy  0.986\n","ep  68  training loss:  0.05991005704542681\n","valid loss  0.05067826434969902  and accuracy  0.978\n","ep  69  training loss:  0.07806746639769499\n","valid loss  0.03042522631585598  and accuracy  0.988\n","ep  70  training loss:  0.06947624454739165\n","valid loss  0.03087146393954754  and accuracy  0.9908\n","ep  71  training loss:  0.06675669487697794\n","valid loss  0.07077886164188385  and accuracy  0.9936\n","ep  72  training loss:  0.062699573077837\n","valid loss  0.05515947937965393  and accuracy  0.9912\n","ep  73  training loss:  0.05760149788184803\n","valid loss  0.033538978546857834  and accuracy  0.9888\n","ep  74  training loss:  0.0678247243969486\n","valid loss  0.033610109239816666  and accuracy  0.9916\n","ep  75  training loss:  0.056889748555698096\n","valid loss  0.05266173183917999  and accuracy  0.9828\n","ep  76  training loss:  0.059067671096657164\n","valid loss  0.07471025735139847  and accuracy  0.9804\n","ep  77  training loss:  0.06168312362037764\n","valid loss  0.44424742460250854  and accuracy  0.972\n","ep  78  training loss:  0.05664107591586379\n","valid loss  0.2663426995277405  and accuracy  0.8576\n","ep  79  training loss:  0.05532341316044421\n","valid loss  0.04205835238099098  and accuracy  0.9908\n","ep  80  training loss:  0.059488914478135736\n","valid loss  0.05902029573917389  and accuracy  0.9792\n","ep  81  training loss:  0.06289202835940116\n","valid loss  0.12980499863624573  and accuracy  0.992\n","ep  82  training loss:  0.0620353162397005\n","valid loss  0.04991450160741806  and accuracy  0.9868\n","ep  83  training loss:  0.058127322271172836\n","valid loss  0.13728167116641998  and accuracy  0.9268\n","ep  84  training loss:  0.06438092668691717\n","valid loss  0.073830746114254  and accuracy  0.9844\n","ep  85  training loss:  0.07446450098866009\n","valid loss  0.04509703442454338  and accuracy  0.9832\n","ep  86  training loss:  0.07014809697144178\n","valid loss  0.07324603945016861  and accuracy  0.988\n","ep  87  training loss:  0.06509637093000505\n","valid loss  0.0253337100148201  and accuracy  0.99\n","ep  88  training loss:  0.0619473745575261\n","valid loss  0.026527386158704758  and accuracy  0.9928\n","ep  89  training loss:  0.06344405529183787\n","valid loss  0.026215793564915657  and accuracy  0.992\n","ep  90  training loss:  0.060378935379905796\n","valid loss  0.0304726492613554  and accuracy  0.9892\n","ep  91  training loss:  0.06408175072663483\n","valid loss  0.11530095338821411  and accuracy  0.9536\n","ep  92  training loss:  0.05954957307874705\n","valid loss  0.016793379560112953  and accuracy  0.9912\n","ep  93  training loss:  0.05467519831161757\n","valid loss  0.08841916918754578  and accuracy  0.9836\n","ep  94  training loss:  0.06158372141269618\n","valid loss  0.06146082654595375  and accuracy  0.9956\n","ep  95  training loss:  0.06106291294788563\n","valid loss  0.051785048097372055  and accuracy  0.9884\n","ep  96  training loss:  0.06255385282589644\n","valid loss  0.12502999603748322  and accuracy  0.9792\n","ep  97  training loss:  0.0596274693356305\n","valid loss  0.10690808296203613  and accuracy  0.994\n","ep  98  training loss:  0.053921930771800686\n","valid loss  0.06949259340763092  and accuracy  0.9836\n","ep  99  training loss:  0.059274609050843544\n","valid loss  0.23117414116859436  and accuracy  0.9828\n","ep  100  training loss:  0.05199630110462808\n","valid loss  0.27323657274246216  and accuracy  0.9824\n","ep  101  training loss:  0.05086449873051077\n","valid loss  0.05737243592739105  and accuracy  0.984\n","ep  102  training loss:  0.05480215568134944\n","valid loss  0.054778534919023514  and accuracy  0.9832\n","ep  103  training loss:  0.04582830812580042\n","valid loss  0.14892655611038208  and accuracy  0.97\n","ep  104  training loss:  0.04567960524574369\n","valid loss  0.0290058720856905  and accuracy  0.9904\n","ep  105  training loss:  0.045123583939907154\n","valid loss  0.07505975663661957  and accuracy  0.9816\n","ep  106  training loss:  0.043392109067620234\n","valid loss  0.03112304024398327  and accuracy  0.9904\n","ep  107  training loss:  0.041993784732040004\n","valid loss  0.029866201803088188  and accuracy  0.9912\n","ep  108  training loss:  0.0453338224821129\n","valid loss  0.16611815989017487  and accuracy  0.9584\n","ep  109  training loss:  0.04564934103927155\n","valid loss  0.02984389290213585  and accuracy  0.9928\n","ep  110  training loss:  0.04131323243137274\n","valid loss  0.022788137197494507  and accuracy  0.9912\n","ep  111  training loss:  0.04429543887989263\n","valid loss  0.037224989384412766  and accuracy  0.9868\n","ep  112  training loss:  0.0462856826554985\n","valid loss  0.030593032017350197  and accuracy  0.9876\n","ep  113  training loss:  0.043182866756743064\n","valid loss  0.019578373059630394  and accuracy  0.9972\n","ep  114  training loss:  0.04420735038336174\n","valid loss  0.04734691604971886  and accuracy  0.9832\n","ep  115  training loss:  0.045972846686542126\n","valid loss  0.022578857839107513  and accuracy  0.9916\n","ep  116  training loss:  0.04308876590123324\n","valid loss  0.04124395549297333  and accuracy  0.9892\n","ep  117  training loss:  0.044424796164920566\n","valid loss  0.02423931658267975  and accuracy  0.992\n","ep  118  training loss:  0.042439095656760205\n","valid loss  0.07681325078010559  and accuracy  0.9804\n","ep  119  training loss:  0.04307691489736516\n","valid loss  0.19032131135463715  and accuracy  0.9804\n","ep  120  training loss:  0.04553804735511189\n","valid loss  0.12545976042747498  and accuracy  0.9636\n","ep  121  training loss:  0.0456734708414696\n","valid loss  0.031989626586437225  and accuracy  0.9904\n","ep  122  training loss:  0.04485492010241709\n","valid loss  0.031126242130994797  and accuracy  0.9848\n","ep  123  training loss:  0.04839390990942473\n","valid loss  0.1398095041513443  and accuracy  0.9648\n","ep  124  training loss:  0.046987944406447935\n","valid loss  0.16336165368556976  and accuracy  0.9696\n","ep  125  training loss:  0.05053232664395303\n","valid loss  0.03451820835471153  and accuracy  0.9928\n","ep  126  training loss:  0.04877136436214016\n","valid loss  0.04859348386526108  and accuracy  0.9916\n","ep  127  training loss:  0.04105551706294658\n","valid loss  0.020437069237232208  and accuracy  0.992\n","ep  128  training loss:  0.0462141459781233\n","valid loss  0.05376748740673065  and accuracy  0.9824\n","ep  129  training loss:  0.04191902547316347\n","valid loss  0.038262080401182175  and accuracy  0.9904\n","ep  130  training loss:  0.048352921229804946\n","valid loss  0.029603952541947365  and accuracy  0.988\n","ep  131  training loss:  0.04616608308169452\n","valid loss  0.04041769728064537  and accuracy  0.984\n","ep  132  training loss:  0.03440422309019175\n","valid loss  0.028634795919060707  and accuracy  0.986\n","ep  133  training loss:  0.03447297948331625\n","valid loss  0.02822950668632984  and accuracy  0.9908\n","ep  134  training loss:  0.037167826310247044\n","valid loss  0.05560637637972832  and accuracy  0.9812\n","ep  135  training loss:  0.03374277475868092\n","valid loss  0.05759204551577568  and accuracy  0.9844\n","ep  136  training loss:  0.03719396363764378\n","valid loss  0.03367706388235092  and accuracy  0.9848\n","ep  137  training loss:  0.03695913629820223\n","valid loss  0.38391929864883423  and accuracy  0.9836\n","ep  138  training loss:  0.04214598704778365\n","valid loss  0.04827059805393219  and accuracy  0.9936\n","ep  139  training loss:  0.03908285764275952\n","valid loss  0.03706154599785805  and accuracy  0.9848\n","ep  140  training loss:  0.036082150656656584\n","valid loss  0.03376612439751625  and accuracy  0.9908\n","ep  141  training loss:  0.04146391991949089\n","valid loss  0.03374585136771202  and accuracy  0.9856\n","ep  142  training loss:  0.031480306146118506\n","valid loss  0.02349463477730751  and accuracy  0.992\n","ep  143  training loss:  0.033362724367722134\n","valid loss  0.04592432454228401  and accuracy  0.9848\n","ep  144  training loss:  0.03598705118635466\n","valid loss  0.02113647200167179  and accuracy  0.9936\n","ep  145  training loss:  0.036007416876689194\n","valid loss  0.03056325390934944  and accuracy  0.986\n","ep  146  training loss:  0.03488311877881025\n","valid loss  0.012603521347045898  and accuracy  0.9968\n","ep  147  training loss:  0.0349165813574752\n","valid loss  0.015974488109350204  and accuracy  0.996\n","ep  148  training loss:  0.033657794332448296\n","valid loss  0.040455352514982224  and accuracy  0.9924\n","ep  149  training loss:  0.0379495949808015\n","valid loss  0.3096568286418915  and accuracy  0.984\n","ep  150  training loss:  0.030142438640579316\n","valid loss  0.06266222149133682  and accuracy  0.9836\n","ep  151  training loss:  0.03646917168420035\n","valid loss  0.019642004743218422  and accuracy  0.9872\n","ep  152  training loss:  0.03339493608621892\n","valid loss  0.0677676871418953  and accuracy  0.9848\n","ep  153  training loss:  0.03300371037621781\n","valid loss  0.041051436215639114  and accuracy  0.986\n","ep  154  training loss:  0.038321834471177166\n","valid loss  0.7915248274803162  and accuracy  0.9948\n","ep  155  training loss:  0.03394594424034891\n","valid loss  0.04586707428097725  and accuracy  0.9816\n","ep  156  training loss:  0.03686091011196546\n","valid loss  0.024104874581098557  and accuracy  0.9912\n","ep  157  training loss:  0.031036371839738043\n","valid loss  0.06123536452651024  and accuracy  0.9904\n","ep  158  training loss:  0.03158702460613911\n","valid loss  0.16942912340164185  and accuracy  0.9828\n","ep  159  training loss:  0.03197830949194212\n","valid loss  0.02213890478014946  and accuracy  0.9932\n","ep  160  training loss:  0.03246321165084983\n","valid loss  0.014943806454539299  and accuracy  0.9956\n","ep  161  training loss:  0.034345135141262244\n","valid loss  0.027713626623153687  and accuracy  0.9884\n","ep  162  training loss:  0.027366523875526787\n","valid loss  0.03421994298696518  and accuracy  0.9852\n","ep  163  training loss:  0.031317874489047964\n","valid loss  0.026037929579615593  and accuracy  0.9924\n","ep  164  training loss:  0.02810781381480335\n","valid loss  0.013557832688093185  and accuracy  0.9948\n","ep  165  training loss:  0.02956845062189267\n","valid loss  0.6637667417526245  and accuracy  0.988\n","ep  166  training loss:  0.04001215498209433\n","valid loss  0.016827402636408806  and accuracy  0.9948\n","ep  167  training loss:  0.029690373538841228\n","valid loss  0.058246612548828125  and accuracy  0.9904\n","ep  168  training loss:  0.027330006493362143\n","valid loss  0.04172191396355629  and accuracy  0.9772\n","ep  169  training loss:  0.02859258934882546\n","valid loss  0.13940706849098206  and accuracy  0.9888\n","ep  170  training loss:  0.026815692194187616\n","valid loss  0.19725967943668365  and accuracy  0.98\n","ep  171  training loss:  0.027730697926762464\n","valid loss  0.021228568628430367  and accuracy  0.9936\n","ep  172  training loss:  0.028553402840739053\n","valid loss  0.23325195908546448  and accuracy  0.93\n","ep  173  training loss:  0.026032655033508923\n","valid loss  0.01461796835064888  and accuracy  0.9932\n","ep  174  training loss:  0.03129581618863357\n","valid loss  0.04167972877621651  and accuracy  0.9836\n","ep  175  training loss:  0.02804869929680099\n","valid loss  0.020624127238988876  and accuracy  0.9932\n","ep  176  training loss:  0.02987451457396777\n","valid loss  0.020648756995797157  and accuracy  0.9956\n","ep  177  training loss:  0.025962876157665283\n","valid loss  0.1948009878396988  and accuracy  0.9856\n","ep  178  training loss:  0.02869161093814631\n","valid loss  0.02403700351715088  and accuracy  0.9908\n","ep  179  training loss:  0.026117672177155212\n","valid loss  0.021145308390259743  and accuracy  0.9892\n","ep  180  training loss:  0.03405059004127438\n","valid loss  0.012129169888794422  and accuracy  0.9956\n","ep  181  training loss:  0.027155627420646644\n","valid loss  0.030972763895988464  and accuracy  0.99\n","ep  182  training loss:  0.02841345971787077\n","valid loss  0.026736758649349213  and accuracy  0.992\n","ep  183  training loss:  0.02684436408394902\n","valid loss  0.0921795666217804  and accuracy  0.996\n","ep  184  training loss:  0.028936510479601902\n","valid loss  0.028971359133720398  and accuracy  0.986\n","ep  185  training loss:  0.023457255712700447\n","valid loss  0.0206813532859087  and accuracy  0.9956\n","ep  186  training loss:  0.028120708209173277\n","valid loss  0.020113104954361916  and accuracy  0.9964\n","ep  187  training loss:  0.029547443773097537\n","valid loss  0.018324855715036392  and accuracy  0.9928\n","ep  188  training loss:  0.02763030203203204\n","valid loss  0.017767446115612984  and accuracy  0.9936\n","ep  189  training loss:  0.021619987351014665\n","valid loss  0.08226966112852097  and accuracy  0.9836\n","ep  190  training loss:  0.0235388481600723\n","valid loss  0.021229475736618042  and accuracy  0.9924\n","ep  191  training loss:  0.028133038095102928\n","valid loss  0.059353552758693695  and accuracy  0.986\n","ep  192  training loss:  0.028047763463476504\n","valid loss  0.01982172578573227  and accuracy  0.994\n","ep  193  training loss:  0.023622745791914534\n","valid loss  0.01764344610273838  and accuracy  0.994\n","ep  194  training loss:  0.02197048617755417\n","valid loss  0.06834851205348969  and accuracy  0.9836\n","ep  195  training loss:  0.024172420199577896\n","valid loss  0.013820184394717216  and accuracy  0.9944\n","ep  196  training loss:  0.026698854209624413\n","valid loss  0.025418352335691452  and accuracy  0.9924\n","ep  197  training loss:  0.03034071886211733\n","valid loss  0.12707778811454773  and accuracy  0.9568\n","ep  198  training loss:  0.026689687823344636\n","valid loss  0.046230413019657135  and accuracy  0.9808\n","ep  199  training loss:  0.022581867048992548\n","valid loss  0.047561466693878174  and accuracy  0.9868\n","ep  200  training loss:  0.024358547614564033\n","valid loss  0.16704070568084717  and accuracy  0.9488\n","ep  201  training loss:  0.022577997584824705\n","valid loss  0.016943078488111496  and accuracy  0.9956\n","ep  202  training loss:  0.025540145742939713\n","valid loss  0.019088560715317726  and accuracy  0.992\n","ep  203  training loss:  0.02477959746489811\n","valid loss  0.014183267019689083  and accuracy  0.996\n","ep  204  training loss:  0.024282669778309493\n","valid loss  0.016096467152237892  and accuracy  0.9964\n","ep  205  training loss:  0.020803145563776694\n","valid loss  0.037191249430179596  and accuracy  0.9936\n","ep  206  training loss:  0.023310146856826208\n","valid loss  0.0244060717523098  and accuracy  0.9924\n","ep  207  training loss:  0.020572311115405733\n","valid loss  0.0675228163599968  and accuracy  0.9888\n","ep  208  training loss:  0.023334552103671042\n","valid loss  0.016540931537747383  and accuracy  0.9932\n","ep  209  training loss:  0.02390693282379558\n","valid loss  0.018098996952176094  and accuracy  0.9896\n","ep  210  training loss:  0.03268089512816182\n","valid loss  0.019649051129817963  and accuracy  0.9928\n","ep  211  training loss:  0.024414030159604944\n","valid loss  0.018024589866399765  and accuracy  0.9952\n","ep  212  training loss:  0.02139161173587169\n","valid loss  0.06885351985692978  and accuracy  0.98\n","ep  213  training loss:  0.02505786176539576\n","valid loss  0.04490140080451965  and accuracy  0.988\n","ep  214  training loss:  0.02723599154410742\n","valid loss  0.046725794672966  and accuracy  0.9888\n","ep  215  training loss:  0.02399896424386726\n","valid loss  0.11718791723251343  and accuracy  0.9952\n","ep  216  training loss:  0.029587808992489112\n","valid loss  0.060705848038196564  and accuracy  0.9964\n","ep  217  training loss:  0.02549844306110162\n","valid loss  0.03157762438058853  and accuracy  0.9964\n","ep  218  training loss:  0.028608526452761893\n","valid loss  0.050258006900548935  and accuracy  0.974\n","ep  219  training loss:  0.02526065801100744\n","valid loss  0.05837821587920189  and accuracy  0.996\n","ep  220  training loss:  0.022316340111609084\n","valid loss  0.024083059281110764  and accuracy  0.9928\n","ep  221  training loss:  0.02732501329469865\n","valid loss  0.05867904797196388  and accuracy  0.988\n","ep  222  training loss:  0.02400242234643699\n","valid loss  0.0420871302485466  and accuracy  0.996\n","ep  223  training loss:  0.025892538465706192\n","valid loss  0.02750193141400814  and accuracy  0.9884\n","ep  224  training loss:  0.02596037403331385\n","valid loss  0.020835867151618004  and accuracy  0.996\n","ep  225  training loss:  0.02214720933417395\n","valid loss  0.039420049637556076  and accuracy  0.9872\n","ep  226  training loss:  0.022679459103866983\n","valid loss  0.014134426601231098  and accuracy  0.9932\n","ep  227  training loss:  0.026213837478290685\n","valid loss  0.10043369978666306  and accuracy  0.9856\n","ep  228  training loss:  0.02429222508698422\n","valid loss  0.014265710487961769  and accuracy  0.994\n","ep  229  training loss:  0.022558272597023084\n","valid loss  0.4434475004673004  and accuracy  0.9964\n","ep  230  training loss:  0.02748311565152373\n","valid loss  0.01258101873099804  and accuracy  0.996\n","ep  231  training loss:  0.02718147585183716\n","valid loss  0.017294974997639656  and accuracy  0.996\n","ep  232  training loss:  0.02075681887889804\n","valid loss  0.018583549186587334  and accuracy  0.996\n","ep  233  training loss:  0.029634995610018422\n","valid loss  0.008189544081687927  and accuracy  0.9984\n","ep  234  training loss:  0.026150254035575648\n","valid loss  0.028617490082979202  and accuracy  0.9956\n","ep  235  training loss:  0.02570888491150503\n","valid loss  0.09070160239934921  and accuracy  0.9968\n","ep  236  training loss:  0.02478079373449746\n","valid loss  0.12914568185806274  and accuracy  0.9948\n","ep  237  training loss:  0.030065762193383177\n","valid loss  0.0317152701318264  and accuracy  0.992\n","ep  238  training loss:  0.02718421025215785\n","valid loss  0.04045433923602104  and accuracy  0.9928\n","ep  239  training loss:  0.02442030723313063\n","valid loss  0.01759430021047592  and accuracy  0.9944\n","ep  240  training loss:  0.019226404519735306\n","valid loss  0.0241627786308527  and accuracy  0.9944\n","ep  241  training loss:  0.034206912084010034\n","valid loss  0.012728419154882431  and accuracy  0.9948\n","ep  242  training loss:  0.031011089489599748\n","valid loss  0.0319184809923172  and accuracy  0.9868\n","ep  243  training loss:  0.02998008014184531\n","valid loss  0.017417389899492264  and accuracy  0.9968\n","ep  244  training loss:  0.03167502251694922\n","valid loss  0.02861730381846428  and accuracy  0.9928\n","ep  245  training loss:  0.027646488865842533\n","valid loss  0.017560746520757675  and accuracy  0.9908\n","ep  246  training loss:  0.026244963481751437\n","valid loss  0.019764237105846405  and accuracy  0.994\n","ep  247  training loss:  0.025194400644973167\n","valid loss  0.19505569338798523  and accuracy  0.9868\n","ep  248  training loss:  0.026212316597491656\n","valid loss  0.021949931979179382  and accuracy  0.9924\n","ep  249  training loss:  0.022100025194905015\n","valid loss  0.013486109673976898  and accuracy  0.9948\n","ep  250  training loss:  0.02708070649477938\n","valid loss  0.0252451803535223  and accuracy  0.9968\n","ep  251  training loss:  0.026407626013716434\n","valid loss  0.012036529369652271  and accuracy  0.9964\n","ep  252  training loss:  0.02373060632623468\n","valid loss  0.04537685960531235  and accuracy  0.9936\n","ep  253  training loss:  0.01967019219176042\n","valid loss  0.1926000863313675  and accuracy  0.9512\n","ep  254  training loss:  0.020991039305803486\n","valid loss  0.2346407175064087  and accuracy  0.9588\n","ep  255  training loss:  0.02439995017043752\n","valid loss  0.018750736489892006  and accuracy  0.9936\n","ep  256  training loss:  0.02542395129635077\n","valid loss  0.048575110733509064  and accuracy  0.9864\n","ep  257  training loss:  0.021115277313492727\n","valid loss  0.011150517500936985  and accuracy  0.9972\n","ep  258  training loss:  0.024078721609401076\n","valid loss  0.016664758324623108  and accuracy  0.9952\n","ep  259  training loss:  0.02362157405889854\n","valid loss  0.03559856116771698  and accuracy  0.9948\n","ep  260  training loss:  0.025940007076764315\n","valid loss  0.026914019137620926  and accuracy  0.988\n","ep  261  training loss:  0.025590185083898043\n","valid loss  0.011886787600815296  and accuracy  0.9968\n","ep  262  training loss:  0.023314112898947543\n","valid loss  0.0257400032132864  and accuracy  0.9896\n","ep  263  training loss:  0.01958441883590782\n","valid loss  0.022477108985185623  and accuracy  0.9864\n","ep  264  training loss:  0.028154962842562314\n","valid loss  0.024022802710533142  and accuracy  0.9932\n","ep  265  training loss:  0.024255932449518063\n","valid loss  0.014059646986424923  and accuracy  0.9964\n","ep  266  training loss:  0.025536221281366615\n","valid loss  0.038544315844774246  and accuracy  0.9856\n","ep  267  training loss:  0.02368657926525791\n","valid loss  0.018865937367081642  and accuracy  0.992\n","ep  268  training loss:  0.028247668984820357\n","valid loss  0.012980201281607151  and accuracy  0.9968\n","ep  269  training loss:  0.019166884777384142\n","valid loss  0.014558405615389347  and accuracy  0.9964\n","ep  270  training loss:  0.027435947194361425\n","valid loss  0.02982933633029461  and accuracy  0.9872\n","ep  271  training loss:  0.02257289294739161\n","valid loss  0.02238409034907818  and accuracy  0.9868\n","ep  272  training loss:  0.024668951692909988\n","valid loss  0.024549944326281548  and accuracy  0.9916\n","ep  273  training loss:  0.021172624265324236\n","valid loss  0.016479067504405975  and accuracy  0.9948\n","ep  274  training loss:  0.026091109222645868\n","valid loss  0.010904214344918728  and accuracy  0.996\n","ep  275  training loss:  0.022476224829684022\n","valid loss  0.02892031893134117  and accuracy  0.99\n","ep  276  training loss:  0.02445817145726891\n","valid loss  0.05286886543035507  and accuracy  0.9884\n","ep  277  training loss:  0.024762802225783337\n","valid loss  0.010058308951556683  and accuracy  0.9964\n","ep  278  training loss:  0.021361532568326687\n","valid loss  0.03658786788582802  and accuracy  0.996\n","ep  279  training loss:  0.02284117813186279\n","valid loss  0.024894751608371735  and accuracy  0.9956\n","ep  280  training loss:  0.019845360854482044\n","valid loss  0.37164923548698425  and accuracy  0.9924\n","ep  281  training loss:  0.02214767433414041\n","valid loss  0.019182927906513214  and accuracy  0.9948\n","ep  282  training loss:  0.020346034241750525\n","valid loss  0.014254524372518063  and accuracy  0.9956\n","ep  283  training loss:  0.020205302624400366\n","valid loss  0.030986666679382324  and accuracy  0.9928\n","ep  284  training loss:  0.01778968259685421\n","valid loss  0.01800478808581829  and accuracy  0.994\n","ep  285  training loss:  0.020949391284221158\n","valid loss  0.019399724900722504  and accuracy  0.9892\n","ep  286  training loss:  0.016885071937575987\n","valid loss  0.015381469391286373  and accuracy  0.9972\n","ep  287  training loss:  0.01735251916640728\n","valid loss  0.01783723197877407  and accuracy  0.9952\n","ep  288  training loss:  0.023689560829166643\n","valid loss  0.018647434189915657  and accuracy  0.9956\n","ep  289  training loss:  0.01873668022024942\n","valid loss  0.08016534149646759  and accuracy  0.9964\n","ep  290  training loss:  0.018253433542657318\n","valid loss  0.017259836196899414  and accuracy  0.9944\n","ep  291  training loss:  0.018022836421008943\n","valid loss  0.06346209347248077  and accuracy  0.9844\n","ep  292  training loss:  0.015518309246050124\n","valid loss  0.014231497421860695  and accuracy  0.9956\n","ep  293  training loss:  0.01658230498421828\n","valid loss  0.012334872968494892  and accuracy  0.996\n","ep  294  training loss:  0.015664322404441995\n","valid loss  0.015596438199281693  and accuracy  0.9956\n","ep  295  training loss:  0.01933112328686441\n","valid loss  0.01926841214299202  and accuracy  0.9932\n","ep  296  training loss:  0.020129158024181564\n","valid loss  0.2879045307636261  and accuracy  0.9872\n","ep  297  training loss:  0.01787525556910466\n","valid loss  0.013042215257883072  and accuracy  0.9952\n","ep  298  training loss:  0.016250394815700373\n","valid loss  0.02271714247763157  and accuracy  0.9944\n","ep  299  training loss:  0.018999466011218866\n","valid loss  0.01958988606929779  and accuracy  0.9964\n","ep  300  training loss:  0.019473300724533227\n","valid loss  0.02008429355919361  and accuracy  0.9916\n","ep  301  training loss:  0.01778110329392407\n","valid loss  0.02772623300552368  and accuracy  0.992\n","ep  302  training loss:  0.01624464589546067\n","valid loss  0.011130244471132755  and accuracy  0.9972\n","ep  303  training loss:  0.016633024234386763\n","valid loss  0.0196524728089571  and accuracy  0.9948\n","ep  304  training loss:  0.017682056703063907\n","valid loss  0.026854928582906723  and accuracy  0.9872\n","ep  305  training loss:  0.021268766893085767\n","valid loss  0.013032362796366215  and accuracy  0.9972\n","ep  306  training loss:  0.02034637407122941\n","valid loss  0.010479975491762161  and accuracy  0.998\n","ep  307  training loss:  0.02046095083081986\n","valid loss  0.0370231568813324  and accuracy  0.9832\n","ep  308  training loss:  0.01605219835592098\n","valid loss  0.02066221460700035  and accuracy  0.9936\n","ep  309  training loss:  0.01576431449564776\n","valid loss  0.009947785176336765  and accuracy  0.9968\n","ep  310  training loss:  0.015467956972524854\n","valid loss  0.01689474657177925  and accuracy  0.9956\n","ep  311  training loss:  0.021014519331492065\n","valid loss  0.018291253596544266  and accuracy  0.996\n","ep  312  training loss:  0.01651099577525806\n","valid loss  0.015027036890387535  and accuracy  0.9928\n","ep  313  training loss:  0.016454746361265524\n","valid loss  0.015058902092278004  and accuracy  0.9964\n","ep  314  training loss:  0.020288883854844685\n","valid loss  1.4512256383895874  and accuracy  0.9924\n","ep  315  training loss:  0.017061851384618704\n","valid loss  0.03215458616614342  and accuracy  0.9884\n","ep  316  training loss:  0.015254889493844827\n","valid loss  0.013281446881592274  and accuracy  0.9968\n","ep  317  training loss:  0.016709659163386164\n","valid loss  0.024767113849520683  and accuracy  0.9932\n","ep  318  training loss:  0.014093514206268793\n","valid loss  0.023389281705021858  and accuracy  0.9952\n","ep  319  training loss:  0.019556203627407606\n","valid loss  0.010073559358716011  and accuracy  0.9972\n","ep  320  training loss:  0.01599649405425713\n","valid loss  0.012253539636731148  and accuracy  0.996\n","ep  321  training loss:  0.020002299141882496\n","valid loss  0.014895661734044552  and accuracy  0.996\n","ep  322  training loss:  0.018636901057428282\n","valid loss  0.14901502430438995  and accuracy  0.994\n","ep  323  training loss:  0.01673793776671215\n","valid loss  0.0307301115244627  and accuracy  0.988\n","ep  324  training loss:  0.01885304053846416\n","valid loss  0.016148893162608147  and accuracy  0.996\n","ep  325  training loss:  0.01714384979186131\n","valid loss  0.01407997589558363  and accuracy  0.9952\n","ep  326  training loss:  0.022782986971525143\n","valid loss  0.01160388719290495  and accuracy  0.9964\n","ep  327  training loss:  0.014348198033099606\n","valid loss  0.02872975543141365  and accuracy  0.9904\n","ep  328  training loss:  0.015013679655500703\n","valid loss  0.015922708436846733  and accuracy  0.9972\n","ep  329  training loss:  0.017542617575521363\n","valid loss  0.025158479809761047  and accuracy  0.9968\n","ep  330  training loss:  0.017719229029858882\n","valid loss  0.021269863471388817  and accuracy  0.9884\n","ep  331  training loss:  0.019250958282765712\n","valid loss  0.01584029570221901  and accuracy  0.9952\n","ep  332  training loss:  0.01858091936016473\n","valid loss  0.011808180250227451  and accuracy  0.994\n","ep  333  training loss:  0.01626498393445837\n","valid loss  0.008993759751319885  and accuracy  0.9972\n","ep  334  training loss:  0.017045054230381957\n","valid loss  0.012259358540177345  and accuracy  0.9956\n","ep  335  training loss:  0.015089699804954405\n","valid loss  0.010098879225552082  and accuracy  0.9964\n","ep  336  training loss:  0.020811944536952783\n","valid loss  0.024724474176764488  and accuracy  0.9944\n","ep  337  training loss:  0.018305582245735644\n","valid loss  0.008372845128178596  and accuracy  0.9984\n","ep  338  training loss:  0.01365170308288048\n","valid loss  0.02906898781657219  and accuracy  0.992\n","ep  339  training loss:  0.017345867426126465\n","valid loss  0.07501242309808731  and accuracy  0.9948\n","ep  340  training loss:  0.01625387023336524\n","valid loss  0.014598110690712929  and accuracy  0.9968\n","ep  341  training loss:  0.01465538492377185\n","valid loss  0.015054076910018921  and accuracy  0.9948\n","ep  342  training loss:  0.014885073261924023\n","valid loss  0.15635612607002258  and accuracy  0.9964\n","ep  343  training loss:  0.014381757235141531\n","valid loss  0.01574375107884407  and accuracy  0.9956\n","ep  344  training loss:  0.014469433495178435\n","valid loss  0.10345179587602615  and accuracy  0.9644\n","ep  345  training loss:  0.01811140503732093\n","valid loss  0.010489052161574364  and accuracy  0.9972\n","ep  346  training loss:  0.01734636681449482\n","valid loss  0.1221705824136734  and accuracy  0.9644\n","ep  347  training loss:  0.013027188428236079\n","valid loss  0.009579968638718128  and accuracy  0.9976\n","ep  348  training loss:  0.01761681167366504\n","valid loss  0.4494887590408325  and accuracy  0.9872\n","ep  349  training loss:  0.015178253439132894\n","valid loss  0.013211656361818314  and accuracy  0.9972\n","ep  350  training loss:  0.018671118364841765\n","valid loss  0.01847180724143982  and accuracy  0.994\n","ep  351  training loss:  0.01400842633853353\n","valid loss  0.011569805443286896  and accuracy  0.9976\n","ep  352  training loss:  0.018752189439441246\n","valid loss  0.01157842855900526  and accuracy  0.9964\n","ep  353  training loss:  0.014000327474759568\n","valid loss  0.011274340562522411  and accuracy  0.996\n","ep  354  training loss:  0.013100480717504992\n","valid loss  0.015467257238924503  and accuracy  0.9944\n","ep  355  training loss:  0.013093566476014465\n","valid loss  0.08543580025434494  and accuracy  0.982\n","ep  356  training loss:  0.017659754536073158\n","valid loss  0.015353905037045479  and accuracy  0.9932\n","ep  357  training loss:  0.01579377436922946\n","valid loss  0.01342903170734644  and accuracy  0.9952\n","ep  358  training loss:  0.014597505129596133\n","valid loss  0.022072384133934975  and accuracy  0.9912\n","ep  359  training loss:  0.021374450551311524\n","valid loss  0.07140554487705231  and accuracy  0.9932\n","ep  360  training loss:  0.014234157692741459\n","valid loss  0.0732911005616188  and accuracy  0.9976\n","ep  361  training loss:  0.013789862560227364\n","valid loss  0.02058108150959015  and accuracy  0.9948\n","ep  362  training loss:  0.012233953401322203\n","valid loss  1.9482474327087402  and accuracy  0.9956\n","ep  363  training loss:  0.015838705513782074\n","valid loss  0.013874347321689129  and accuracy  0.9968\n","ep  364  training loss:  0.016126183970410674\n","valid loss  0.009778195060789585  and accuracy  0.998\n","ep  365  training loss:  0.01354014363858744\n","valid loss  0.01171171385794878  and accuracy  0.996\n","ep  366  training loss:  0.015369695255527782\n","valid loss  0.05325009673833847  and accuracy  0.9948\n","ep  367  training loss:  0.013028199726056056\n","valid loss  0.02401028200984001  and accuracy  0.9964\n","ep  368  training loss:  0.012867049635431688\n","valid loss  0.011277291923761368  and accuracy  0.9968\n","ep  369  training loss:  0.013408559230905307\n","valid loss  0.013374053873121738  and accuracy  0.9948\n","ep  370  training loss:  0.01340197515328322\n","valid loss  0.012392154894769192  and accuracy  0.9972\n","ep  371  training loss:  0.013449025356742025\n","valid loss  0.019771765917539597  and accuracy  0.9952\n","ep  372  training loss:  0.011325550548590085\n","valid loss  0.009429465979337692  and accuracy  0.9972\n","ep  373  training loss:  0.01238635819043058\n","valid loss  0.013855577446520329  and accuracy  0.9968\n","ep  374  training loss:  0.013703775629672055\n","valid loss  0.010812556371092796  and accuracy  0.9952\n","ep  375  training loss:  0.01359393372533629\n","valid loss  0.013912031427025795  and accuracy  0.9968\n","ep  376  training loss:  0.016917239686700475\n","valid loss  0.011158646084368229  and accuracy  0.9968\n","ep  377  training loss:  0.01515771698018198\n","valid loss  0.01983829401433468  and accuracy  0.9908\n","ep  378  training loss:  0.01221018067542187\n","valid loss  0.012067665345966816  and accuracy  0.9956\n","ep  379  training loss:  0.013390940623919337\n","valid loss  0.018755920231342316  and accuracy  0.9928\n","ep  380  training loss:  0.013029500652218598\n","valid loss  0.013765846379101276  and accuracy  0.9964\n","ep  381  training loss:  0.019271981705773496\n","valid loss  0.00852888822555542  and accuracy  0.9976\n","ep  382  training loss:  0.013317675536548647\n","valid loss  0.06261444836854935  and accuracy  0.9884\n","ep  383  training loss:  0.013349076796191022\n","valid loss  0.012008117511868477  and accuracy  0.9972\n","ep  384  training loss:  0.011674218924512415\n","valid loss  0.012440566904842854  and accuracy  0.996\n","ep  385  training loss:  0.01320317696770938\n","valid loss  0.014524461701512337  and accuracy  0.9968\n","ep  386  training loss:  0.011404732084796126\n","valid loss  0.011254679411649704  and accuracy  0.9948\n","ep  387  training loss:  0.01125250931127266\n","valid loss  0.01150945108383894  and accuracy  0.9964\n","ep  388  training loss:  0.011609327174459071\n","valid loss  0.031903766095638275  and accuracy  0.9868\n","ep  389  training loss:  0.010591035148044792\n","valid loss  0.011732446029782295  and accuracy  0.996\n","ep  390  training loss:  0.011604646176931507\n","valid loss  0.006173483096063137  and accuracy  0.9984\n","ep  391  training loss:  0.010968465781783343\n","valid loss  0.056367725133895874  and accuracy  0.9824\n","ep  392  training loss:  0.01285730173455264\n","valid loss  0.016109222546219826  and accuracy  0.9956\n","ep  393  training loss:  0.011044315350803687\n","valid loss  0.006246985401958227  and accuracy  0.9976\n","ep  394  training loss:  0.018844838850301745\n","valid loss  0.009131977334618568  and accuracy  0.9968\n","ep  395  training loss:  0.011715841799256693\n","valid loss  0.014646102674305439  and accuracy  0.9956\n","ep  396  training loss:  0.012251917556632436\n","valid loss  0.013470754958689213  and accuracy  0.9952\n","ep  397  training loss:  0.010641901803216187\n","valid loss  0.0638052225112915  and accuracy  0.9968\n","ep  398  training loss:  0.011321148350135155\n","valid loss  0.017719745635986328  and accuracy  0.9952\n","ep  399  training loss:  0.01406576422259541\n","valid loss  0.22327575087547302  and accuracy  0.9864\n","ep  400  training loss:  0.012679468908072674\n","valid loss  0.009741335175931454  and accuracy  0.9972\n","ep  401  training loss:  0.012832703287223728\n","valid loss  0.00893185380846262  and accuracy  0.9972\n","ep  402  training loss:  0.01468292207018972\n","valid loss  0.007919835858047009  and accuracy  0.9972\n","ep  403  training loss:  0.014381556997120255\n","valid loss  0.034528862684965134  and accuracy  0.9908\n","ep  404  training loss:  0.010897876697901938\n","valid loss  0.01226799376308918  and accuracy  0.996\n","ep  405  training loss:  0.011400515608313462\n","valid loss  0.011728746816515923  and accuracy  0.9972\n","ep  406  training loss:  0.014786050535464234\n","valid loss  0.009016559459269047  and accuracy  0.998\n","ep  407  training loss:  0.011528944697934132\n","valid loss  0.007842952385544777  and accuracy  0.9976\n","ep  408  training loss:  0.011228102887084866\n","valid loss  0.47336089611053467  and accuracy  0.9968\n","ep  409  training loss:  0.0110140440263863\n","valid loss  0.012859022244811058  and accuracy  0.9976\n","ep  410  training loss:  0.014271952574680481\n","valid loss  0.014583513140678406  and accuracy  0.9952\n","ep  411  training loss:  0.011934019953308143\n","valid loss  0.019891707226634026  and accuracy  0.9952\n","ep  412  training loss:  0.0142153354242593\n","valid loss  0.00595424510538578  and accuracy  0.998\n","ep  413  training loss:  0.011826022356292654\n","valid loss  0.008631845004856586  and accuracy  0.998\n","ep  414  training loss:  0.011455818954025731\n","valid loss  0.009179269894957542  and accuracy  0.9972\n","ep  415  training loss:  0.0113523176720863\n","valid loss  0.14221714437007904  and accuracy  0.9964\n","ep  416  training loss:  0.010231729803233888\n","valid loss  0.010170027613639832  and accuracy  0.996\n","ep  417  training loss:  0.010823005851571468\n","valid loss  0.018991978839039803  and accuracy  0.9964\n","ep  418  training loss:  0.012672568739681615\n","valid loss  0.00884852185845375  and accuracy  0.9972\n","ep  419  training loss:  0.012655704465046665\n","valid loss  0.04400678724050522  and accuracy  0.9856\n","ep  420  training loss:  0.010943247061205863\n","valid loss  0.00751112587749958  and accuracy  0.9976\n","ep  421  training loss:  0.011014070093261585\n","valid loss  0.009248402900993824  and accuracy  0.9972\n","ep  422  training loss:  0.010849409447365035\n","valid loss  0.019718999043107033  and accuracy  0.9912\n","ep  423  training loss:  0.009820373997180273\n","valid loss  0.2416258305311203  and accuracy  0.992\n","ep  424  training loss:  0.010378272320609117\n","valid loss  0.013650543056428432  and accuracy  0.9956\n","ep  425  training loss:  0.01035155181261499\n","valid loss  0.005123557057231665  and accuracy  0.9976\n","ep  426  training loss:  0.009380581840349124\n","valid loss  0.01699730195105076  and accuracy  0.996\n","ep  427  training loss:  0.011658097116541264\n","valid loss  0.006910921540111303  and accuracy  0.9976\n","ep  428  training loss:  0.011154270989064808\n","valid loss  0.01690427027642727  and accuracy  0.9964\n","ep  429  training loss:  0.012088785035311678\n","valid loss  0.008776958100497723  and accuracy  0.9964\n","ep  430  training loss:  0.009750914790404363\n","valid loss  0.0206831693649292  and accuracy  0.9968\n","ep  431  training loss:  0.011005927350821232\n","valid loss  0.020232897251844406  and accuracy  0.9908\n","ep  432  training loss:  0.009985652108109925\n","valid loss  0.055484093725681305  and accuracy  0.988\n","ep  433  training loss:  0.010590661999308785\n","valid loss  0.02178744599223137  and accuracy  0.992\n","ep  434  training loss:  0.012533297789923821\n","valid loss  0.01475988607853651  and accuracy  0.996\n","ep  435  training loss:  0.008983390204612954\n","valid loss  0.02141163870692253  and accuracy  0.9924\n","ep  436  training loss:  0.012712862506797732\n","valid loss  1.7029482126235962  and accuracy  0.988\n","ep  437  training loss:  0.011133998736820413\n","valid loss  0.0134641844779253  and accuracy  0.9968\n","ep  438  training loss:  0.00932776872367705\n","valid loss  0.01824324205517769  and accuracy  0.9976\n","ep  439  training loss:  0.014335618876044912\n","valid loss  0.010650456883013248  and accuracy  0.9964\n","ep  440  training loss:  0.010697242891050873\n","valid loss  0.012125530280172825  and accuracy  0.9972\n","ep  441  training loss:  0.00975590133523217\n","valid loss  0.008652254939079285  and accuracy  0.998\n","ep  442  training loss:  0.008862027925515638\n","valid loss  0.023375414311885834  and accuracy  0.9868\n","ep  443  training loss:  0.009834722403331945\n","valid loss  0.42154693603515625  and accuracy  0.994\n","ep  444  training loss:  0.012009228036028802\n","valid loss  0.014151221141219139  and accuracy  0.9936\n","ep  445  training loss:  0.011043945570882343\n","valid loss  0.012610893696546555  and accuracy  0.9952\n","ep  446  training loss:  0.00967363276177782\n","valid loss  0.013636503368616104  and accuracy  0.9968\n","ep  447  training loss:  0.011323830991237453\n","valid loss  0.008109803311526775  and accuracy  0.9976\n","ep  448  training loss:  0.010598888752490545\n","valid loss  0.5447903871536255  and accuracy  0.9932\n","ep  449  training loss:  0.01016957886117547\n","valid loss  0.016193630173802376  and accuracy  0.9896\n","ep  450  training loss:  0.010922265856367951\n","valid loss  0.009895386174321175  and accuracy  0.9944\n","ep  451  training loss:  0.009998429829005905\n","valid loss  0.011430110782384872  and accuracy  0.996\n","ep  452  training loss:  0.009521158883261775\n","valid loss  0.05276801809668541  and accuracy  0.9852\n","ep  453  training loss:  0.009671270211109589\n","valid loss  0.015870753675699234  and accuracy  0.9948\n","ep  454  training loss:  0.008389954124254301\n","valid loss  0.01031895074993372  and accuracy  0.9964\n","ep  455  training loss:  0.01246777855162276\n","valid loss  0.009921643882989883  and accuracy  0.9976\n","ep  456  training loss:  0.01089064238643202\n","valid loss  0.02118639461696148  and accuracy  0.996\n","ep  457  training loss:  0.010948278028512318\n","valid loss  0.00881698913872242  and accuracy  0.9972\n","ep  458  training loss:  0.010579231190919804\n","valid loss  0.038619548082351685  and accuracy  0.9892\n","ep  459  training loss:  0.009078380014996787\n","valid loss  0.02231767401099205  and accuracy  0.992\n","ep  460  training loss:  0.015163024824244692\n","valid loss  0.01740051433444023  and accuracy  0.9944\n","ep  461  training loss:  0.010893818113746354\n","valid loss  0.13801704347133636  and accuracy  0.996\n","ep  462  training loss:  0.015099885916098685\n","valid loss  0.009268254972994328  and accuracy  0.9968\n","ep  463  training loss:  0.010899798529856279\n","valid loss  0.011517548002302647  and accuracy  0.996\n","ep  464  training loss:  0.011563032648704723\n","valid loss  0.01111192163079977  and accuracy  0.9964\n","ep  465  training loss:  0.013662737796759649\n","valid loss  0.016621317714452744  and accuracy  0.996\n","ep  466  training loss:  0.014479256425221033\n","valid loss  0.007634930312633514  and accuracy  0.9972\n","ep  467  training loss:  0.011315767650182988\n","valid loss  0.2099848836660385  and accuracy  0.9896\n","ep  468  training loss:  0.019801124174264082\n","valid loss  0.010244383476674557  and accuracy  0.9972\n","ep  469  training loss:  0.011085220557569518\n","valid loss  0.012780189514160156  and accuracy  0.996\n","ep  470  training loss:  0.011146368387487306\n","valid loss  0.006902896333485842  and accuracy  0.9984\n","ep  471  training loss:  0.010855815127622861\n","valid loss  0.007810597773641348  and accuracy  0.998\n","ep  472  training loss:  0.012069282304778835\n","valid loss  0.011029726825654507  and accuracy  0.9976\n","ep  473  training loss:  0.013354665589042322\n","valid loss  0.018729839473962784  and accuracy  0.9904\n","ep  474  training loss:  0.010615664684647901\n","valid loss  0.011455061845481396  and accuracy  0.9968\n","ep  475  training loss:  0.012117796192401036\n","valid loss  0.05920344963669777  and accuracy  0.984\n","ep  476  training loss:  0.010401301704103371\n","valid loss  0.005560724064707756  and accuracy  0.9984\n","ep  477  training loss:  0.010026016507621643\n","valid loss  0.009691741317510605  and accuracy  0.9972\n","ep  478  training loss:  0.01236991220405249\n","valid loss  0.005497532431036234  and accuracy  0.9988\n","ep  479  training loss:  0.014422642529350462\n","valid loss  0.0046967449598014355  and accuracy  0.9984\n","ep  480  training loss:  0.009940347052778811\n","valid loss  0.007222938351333141  and accuracy  0.9972\n","ep  481  training loss:  0.010392001938662793\n","valid loss  0.11304458230733871  and accuracy  0.9924\n","ep  482  training loss:  0.0103655130927155\n","valid loss  0.014869415201246738  and accuracy  0.9956\n","ep  483  training loss:  0.01301201801804639\n","valid loss  0.06613465398550034  and accuracy  0.9952\n","ep  484  training loss:  0.011775282847530568\n","valid loss  0.017425861209630966  and accuracy  0.992\n","ep  485  training loss:  0.00860152874109204\n","valid loss  0.01251312904059887  and accuracy  0.9964\n","ep  486  training loss:  0.010842906332596509\n","valid loss  0.018681563436985016  and accuracy  0.994\n","ep  487  training loss:  0.007916260837957882\n","valid loss  0.05917004495859146  and accuracy  0.9848\n","ep  488  training loss:  0.009271142820911456\n","valid loss  0.017088863998651505  and accuracy  0.9952\n","ep  489  training loss:  0.011137679399247062\n","valid loss  0.005611457396298647  and accuracy  0.9984\n","ep  490  training loss:  0.008014783052734915\n","valid loss  0.006632762961089611  and accuracy  0.998\n","ep  491  training loss:  0.0092394277710164\n","valid loss  0.010253595188260078  and accuracy  0.9964\n","ep  492  training loss:  0.015243459935024972\n","valid loss  0.008121158927679062  and accuracy  0.9972\n","ep  493  training loss:  0.00982381530519111\n","valid loss  0.01071093324571848  and accuracy  0.9964\n","ep  494  training loss:  0.011751204800177833\n","valid loss  0.01269772369414568  and accuracy  0.9952\n","ep  495  training loss:  0.012885511609037531\n","valid loss  0.007985901087522507  and accuracy  0.9968\n","ep  496  training loss:  0.01117642328280801\n","valid loss  0.005855432711541653  and accuracy  0.9976\n","ep  497  training loss:  0.012064342140718071\n","valid loss  0.08777406066656113  and accuracy  0.9768\n","ep  498  training loss:  0.009357199315895811\n","valid loss  0.013079226948320866  and accuracy  0.9956\n","ep  499  training loss:  0.009723375381840356\n","valid loss  0.009134777821600437  and accuracy  0.996\n","ep  500  training loss:  0.009202937498492332\n","valid loss  0.005232200026512146  and accuracy  0.9984\n","ep  501  training loss:  0.012885927298563027\n","valid loss  0.31368330121040344  and accuracy  0.996\n","ep  502  training loss:  0.00959447382474462\n","valid loss  0.04617318883538246  and accuracy  0.9856\n","ep  503  training loss:  0.009898917734853177\n","valid loss  0.011058364994823933  and accuracy  0.9968\n","ep  504  training loss:  0.01040090011109442\n","valid loss  0.008963960222899914  and accuracy  0.9972\n","ep  505  training loss:  0.013655686181662697\n","valid loss  0.008005631156265736  and accuracy  0.9972\n","ep  506  training loss:  0.01075048612019341\n","valid loss  0.03483826667070389  and accuracy  0.9884\n","ep  507  training loss:  0.01801493167578136\n","valid loss  0.015669774264097214  and accuracy  0.9952\n","ep  508  training loss:  0.012312517452886865\n","valid loss  0.023830102756619453  and accuracy  0.9884\n","ep  509  training loss:  0.013132902085995518\n","valid loss  0.0040230900049209595  and accuracy  0.9984\n","ep  510  training loss:  0.008453779722970477\n","valid loss  0.20825496315956116  and accuracy  0.9688\n","ep  511  training loss:  0.010474764392111688\n","valid loss  0.11794489622116089  and accuracy  0.9804\n","ep  512  training loss:  0.009754875958670056\n","valid loss  0.012901176698505878  and accuracy  0.9948\n","ep  513  training loss:  0.00861972939337033\n","valid loss  0.008734590373933315  and accuracy  0.9964\n","ep  514  training loss:  0.013010485115822465\n","valid loss  0.007748594041913748  and accuracy  0.9968\n","ep  515  training loss:  0.010848193997909393\n","valid loss  0.005594081245362759  and accuracy  0.9976\n","ep  516  training loss:  0.009790722911309037\n","valid loss  0.013075568713247776  and accuracy  0.9956\n","ep  517  training loss:  0.012368274823291595\n","valid loss  0.007116307970136404  and accuracy  0.9972\n","ep  518  training loss:  0.00886233637357112\n","valid loss  0.006427181418985128  and accuracy  0.9976\n","ep  519  training loss:  0.009684570531776387\n","valid loss  0.03646358475089073  and accuracy  0.9884\n","ep  520  training loss:  0.009511820603781616\n","valid loss  0.01325959898531437  and accuracy  0.9964\n","ep  521  training loss:  0.011049202999669435\n","valid loss  0.031184906139969826  and accuracy  0.9888\n","ep  522  training loss:  0.014175058397250315\n","valid loss  0.01377678383141756  and accuracy  0.9952\n","ep  523  training loss:  0.00937726452604717\n","valid loss  0.14089347422122955  and accuracy  0.9928\n","ep  524  training loss:  0.011508033941540798\n","valid loss  0.005300725810229778  and accuracy  0.9976\n","ep  525  training loss:  0.011475810638079587\n","valid loss  0.004195595625787973  and accuracy  0.998\n","ep  526  training loss:  0.007168789023053272\n","valid loss  0.07633437216281891  and accuracy  0.9964\n","ep  527  training loss:  0.010673685432421604\n","valid loss  0.005587244406342506  and accuracy  0.9984\n","ep  528  training loss:  0.011291641887571912\n","valid loss  0.01578771509230137  and accuracy  0.9964\n","ep  529  training loss:  0.009955317866236595\n","valid loss  0.010950406081974506  and accuracy  0.996\n","ep  530  training loss:  0.01262813240771572\n","valid loss  0.011323376558721066  and accuracy  0.9968\n","ep  531  training loss:  0.0072490076419621735\n","valid loss  0.01295793429017067  and accuracy  0.9972\n","ep  532  training loss:  0.010821935768563372\n","valid loss  0.005988997872918844  and accuracy  0.9984\n","ep  533  training loss:  0.007325420417235778\n","valid loss  0.010099194012582302  and accuracy  0.9968\n","ep  534  training loss:  0.009691561989246969\n","valid loss  0.005322046112269163  and accuracy  0.9984\n","ep  535  training loss:  0.014032533578518231\n","valid loss  0.010289173573255539  and accuracy  0.9964\n","ep  536  training loss:  0.01342194632454122\n","valid loss  0.008211696520447731  and accuracy  0.9964\n","ep  537  training loss:  0.009743211061886425\n","valid loss  0.011432293802499771  and accuracy  0.9968\n","ep  538  training loss:  0.009834261436463395\n","valid loss  0.0069755325093865395  and accuracy  0.9968\n","ep  539  training loss:  0.008199357839571512\n","valid loss  0.01200427208095789  and accuracy  0.9948\n","ep  540  training loss:  0.008890912354881746\n","valid loss  0.02121363766491413  and accuracy  0.9948\n","ep  541  training loss:  0.009315975584113454\n","valid loss  0.0045752087607979774  and accuracy  0.998\n","ep  542  training loss:  0.009838709852612225\n","valid loss  0.015043034218251705  and accuracy  0.9944\n","ep  543  training loss:  0.007431816178780547\n","valid loss  0.005560566205531359  and accuracy  0.998\n","ep  544  training loss:  0.00933241572623784\n","valid loss  0.01685379073023796  and accuracy  0.9972\n","ep  545  training loss:  0.008484404272845896\n","valid loss  0.0076773809269070625  and accuracy  0.9964\n","ep  546  training loss:  0.008053521956358035\n","valid loss  0.0035632560029625893  and accuracy  0.9988\n","ep  547  training loss:  0.00775794195864994\n","valid loss  0.011133878491818905  and accuracy  0.9972\n","ep  548  training loss:  0.009911871932771975\n","valid loss  0.013460718095302582  and accuracy  0.9896\n","ep  549  training loss:  0.008670739115178315\n","valid loss  0.010802384465932846  and accuracy  0.9964\n","ep  550  training loss:  0.010553577900445216\n","valid loss  0.0031388471834361553  and accuracy  0.9988\n","ep  551  training loss:  0.008172729213137187\n","valid loss  0.011710481718182564  and accuracy  0.9964\n","ep  552  training loss:  0.009369517285777416\n","valid loss  0.006379385944455862  and accuracy  0.998\n","ep  553  training loss:  0.010733096415988968\n","valid loss  0.008899779058992863  and accuracy  0.9968\n","ep  554  training loss:  0.010240913984235697\n","valid loss  0.4170415997505188  and accuracy  0.9968\n","ep  555  training loss:  0.013276746041194774\n","valid loss  0.0027485289610922337  and accuracy  0.9992\n","ep  556  training loss:  0.01245325911672765\n","valid loss  0.00524062430486083  and accuracy  0.998\n","ep  557  training loss:  0.013749149488609638\n","valid loss  0.010370313189923763  and accuracy  0.9968\n","ep  558  training loss:  0.011530293628823952\n","valid loss  0.0038357635494321585  and accuracy  0.9984\n","ep  559  training loss:  0.008718340201441608\n","valid loss  0.00826322752982378  and accuracy  0.9964\n","ep  560  training loss:  0.014442932556849678\n","valid loss  0.022108670324087143  and accuracy  0.9936\n","ep  561  training loss:  0.014481814931406645\n","valid loss  0.08867429196834564  and accuracy  0.9716\n","ep  562  training loss:  0.01556845154597457\n","valid loss  0.018589666113257408  and accuracy  0.9948\n","ep  563  training loss:  0.016482154609084185\n","valid loss  0.07094671577215195  and accuracy  0.9868\n","ep  564  training loss:  0.01841106562418437\n","valid loss  0.01754269190132618  and accuracy  0.996\n","ep  565  training loss:  0.01517469819293207\n","valid loss  0.19370608031749725  and accuracy  0.9892\n","ep  566  training loss:  0.016113599200310653\n","valid loss  0.010333951562643051  and accuracy  0.9948\n","ep  567  training loss:  0.01135307168703717\n","valid loss  0.010012764483690262  and accuracy  0.9976\n","ep  568  training loss:  0.01630162132547881\n","valid loss  0.06180402263998985  and accuracy  0.9964\n","ep  569  training loss:  0.015367314561420554\n","valid loss  0.007989206351339817  and accuracy  0.9972\n","ep  570  training loss:  0.01655380152644388\n","valid loss  0.10952939093112946  and accuracy  0.9816\n","ep  571  training loss:  0.016584199337219333\n","valid loss  0.008219406008720398  and accuracy  0.9972\n","ep  572  training loss:  0.013273615087050989\n","valid loss  0.007354848552495241  and accuracy  0.998\n","ep  573  training loss:  0.012462416136176324\n","valid loss  0.008503899909555912  and accuracy  0.9972\n","ep  574  training loss:  0.018959289095602915\n","valid loss  0.013193706050515175  and accuracy  0.9932\n","ep  575  training loss:  0.01623083344330321\n","valid loss  0.010050556622445583  and accuracy  0.9968\n","ep  576  training loss:  0.01239543270530736\n","valid loss  0.005422772839665413  and accuracy  0.9984\n","ep  577  training loss:  0.016315106873255776\n","valid loss  0.01183020044118166  and accuracy  0.9972\n","ep  578  training loss:  0.011717465055851964\n","valid loss  0.003919654525816441  and accuracy  0.9984\n","ep  579  training loss:  0.01360721243740085\n","valid loss  0.010490496642887592  and accuracy  0.9964\n","ep  580  training loss:  0.015197410659852695\n","valid loss  0.040389735251665115  and accuracy  0.9968\n","ep  581  training loss:  0.014364693708400706\n","valid loss  0.02402343973517418  and accuracy  0.9908\n","ep  582  training loss:  0.0127390491460461\n","valid loss  0.006934759672731161  and accuracy  0.9984\n","ep  583  training loss:  0.015137364655074044\n","valid loss  0.008445649407804012  and accuracy  0.9988\n","ep  584  training loss:  0.016181272377537366\n","valid loss  0.006326736882328987  and accuracy  0.9984\n","ep  585  training loss:  0.014783646288714113\n","valid loss  0.023357955738902092  and accuracy  0.9944\n","ep  586  training loss:  0.014621460097070283\n","valid loss  0.017354818060994148  and accuracy  0.9952\n","ep  587  training loss:  0.01895244641275216\n","valid loss  0.012408457696437836  and accuracy  0.9968\n","ep  588  training loss:  0.019620047947089984\n","valid loss  0.00723221292719245  and accuracy  0.9976\n","ep  589  training loss:  0.016618019953410723\n","valid loss  0.0177395511418581  and accuracy  0.9984\n","ep  590  training loss:  0.015050680276996185\n","valid loss  0.019848043099045753  and accuracy  0.9916\n","ep  591  training loss:  0.013055349574039215\n","valid loss  0.007212156895548105  and accuracy  0.998\n","ep  592  training loss:  0.012271984954305632\n","valid loss  0.006685572676360607  and accuracy  0.998\n","ep  593  training loss:  0.013569518402376813\n","valid loss  0.05693120136857033  and accuracy  0.988\n","ep  594  training loss:  0.013366153405601782\n","valid loss  0.005870761349797249  and accuracy  0.9976\n","ep  595  training loss:  0.010982846002715423\n","valid loss  0.015582223422825336  and accuracy  0.9936\n","ep  596  training loss:  0.010228064995720617\n","valid loss  0.00879634078592062  and accuracy  0.9964\n","ep  597  training loss:  0.011518589834581358\n","valid loss  0.06534173339605331  and accuracy  0.9968\n","ep  598  training loss:  0.012257853837193449\n","valid loss  0.0145723232999444  and accuracy  0.9948\n","ep  599  training loss:  0.015682167037576664\n","valid loss  0.008223265409469604  and accuracy  0.9972\n","ep  600  training loss:  0.010486733452754107\n","valid loss  0.017737815156579018  and accuracy  0.9932\n","ep  601  training loss:  0.009484124550202585\n","valid loss  0.007601441815495491  and accuracy  0.9968\n","ep  602  training loss:  0.011802764900335667\n","valid loss  0.006606277544051409  and accuracy  0.9984\n","ep  603  training loss:  0.01529365309119965\n","valid loss  0.023191183805465698  and accuracy  0.9944\n","ep  604  training loss:  0.012396955236305037\n","valid loss  0.007381807081401348  and accuracy  0.9984\n","ep  605  training loss:  0.010638512925478192\n","valid loss  0.0071685342118144035  and accuracy  0.9984\n","ep  606  training loss:  0.011925343662484583\n","valid loss  0.012577788904309273  and accuracy  0.998\n","ep  607  training loss:  0.013194462747708916\n","valid loss  0.008483652025461197  and accuracy  0.9964\n","ep  608  training loss:  0.012637042477715651\n","valid loss  0.06513652950525284  and accuracy  0.9956\n","ep  609  training loss:  0.011317721241901713\n","valid loss  0.004874963313341141  and accuracy  0.9988\n","ep  610  training loss:  0.010178563321306648\n","valid loss  0.012168502435088158  and accuracy  0.9968\n","ep  611  training loss:  0.009258560307032771\n","valid loss  0.00857772957533598  and accuracy  0.9968\n","ep  612  training loss:  0.013593560310407359\n","valid loss  0.008492466993629932  and accuracy  0.9964\n","ep  613  training loss:  0.010868144936812209\n","valid loss  0.008486597798764706  and accuracy  0.9964\n","ep  614  training loss:  0.01076968270022466\n","valid loss  0.013887008652091026  and accuracy  0.9972\n","ep  615  training loss:  0.011484065756254784\n","valid loss  0.009501948952674866  and accuracy  0.9972\n","ep  616  training loss:  0.01059549814724933\n","valid loss  0.002827885327860713  and accuracy  0.9988\n","ep  617  training loss:  0.011829483597315069\n","valid loss  0.9187051057815552  and accuracy  0.9812\n","ep  618  training loss:  0.01280434635095129\n","valid loss  0.4471093714237213  and accuracy  0.9972\n","ep  619  training loss:  0.010303437502158118\n","valid loss  0.015862779691815376  and accuracy  0.9948\n","ep  620  training loss:  0.01093931527714076\n","valid loss  0.004078577272593975  and accuracy  0.9988\n","ep  621  training loss:  0.01175189181350869\n","valid loss  0.17896462976932526  and accuracy  0.996\n","ep  622  training loss:  0.010521353856242586\n","valid loss  0.00682967621833086  and accuracy  0.9976\n","ep  623  training loss:  0.010109337239016396\n","valid loss  0.004895416088402271  and accuracy  0.9976\n","ep  624  training loss:  0.009557757350020573\n","valid loss  0.18693150579929352  and accuracy  0.9968\n","ep  625  training loss:  0.01380324890620959\n","valid loss  0.22256462275981903  and accuracy  0.984\n","ep  626  training loss:  0.008795269341759612\n","valid loss  0.00955862458795309  and accuracy  0.9976\n","ep  627  training loss:  0.01111786021893041\n","valid loss  0.004164361860603094  and accuracy  0.998\n","ep  628  training loss:  0.010229961087228095\n","valid loss  0.007423167582601309  and accuracy  0.9976\n","ep  629  training loss:  0.008467642307564756\n","valid loss  0.0048704384826123714  and accuracy  0.9984\n","ep  630  training loss:  0.009340786066451369\n","valid loss  0.0035124740097671747  and accuracy  0.9992\n","ep  631  training loss:  0.010480134223295239\n","valid loss  0.030365463346242905  and accuracy  0.9968\n","ep  632  training loss:  0.011875196600147212\n","valid loss  0.011229750700294971  and accuracy  0.9968\n","ep  633  training loss:  0.013149947060627021\n","valid loss  0.013590628281235695  and accuracy  0.9952\n","ep  634  training loss:  0.008921539386685275\n","valid loss  0.004395277705043554  and accuracy  0.9988\n","ep  635  training loss:  0.011062364270010614\n","valid loss  13.047256469726562  and accuracy  0.9984\n","ep  636  training loss:  0.008378509661169783\n","valid loss  0.008362289518117905  and accuracy  0.996\n","ep  637  training loss:  0.008511846879442437\n","valid loss  1.0837323665618896  and accuracy  0.9972\n","ep  638  training loss:  0.009595096447036433\n","valid loss  0.022946815937757492  and accuracy  0.996\n","ep  639  training loss:  0.0090761711328355\n","valid loss  0.010851803235709667  and accuracy  0.9968\n","ep  640  training loss:  0.00875107609288543\n","valid loss  0.005475747399032116  and accuracy  0.9984\n","ep  641  training loss:  0.010441205507469914\n","valid loss  0.011058064177632332  and accuracy  0.9956\n","ep  642  training loss:  0.011442566204447072\n","valid loss  0.006076756864786148  and accuracy  0.9964\n","ep  643  training loss:  0.009980787960679846\n","valid loss  0.005546963308006525  and accuracy  0.998\n","ep  644  training loss:  0.00852134829596976\n","valid loss  0.006064058281481266  and accuracy  0.9972\n","ep  645  training loss:  0.01267919238229045\n","valid loss  0.0051604751497507095  and accuracy  0.9976\n","ep  646  training loss:  0.009852753453333607\n","valid loss  0.021577171981334686  and accuracy  0.9912\n","ep  647  training loss:  0.010466381163477121\n","valid loss  0.023320365697145462  and accuracy  0.9964\n","ep  648  training loss:  0.012416232445002758\n","valid loss  0.08439277112483978  and accuracy  0.9804\n","ep  649  training loss:  0.008799842310523723\n","valid loss  0.002763669239357114  and accuracy  0.9988\n","ep  650  training loss:  0.009178091102397937\n","valid loss  0.027132617309689522  and accuracy  0.9932\n","ep  651  training loss:  0.012106283582798895\n","valid loss  0.4751230478286743  and accuracy  0.9936\n","ep  652  training loss:  0.009296715876154694\n","valid loss  0.047661639750003815  and accuracy  0.982\n","ep  653  training loss:  0.011395294213612779\n","valid loss  0.008010732010006905  and accuracy  0.9968\n","ep  654  training loss:  0.016039773123268285\n","valid loss  0.008199630305171013  and accuracy  0.9956\n","ep  655  training loss:  0.01212112328742389\n","valid loss  0.10980908572673798  and accuracy  0.98\n","ep  656  training loss:  0.009014369162085633\n","valid loss  1.2938904762268066  and accuracy  0.9908\n","ep  657  training loss:  0.010031667014629651\n","valid loss  0.007085951045155525  and accuracy  0.9972\n","ep  658  training loss:  0.009893060012098985\n","valid loss  0.00998004712164402  and accuracy  0.9964\n","ep  659  training loss:  0.01047705191361637\n","valid loss  0.017707811668515205  and accuracy  0.9956\n","ep  660  training loss:  0.008234609566206861\n","valid loss  0.006932112853974104  and accuracy  0.9976\n","ep  661  training loss:  0.00909231184910948\n","valid loss  0.00534928310662508  and accuracy  0.998\n","ep  662  training loss:  0.012763590614546414\n","valid loss  0.012486509047448635  and accuracy  0.9968\n","ep  663  training loss:  0.01158598758888557\n","valid loss  0.0020270291715860367  and accuracy  0.9988\n","ep  664  training loss:  0.007775980403651905\n","valid loss  0.007298996672034264  and accuracy  0.9964\n","ep  665  training loss:  0.00923982289526922\n","valid loss  0.004868584219366312  and accuracy  0.9976\n","ep  666  training loss:  0.014498115216444764\n","valid loss  0.004934364929795265  and accuracy  0.9988\n","ep  667  training loss:  0.009199479883167102\n","valid loss  0.03527408093214035  and accuracy  0.9976\n","ep  668  training loss:  0.009759293047565109\n","valid loss  0.010816154070198536  and accuracy  0.9976\n","ep  669  training loss:  0.010952564335849404\n","valid loss  0.006332483142614365  and accuracy  0.9976\n","ep  670  training loss:  0.00952139224375208\n","valid loss  0.0061749788001179695  and accuracy  0.9984\n","ep  671  training loss:  0.007856582991961673\n","valid loss  0.005518562160432339  and accuracy  0.9976\n","ep  672  training loss:  0.009472119770052577\n","valid loss  0.017019977793097496  and accuracy  0.9976\n","ep  673  training loss:  0.009379865547207062\n","valid loss  0.028875000774860382  and accuracy  0.996\n","ep  674  training loss:  0.0069045632125703145\n","valid loss  0.031865913420915604  and accuracy  0.9936\n","ep  675  training loss:  0.010537051554273851\n","valid loss  0.008101443760097027  and accuracy  0.9972\n","ep  676  training loss:  0.010407583192735685\n","valid loss  0.005511889699846506  and accuracy  0.9984\n","ep  677  training loss:  0.01347661496573702\n","valid loss  0.018977290019392967  and accuracy  0.9972\n","ep  678  training loss:  0.011575305356055282\n","valid loss  0.007558782119303942  and accuracy  0.9976\n","ep  679  training loss:  0.010758363555747788\n","valid loss  0.005771797150373459  and accuracy  0.998\n","ep  680  training loss:  0.010299311492618052\n","valid loss  0.1309855878353119  and accuracy  0.9956\n","ep  681  training loss:  0.008252512489808614\n","valid loss  0.005313457455486059  and accuracy  0.9984\n","ep  682  training loss:  0.008194271345154214\n","valid loss  0.007600246462970972  and accuracy  0.9972\n","ep  683  training loss:  0.009332285604307919\n","valid loss  0.001193402917124331  and accuracy  0.9992\n","ep  684  training loss:  0.008955298768628984\n","valid loss  0.03366992622613907  and accuracy  0.9976\n","ep  685  training loss:  0.009675567125818633\n","valid loss  0.005614019930362701  and accuracy  0.9984\n","ep  686  training loss:  0.00853798085876132\n","valid loss  0.023649025708436966  and accuracy  0.996\n","ep  687  training loss:  0.008417522624658093\n","valid loss  0.00885939784348011  and accuracy  0.9956\n","ep  688  training loss:  0.012557353771477035\n","valid loss  0.006801104173064232  and accuracy  0.9976\n","ep  689  training loss:  0.008021664255832188\n","valid loss  0.012812169268727303  and accuracy  0.998\n","ep  690  training loss:  0.010505418109509194\n","valid loss  0.017546651884913445  and accuracy  0.9952\n","ep  691  training loss:  0.006768407976670686\n","valid loss  0.0029942591208964586  and accuracy  0.9992\n","ep  692  training loss:  0.01051826678640786\n","valid loss  0.004847296513617039  and accuracy  0.9984\n","ep  693  training loss:  0.007274161618974842\n","valid loss  0.011989870108664036  and accuracy  0.9976\n","ep  694  training loss:  0.007223946811172573\n","valid loss  0.00489374715834856  and accuracy  0.998\n","ep  695  training loss:  0.009317837790686212\n","valid loss  0.014392850920557976  and accuracy  0.9948\n","ep  696  training loss:  0.009277961063668497\n","valid loss  0.008155465126037598  and accuracy  0.9976\n","ep  697  training loss:  0.007887041976608243\n","valid loss  0.00589844910427928  and accuracy  0.9984\n","ep  698  training loss:  0.011123657367629233\n","valid loss  0.18845410645008087  and accuracy  0.9804\n","ep  699  training loss:  0.007877449834845132\n","valid loss  0.006820401176810265  and accuracy  0.9964\n","ep  700  training loss:  0.00791974605835006\n","valid loss  0.007314450107514858  and accuracy  0.9976\n","ep  701  training loss:  0.009165190518301876\n","valid loss  0.015656711533665657  and accuracy  0.9956\n","ep  702  training loss:  0.007322389796665133\n","valid loss  0.014438766054809093  and accuracy  0.9948\n","ep  703  training loss:  0.009836916240726103\n","valid loss  0.0030139510054141283  and accuracy  0.9996\n","ep  704  training loss:  0.009748712500291534\n","valid loss  0.07959501445293427  and accuracy  0.9796\n","ep  705  training loss:  0.007002625287905724\n","valid loss  0.005232682917267084  and accuracy  0.9984\n","ep  706  training loss:  0.006517382582688523\n","valid loss  0.0119283152744174  and accuracy  0.998\n","ep  707  training loss:  0.007319985061965181\n","valid loss  0.013244418427348137  and accuracy  0.9952\n","ep  708  training loss:  0.008600568980592483\n","valid loss  0.009142046794295311  and accuracy  0.9976\n","ep  709  training loss:  0.008910105498809502\n","valid loss  0.2414952665567398  and accuracy  0.968\n","ep  710  training loss:  0.006328702159169514\n","valid loss  0.09382369369268417  and accuracy  0.984\n","ep  711  training loss:  0.00806033426302693\n","valid loss  0.004653851501643658  and accuracy  0.9988\n","ep  712  training loss:  0.006169354602620028\n","valid loss  0.24991916120052338  and accuracy  0.9968\n","ep  713  training loss:  0.008487885489905244\n","valid loss  0.005865305196493864  and accuracy  0.9984\n","ep  714  training loss:  0.008570605860467265\n","valid loss  0.01112891361117363  and accuracy  0.996\n","ep  715  training loss:  0.007573578704483706\n","valid loss  0.00528767379000783  and accuracy  0.9984\n","ep  716  training loss:  0.00580567127825527\n","valid loss  0.0039057934191077948  and accuracy  0.9988\n","ep  717  training loss:  0.0068907918300173815\n","valid loss  0.00988654512912035  and accuracy  0.9968\n","ep  718  training loss:  0.010065283213274582\n","valid loss  0.005252595990896225  and accuracy  0.9984\n","ep  719  training loss:  0.008251272715369714\n","valid loss  0.008163982070982456  and accuracy  0.9988\n","ep  720  training loss:  0.007607777977925978\n","valid loss  0.02685208059847355  and accuracy  0.998\n","ep  721  training loss:  0.008458121354705509\n","valid loss  0.0073108854703605175  and accuracy  0.9984\n","ep  722  training loss:  0.007983415397727662\n","valid loss  0.007731780409812927  and accuracy  0.9968\n","ep  723  training loss:  0.007455593496896789\n","valid loss  0.10352713614702225  and accuracy  0.9984\n","ep  724  training loss:  0.00791871108135033\n","valid loss  1.8574659824371338  and accuracy  0.9964\n","ep  725  training loss:  0.007450597768695165\n","valid loss  0.009030772373080254  and accuracy  0.9964\n","ep  726  training loss:  0.007275296315766354\n","valid loss  0.028223147615790367  and accuracy  0.9968\n","ep  727  training loss:  0.011320113527180299\n","valid loss  0.0034445442724972963  and accuracy  0.9992\n","ep  728  training loss:  0.009338727049920025\n","valid loss  0.013931561261415482  and accuracy  0.9956\n","ep  729  training loss:  0.009271037768797147\n","valid loss  0.007471567485481501  and accuracy  0.9976\n","ep  730  training loss:  0.0062812806394315695\n","valid loss  0.005567209329456091  and accuracy  0.9992\n","ep  731  training loss:  0.007884529263787795\n","valid loss  0.010999281890690327  and accuracy  0.9984\n","ep  732  training loss:  0.006111712569934568\n","valid loss  0.0036072705406695604  and accuracy  0.998\n","ep  733  training loss:  0.009483022601206856\n","valid loss  0.02760300040245056  and accuracy  0.9964\n","ep  734  training loss:  0.00805056968171198\n","valid loss  0.006373477168381214  and accuracy  0.998\n","ep  735  training loss:  0.0085544346366783\n","valid loss  0.0031406348571181297  and accuracy  0.9988\n","ep  736  training loss:  0.008060006171428807\n","valid loss  0.01768399588763714  and accuracy  0.9984\n","ep  737  training loss:  0.006374549968757798\n","valid loss  0.0036193171981722116  and accuracy  0.9988\n","ep  738  training loss:  0.006839235590291778\n","valid loss  0.0028655999340116978  and accuracy  0.9996\n","ep  739  training loss:  0.00662375747510287\n","valid loss  0.004621236585080624  and accuracy  0.9992\n","ep  740  training loss:  0.005716920461419883\n","valid loss  0.0031782116275280714  and accuracy  0.9992\n","ep  741  training loss:  0.007537296791898097\n","valid loss  0.012955717742443085  and accuracy  0.996\n","ep  742  training loss:  0.010963383739396888\n","valid loss  0.07387323677539825  and accuracy  0.9808\n","ep  743  training loss:  0.008536362703985534\n","valid loss  0.15297195315361023  and accuracy  0.9972\n","ep  744  training loss:  0.008344338807031643\n","valid loss  0.03815468028187752  and accuracy  0.9956\n","ep  745  training loss:  0.008408714450051643\n","valid loss  0.00862608291208744  and accuracy  0.9972\n","ep  746  training loss:  0.006722405721579888\n","valid loss  0.006305916700512171  and accuracy  0.998\n","ep  747  training loss:  0.007016939070144591\n","valid loss  0.007030564825981855  and accuracy  0.996\n","ep  748  training loss:  0.008596559944152109\n","valid loss  0.10305311530828476  and accuracy  0.996\n","ep  749  training loss:  0.00700983497667104\n","valid loss  0.0029166997410357  and accuracy  0.9988\n","ep  750  training loss:  0.005540618876385186\n","valid loss  0.011383253149688244  and accuracy  0.998\n","ep  751  training loss:  0.005297882488052147\n","valid loss  0.005656336434185505  and accuracy  0.998\n","ep  752  training loss:  0.008038650465024914\n","valid loss  0.005281995981931686  and accuracy  0.9976\n","ep  753  training loss:  0.005100796153214223\n","valid loss  0.005430961959064007  and accuracy  0.998\n","ep  754  training loss:  0.00887681805999581\n","valid loss  0.05629202350974083  and accuracy  0.9976\n","ep  755  training loss:  0.006739329597474011\n","valid loss  0.007564620114862919  and accuracy  0.9976\n","ep  756  training loss:  0.0058436686073206155\n","valid loss  0.009997157379984856  and accuracy  0.9964\n","ep  757  training loss:  0.005275798966316464\n","valid loss  0.006411770824342966  and accuracy  0.9976\n","ep  758  training loss:  0.006390941586323992\n","valid loss  0.0074034384451806545  and accuracy  0.9976\n","ep  759  training loss:  0.006973070452783694\n","valid loss  0.006301248446106911  and accuracy  0.9976\n","ep  760  training loss:  0.006068971384992674\n","valid loss  0.003901253454387188  and accuracy  0.9988\n","ep  761  training loss:  0.007872304742012068\n","valid loss  0.03703726455569267  and accuracy  0.9904\n","ep  762  training loss:  0.006707137653429791\n","valid loss  0.07478540390729904  and accuracy  0.9944\n","ep  763  training loss:  0.0065106326666618565\n","valid loss  0.0019168128492310643  and accuracy  0.9996\n","ep  764  training loss:  0.007257476620973104\n","valid loss  0.026562683284282684  and accuracy  0.992\n","ep  765  training loss:  0.005986817251784815\n","valid loss  0.002582595217972994  and accuracy  0.9992\n","ep  766  training loss:  0.009106410402659032\n","valid loss  0.0065491716377437115  and accuracy  0.998\n","ep  767  training loss:  0.00783987701790093\n","valid loss  0.01581219956278801  and accuracy  0.994\n","ep  768  training loss:  0.005665097157822138\n","valid loss  0.006673986092209816  and accuracy  0.9976\n","ep  769  training loss:  0.0059894729550658155\n","valid loss  0.002584361005574465  and accuracy  0.9996\n","ep  770  training loss:  0.007234110811517045\n","valid loss  0.011793743818998337  and accuracy  0.996\n","ep  771  training loss:  0.0071563829262440435\n","valid loss  0.008180046454071999  and accuracy  0.998\n","ep  772  training loss:  0.006044437440286592\n","valid loss  0.005349684506654739  and accuracy  0.9984\n","ep  773  training loss:  0.005345938517493403\n","valid loss  0.00354774366132915  and accuracy  0.9984\n","ep  774  training loss:  0.004890893640925431\n","valid loss  0.0029575671069324017  and accuracy  0.9992\n","ep  775  training loss:  0.005800794682532359\n","valid loss  0.007124374620616436  and accuracy  0.9968\n","ep  776  training loss:  0.006302503023806692\n","valid loss  0.0031381973531097174  and accuracy  0.9988\n","ep  777  training loss:  0.005828048488445709\n","valid loss  0.01433844119310379  and accuracy  0.9984\n","ep  778  training loss:  0.0069546437811279106\n","valid loss  0.002237567910924554  and accuracy  0.9992\n","ep  779  training loss:  0.0068279103440438815\n","valid loss  0.003000736003741622  and accuracy  0.9992\n","ep  780  training loss:  0.0057080058455444655\n","valid loss  0.09535989165306091  and accuracy  0.9868\n","ep  781  training loss:  0.005406817921962938\n","valid loss  0.002135086804628372  and accuracy  0.9996\n","ep  782  training loss:  0.0064178873642080575\n","valid loss  0.01017861906439066  and accuracy  0.9944\n","ep  783  training loss:  0.0075105529982298705\n","valid loss  0.0027773678302764893  and accuracy  0.9996\n","ep  784  training loss:  0.006734809902985009\n","valid loss  0.00436054915189743  and accuracy  0.9984\n","ep  785  training loss:  0.01103680573399039\n","valid loss  0.013887675479054451  and accuracy  0.9952\n","ep  786  training loss:  0.005880683474863381\n","valid loss  0.008233360014855862  and accuracy  0.9964\n","ep  787  training loss:  0.005043552445585765\n","valid loss  0.00789197813719511  and accuracy  0.9964\n","ep  788  training loss:  0.005272309890006192\n","valid loss  0.00457393703982234  and accuracy  0.9988\n","ep  789  training loss:  0.005664836459022079\n","valid loss  0.027755090966820717  and accuracy  0.9984\n","ep  790  training loss:  0.006644788696985016\n","valid loss  0.013658384792506695  and accuracy  0.9976\n","ep  791  training loss:  0.007303382209238366\n","valid loss  0.05156436935067177  and accuracy  0.9796\n","ep  792  training loss:  0.005985865277181482\n","valid loss  0.004487538710236549  and accuracy  0.998\n","ep  793  training loss:  0.006434342660567357\n","valid loss  0.00686524948105216  and accuracy  0.998\n","ep  794  training loss:  0.005958716150613598\n","valid loss  0.009733889251947403  and accuracy  0.9968\n","ep  795  training loss:  0.005858058834186637\n","valid loss  0.0045241438783705235  and accuracy  0.9988\n","ep  796  training loss:  0.004320402513575559\n","valid loss  0.003381510963663459  and accuracy  0.9992\n","ep  797  training loss:  0.006462707534100622\n","valid loss  0.0037605620454996824  and accuracy  0.9984\n","ep  798  training loss:  0.006714778002216098\n","valid loss  0.004526094999164343  and accuracy  0.998\n","ep  799  training loss:  0.005761753302772852\n","valid loss  0.0043307216838002205  and accuracy  0.9972\n","ep  800  training loss:  0.005469181949977521\n","valid loss  0.005332300905138254  and accuracy  0.9988\n","ep  801  training loss:  0.009630526622525401\n","valid loss  0.00353402947075665  and accuracy  0.9984\n","ep  802  training loss:  0.007091469805426975\n","valid loss  0.009407328441739082  and accuracy  0.996\n","ep  803  training loss:  0.005219854244336563\n","valid loss  0.005863864906132221  and accuracy  0.9972\n","ep  804  training loss:  0.005189958649612139\n","valid loss  0.015593665651977062  and accuracy  0.9968\n","ep  805  training loss:  0.006532278856766455\n","valid loss  0.0027586219366639853  and accuracy  0.9988\n","ep  806  training loss:  0.005284225438182321\n","valid loss  0.005327614489942789  and accuracy  0.998\n","ep  807  training loss:  0.005248119944711638\n","valid loss  0.013855704106390476  and accuracy  0.9964\n","ep  808  training loss:  0.006267919034579465\n","valid loss  0.0036122698802500963  and accuracy  0.9988\n","ep  809  training loss:  0.006285243161514704\n","valid loss  0.00864967331290245  and accuracy  0.996\n","ep  810  training loss:  0.005660062426573099\n","valid loss  0.001537881325930357  and accuracy  0.9992\n","ep  811  training loss:  0.005681197110465334\n","valid loss  0.005530267488211393  and accuracy  0.998\n","ep  812  training loss:  0.005643636940012724\n","valid loss  0.03285007178783417  and accuracy  0.9972\n","ep  813  training loss:  0.004717662704358612\n","valid loss  0.005727597512304783  and accuracy  0.998\n","ep  814  training loss:  0.004636899704410971\n","valid loss  0.0037534937728196383  and accuracy  0.9988\n","ep  815  training loss:  0.004973239324489251\n","valid loss  0.0069811418652534485  and accuracy  0.9968\n","ep  816  training loss:  0.00549452602005626\n","valid loss  0.002703410806134343  and accuracy  0.9992\n","ep  817  training loss:  0.005136380006802674\n","valid loss  0.010622164234519005  and accuracy  0.9972\n","ep  818  training loss:  0.006548836178371163\n","valid loss  0.007032051216810942  and accuracy  0.9976\n","ep  819  training loss:  0.007046318454715888\n","valid loss  0.010251728817820549  and accuracy  0.9952\n","ep  820  training loss:  0.006629273558350918\n","valid loss  0.0057299635373055935  and accuracy  0.998\n","ep  821  training loss:  0.012192618347855634\n","valid loss  0.009041735902428627  and accuracy  0.998\n","ep  822  training loss:  0.006721362971521009\n","valid loss  0.005414298735558987  and accuracy  0.9984\n","ep  823  training loss:  0.008415129214301773\n","valid loss  0.010579840280115604  and accuracy  0.9976\n","ep  824  training loss:  0.006029560107944302\n","valid loss  0.005652565509080887  and accuracy  0.9976\n","ep  825  training loss:  0.007318072075177163\n","valid loss  0.0453622080385685  and accuracy  0.9936\n","ep  826  training loss:  0.006783877677992054\n","valid loss  0.03963169828057289  and accuracy  0.9836\n","ep  827  training loss:  0.007931796589263079\n","valid loss  0.0037188942078500986  and accuracy  0.9984\n","ep  828  training loss:  0.005653912078513589\n","valid loss  0.006674407050013542  and accuracy  0.9992\n","ep  829  training loss:  0.008799822986920433\n","valid loss  0.004486748017370701  and accuracy  0.9984\n","ep  830  training loss:  0.008718027841252939\n","valid loss  0.0023054969497025013  and accuracy  0.9988\n","ep  831  training loss:  0.00684134329431339\n","valid loss  0.0015505085466429591  and accuracy  0.9996\n","ep  832  training loss:  0.005590100585624364\n","valid loss  0.05314921215176582  and accuracy  0.9992\n","ep  833  training loss:  0.009012542422934216\n","valid loss  0.1078202873468399  and accuracy  0.9884\n","ep  834  training loss:  0.007879251219790403\n","valid loss  0.0038550682365894318  and accuracy  0.9988\n","ep  835  training loss:  0.00813950564997523\n","valid loss  0.011861191131174564  and accuracy  0.9956\n","ep  836  training loss:  0.005091020322814742\n","valid loss  0.07558651268482208  and accuracy  0.9972\n","ep  837  training loss:  0.007105792851447698\n","valid loss  0.01484963670372963  and accuracy  0.994\n","ep  838  training loss:  0.009055101710787255\n","valid loss  0.02541189454495907  and accuracy  0.998\n","ep  839  training loss:  0.004828918601959826\n","valid loss  0.35319292545318604  and accuracy  0.624\n","ep  840  training loss:  0.005086729038995966\n","valid loss  0.00458215968683362  and accuracy  0.9972\n","ep  841  training loss:  0.004495692664801948\n","valid loss  0.005038828123360872  and accuracy  0.9984\n","ep  842  training loss:  0.006418377612862749\n","valid loss  0.0035869483835995197  and accuracy  0.9992\n","ep  843  training loss:  0.005738297119055037\n","valid loss  0.008386222645640373  and accuracy  0.9964\n","ep  844  training loss:  0.005814349050279532\n","valid loss  0.004942664876580238  and accuracy  0.9976\n","ep  845  training loss:  0.005481633902573944\n","valid loss  0.005387602373957634  and accuracy  0.998\n","ep  846  training loss:  0.004982842196211314\n","valid loss  0.0023813918232917786  and accuracy  0.9992\n","ep  847  training loss:  0.006770276200727093\n","valid loss  0.002731551881879568  and accuracy  0.9984\n","ep  848  training loss:  0.007927002063908655\n","valid loss  0.027926284819841385  and accuracy  0.992\n","ep  849  training loss:  0.0074223558957609796\n","valid loss  0.005797071382403374  and accuracy  0.9988\n","ep  850  training loss:  0.0061212595350762745\n","valid loss  0.011048892512917519  and accuracy  0.9988\n","ep  851  training loss:  0.006118970956558559\n","valid loss  0.009640893898904324  and accuracy  0.9968\n","ep  852  training loss:  0.0052186095693804065\n","valid loss  0.007387271150946617  and accuracy  0.9976\n","ep  853  training loss:  0.004828969489174204\n","valid loss  0.006424117833375931  and accuracy  0.9976\n","ep  854  training loss:  0.005514372063852041\n","valid loss  0.006857682019472122  and accuracy  0.9972\n","ep  855  training loss:  0.005984694691320164\n","valid loss  0.01016709953546524  and accuracy  0.996\n","ep  856  training loss:  0.005204236299370553\n","valid loss  0.01110770832747221  and accuracy  0.9968\n","ep  857  training loss:  0.006588584652452108\n","valid loss  0.004636326804757118  and accuracy  0.9988\n","ep  858  training loss:  0.0064715952503838275\n","valid loss  0.004843925125896931  and accuracy  0.9984\n","ep  859  training loss:  0.007575201851893323\n","valid loss  0.010055757127702236  and accuracy  0.998\n","ep  860  training loss:  0.005622828075838871\n","valid loss  0.005034284200519323  and accuracy  0.9984\n","ep  861  training loss:  0.005659034853108211\n","valid loss  0.0029079748783260584  and accuracy  0.9988\n","ep  862  training loss:  0.007003321025579247\n","valid loss  0.011163059622049332  and accuracy  0.9956\n","ep  863  training loss:  0.007353249268639597\n","valid loss  0.008847351185977459  and accuracy  0.9972\n","ep  864  training loss:  0.006089524807641494\n","valid loss  0.009264311753213406  and accuracy  0.9972\n","ep  865  training loss:  0.005392099003753575\n","valid loss  0.004183187615126371  and accuracy  0.9988\n","ep  866  training loss:  0.006751468318230418\n","valid loss  0.053168658167123795  and accuracy  0.998\n","ep  867  training loss:  0.010436984447517478\n","valid loss  0.003984315320849419  and accuracy  0.9988\n","ep  868  training loss:  0.006995275697900251\n","valid loss  0.009213829413056374  and accuracy  0.9968\n","ep  869  training loss:  0.007891666972899674\n","valid loss  0.0029836739413440228  and accuracy  0.9992\n","ep  870  training loss:  0.008248943173611788\n","valid loss  0.002849797485396266  and accuracy  0.9988\n","ep  871  training loss:  0.005227277371795412\n","valid loss  0.005931441206485033  and accuracy  0.998\n","ep  872  training loss:  0.006687974443862887\n","valid loss  0.004076563287526369  and accuracy  0.9984\n","ep  873  training loss:  0.005236104165588417\n","valid loss  0.008641181513667107  and accuracy  0.9984\n","ep  874  training loss:  0.004109160719432052\n","valid loss  0.3701306879520416  and accuracy  0.9972\n","ep  875  training loss:  0.005792038858143381\n","valid loss  0.006995835341513157  and accuracy  0.998\n","ep  876  training loss:  0.007035986752263689\n","valid loss  0.0725727453827858  and accuracy  0.9988\n","ep  877  training loss:  0.005065423188326495\n","valid loss  0.007943045347929  and accuracy  0.9972\n","ep  878  training loss:  0.005214521999537755\n","valid loss  0.008032398298382759  and accuracy  0.9972\n","ep  879  training loss:  0.004762391001437513\n","valid loss  0.008494819514453411  and accuracy  0.9964\n","ep  880  training loss:  0.004787039908195934\n","valid loss  0.013205423019826412  and accuracy  0.9972\n","ep  881  training loss:  0.004902977967830598\n","valid loss  0.010740095749497414  and accuracy  0.9972\n","ep  882  training loss:  0.004483572037335314\n","valid loss  0.0041786241345107555  and accuracy  0.998\n","ep  883  training loss:  0.005123185650176084\n","valid loss  0.00516132265329361  and accuracy  0.9976\n","ep  884  training loss:  0.00490200701982214\n","valid loss  0.005383024457842112  and accuracy  0.9988\n","ep  885  training loss:  0.00527507199809041\n","valid loss  0.002016569022089243  and accuracy  0.9988\n","ep  886  training loss:  0.004988903645284968\n","valid loss  0.0038188111502677202  and accuracy  0.9984\n","ep  887  training loss:  0.00514155134453724\n","valid loss  0.007425174117088318  and accuracy  0.996\n","ep  888  training loss:  0.004975965381559053\n","valid loss  0.007979669608175755  and accuracy  0.9976\n","ep  889  training loss:  0.005125286827251536\n","valid loss  0.06355244666337967  and accuracy  0.9892\n","ep  890  training loss:  0.0045545692236795545\n","valid loss  0.007567569613456726  and accuracy  0.9976\n","ep  891  training loss:  0.0048552384282361015\n","valid loss  0.0031800707802176476  and accuracy  0.9992\n","ep  892  training loss:  0.004039912696010005\n","valid loss  0.0014273960841819644  and accuracy  0.9996\n","ep  893  training loss:  0.005386010186181885\n","valid loss  0.006063329521566629  and accuracy  0.9984\n","ep  894  training loss:  0.005292513693568021\n","valid loss  0.0035073752515017986  and accuracy  0.9992\n","ep  895  training loss:  0.004995759894241351\n","valid loss  0.0630466639995575  and accuracy  0.9972\n","ep  896  training loss:  0.004274055388268288\n","valid loss  0.004760644398629665  and accuracy  0.9984\n","ep  897  training loss:  0.004565774164478848\n","valid loss  0.03684505075216293  and accuracy  0.9968\n","ep  898  training loss:  0.0053000699033832616\n","valid loss  0.0120075149461627  and accuracy  0.996\n","ep  899  training loss:  0.005631407543111754\n","valid loss  0.028300628066062927  and accuracy  0.9968\n","ep  900  training loss:  0.0071082027459455615\n","valid loss  0.0011114737717434764  and accuracy  0.9996\n","ep  901  training loss:  0.005908817011579167\n","valid loss  0.018313271924853325  and accuracy  0.9964\n","ep  902  training loss:  0.004797982943792464\n","valid loss  0.002945209387689829  and accuracy  0.9992\n","ep  903  training loss:  0.0036836513467652173\n","valid loss  0.07859471440315247  and accuracy  0.9968\n","ep  904  training loss:  0.0059545519409074325\n","valid loss  0.03039407916367054  and accuracy  0.998\n","ep  905  training loss:  0.0036179779290135016\n","valid loss  0.009715242311358452  and accuracy  0.996\n","ep  906  training loss:  0.006618871910357043\n","valid loss  0.0075057982467114925  and accuracy  0.998\n","ep  907  training loss:  0.003445469885290073\n","valid loss  0.003293077927082777  and accuracy  0.9988\n","ep  908  training loss:  0.004104086376079978\n","valid loss  0.004994214978069067  and accuracy  0.9972\n","ep  909  training loss:  0.00475145401675745\n","valid loss  0.0023496325593441725  and accuracy  0.9992\n","ep  910  training loss:  0.004392218539055117\n","valid loss  0.005755047779530287  and accuracy  0.998\n","ep  911  training loss:  0.004721355135841772\n","valid loss  0.2593369483947754  and accuracy  0.9972\n","ep  912  training loss:  0.004683271394373883\n","valid loss  0.007276425138115883  and accuracy  0.9968\n","ep  913  training loss:  0.0038250946649709947\n","valid loss  0.0043667699210345745  and accuracy  0.9988\n","ep  914  training loss:  0.004328688203662711\n","valid loss  0.0041680242866277695  and accuracy  0.9988\n","ep  915  training loss:  0.004360925004318452\n","valid loss  0.002900266321375966  and accuracy  0.9988\n","ep  916  training loss:  0.0044038741742434264\n","valid loss  0.003565590362995863  and accuracy  0.9992\n","ep  917  training loss:  0.0039336314829451794\n","valid loss  0.003518035402521491  and accuracy  0.9984\n","ep  918  training loss:  0.004923761006353281\n","valid loss  0.002277241786941886  and accuracy  0.9988\n","ep  919  training loss:  0.004939774440793883\n","valid loss  0.006922817323356867  and accuracy  0.9984\n","ep  920  training loss:  0.005475268049545922\n","valid loss  0.04410656914114952  and accuracy  0.9872\n","ep  921  training loss:  0.00484723115500127\n","valid loss  0.013432424515485764  and accuracy  0.9976\n","ep  922  training loss:  0.004482532907895218\n","valid loss  0.00582899572327733  and accuracy  0.9976\n","ep  923  training loss:  0.004247109536676854\n","valid loss  0.0030218223109841347  and accuracy  0.9992\n","ep  924  training loss:  0.004977324163562075\n","valid loss  0.002078019781038165  and accuracy  0.9996\n","ep  925  training loss:  0.004401589058138152\n","valid loss  0.002281653229147196  and accuracy  0.9996\n","ep  926  training loss:  0.003367170292553451\n","valid loss  0.020039992406964302  and accuracy  0.994\n","ep  927  training loss:  0.0032574032578347616\n","valid loss  0.002422217046841979  and accuracy  0.9988\n","ep  928  training loss:  0.006224039056409185\n","valid loss  0.0033649399410933256  and accuracy  0.9992\n","ep  929  training loss:  0.00654204859072018\n","valid loss  0.0028634415939450264  and accuracy  0.9992\n","ep  930  training loss:  0.004737870384928012\n","valid loss  0.0022887145169079304  and accuracy  0.9988\n","ep  931  training loss:  0.0047116218908088685\n","valid loss  0.005698021501302719  and accuracy  0.9984\n","ep  932  training loss:  0.004837157740095454\n","valid loss  0.0028567712288349867  and accuracy  0.9992\n","ep  933  training loss:  0.0047096930942488275\n","valid loss  0.005171051248908043  and accuracy  0.998\n","ep  934  training loss:  0.0053853335570554165\n","valid loss  0.10598945617675781  and accuracy  0.998\n","ep  935  training loss:  0.005887705610116497\n","valid loss  0.0022676510270684958  and accuracy  0.9988\n","ep  936  training loss:  0.0072156543366924735\n","valid loss  0.005749429110437632  and accuracy  0.9984\n","ep  937  training loss:  0.005026637960786225\n","valid loss  0.002701581222936511  and accuracy  0.9988\n","ep  938  training loss:  0.006892073734036244\n","valid loss  0.006354056764394045  and accuracy  0.9972\n","ep  939  training loss:  0.004933124230381793\n","valid loss  0.012384764850139618  and accuracy  0.9988\n","ep  940  training loss:  0.00792837764886404\n","valid loss  0.017018908634781837  and accuracy  0.9964\n","ep  941  training loss:  0.006090069232834122\n","valid loss  0.0029865477699786425  and accuracy  0.9992\n","ep  942  training loss:  0.005273500140570161\n","valid loss  0.04255126789212227  and accuracy  0.9892\n","ep  943  training loss:  0.005991658112905994\n","valid loss  0.01173918042331934  and accuracy  0.9976\n","ep  944  training loss:  0.004786390100510707\n","valid loss  0.0577736459672451  and accuracy  0.9984\n","ep  945  training loss:  0.006472739523984278\n","valid loss  0.010664848610758781  and accuracy  0.9972\n","ep  946  training loss:  0.004510413681237906\n","valid loss  0.011534374207258224  and accuracy  0.996\n","ep  947  training loss:  0.0045743192512495735\n","valid loss  0.0025512059219181538  and accuracy  0.9988\n","ep  948  training loss:  0.006271434286104273\n","valid loss  0.00574884470552206  and accuracy  0.998\n","ep  949  training loss:  0.00555099138944339\n","valid loss  0.005059033632278442  and accuracy  0.9988\n","ep  950  training loss:  0.0036171356992504337\n","valid loss  0.003155947895720601  and accuracy  0.9992\n","ep  951  training loss:  0.004380618188766071\n","valid loss  0.011099984869360924  and accuracy  0.9968\n","ep  952  training loss:  0.006123366766665396\n","valid loss  0.009016426280140877  and accuracy  0.9972\n","ep  953  training loss:  0.005880378597489056\n","valid loss  0.005717527586966753  and accuracy  0.9976\n","ep  954  training loss:  0.004872393605046221\n","valid loss  0.008291330188512802  and accuracy  0.9964\n","ep  955  training loss:  0.007823750277390375\n","valid loss  0.016723239794373512  and accuracy  0.9944\n","ep  956  training loss:  0.005012658030464478\n","valid loss  0.003629205282777548  and accuracy  0.998\n","ep  957  training loss:  0.006940016320149615\n","valid loss  0.00338340294547379  and accuracy  0.9988\n","ep  958  training loss:  0.004167464392480345\n","valid loss  0.13227640092372894  and accuracy  0.9856\n","ep  959  training loss:  0.005944886321573773\n","valid loss  0.008707474917173386  and accuracy  0.9968\n","ep  960  training loss:  0.005094284116115869\n","valid loss  0.003253314644098282  and accuracy  0.9988\n","ep  961  training loss:  0.004679768971845754\n","valid loss  0.0014755791053175926  and accuracy  0.9992\n","ep  962  training loss:  0.007008444902680925\n","valid loss  0.417767196893692  and accuracy  0.9828\n","ep  963  training loss:  0.003744610768941199\n","valid loss  0.008312663994729519  and accuracy  0.9956\n","ep  964  training loss:  0.006738305260573723\n","valid loss  0.007493907120078802  and accuracy  0.9972\n","ep  965  training loss:  0.0068627013513827976\n","valid loss  0.0028518675826489925  and accuracy  0.9984\n","ep  966  training loss:  0.004808473189735785\n","valid loss  0.009832075797021389  and accuracy  0.9968\n","ep  967  training loss:  0.006175254295266639\n","valid loss  0.0049948482774198055  and accuracy  0.9988\n","ep  968  training loss:  0.005380937109891211\n","valid loss  0.00493668345734477  and accuracy  0.9992\n","ep  969  training loss:  0.005152474199267941\n","valid loss  0.01058416347950697  and accuracy  0.998\n","ep  970  training loss:  0.004845734340610178\n","valid loss  0.03401478752493858  and accuracy  0.996\n","ep  971  training loss:  0.005840022331603059\n","valid loss  0.004622769542038441  and accuracy  0.9976\n","ep  972  training loss:  0.006584505323728028\n","valid loss  0.003587886458262801  and accuracy  0.9992\n","ep  973  training loss:  0.005100689165734863\n","valid loss  0.0025683261919766665  and accuracy  0.9992\n","ep  974  training loss:  0.005437210432720145\n","valid loss  0.004915881436318159  and accuracy  0.998\n","ep  975  training loss:  0.006901946433692024\n","valid loss  0.006620205473154783  and accuracy  0.9972\n","ep  976  training loss:  0.007431234858148037\n","valid loss  0.004618033766746521  and accuracy  0.9984\n","ep  977  training loss:  0.008260781191017448\n","valid loss  0.004155580885708332  and accuracy  0.9988\n","ep  978  training loss:  0.009488718223613307\n","valid loss  0.006757830735296011  and accuracy  0.998\n","ep  979  training loss:  0.005608596507115958\n","valid loss  0.005574928130954504  and accuracy  0.9984\n","ep  980  training loss:  0.009220946185043665\n","valid loss  0.005636371206492186  and accuracy  0.9988\n","ep  981  training loss:  0.00495728107352093\n","valid loss  0.06223490089178085  and accuracy  0.9968\n","ep  982  training loss:  0.005061985252515922\n","valid loss  0.004293293226510286  and accuracy  0.998\n","ep  983  training loss:  0.005163025253437407\n","valid loss  0.00780187314376235  and accuracy  0.9984\n","ep  984  training loss:  0.004948800996366873\n","valid loss  0.007617500144988298  and accuracy  0.9968\n","ep  985  training loss:  0.004429685754006882\n","valid loss  0.007131851278245449  and accuracy  0.9968\n","ep  986  training loss:  0.004420020178121776\n","valid loss  0.002090648515149951  and accuracy  0.9992\n","ep  987  training loss:  0.004594744083318398\n","valid loss  0.002980379154905677  and accuracy  0.9988\n","ep  988  training loss:  0.005420240134021092\n","valid loss  0.016202908009290695  and accuracy  0.9976\n","ep  989  training loss:  0.0050712933868109975\n","valid loss  0.0029162142891436815  and accuracy  0.9988\n","ep  990  training loss:  0.005555896000455308\n","valid loss  0.0014988172333687544  and accuracy  0.9996\n","ep  991  training loss:  0.004498636586322446\n","valid loss  0.006553542334586382  and accuracy  0.9968\n","ep  992  training loss:  0.005282553554120677\n","valid loss  0.0026016305200755596  and accuracy  0.9996\n","ep  993  training loss:  0.005054264271254342\n","valid loss  0.0020156260579824448  and accuracy  0.9992\n","ep  994  training loss:  0.005252784546052767\n","valid loss  0.016727866604924202  and accuracy  0.9968\n","ep  995  training loss:  0.00423768794664002\n","valid loss  0.03534018248319626  and accuracy  0.992\n","ep  996  training loss:  0.003959975944128327\n","valid loss  0.0061795348301529884  and accuracy  0.9964\n","ep  997  training loss:  0.0051260060250987085\n","valid loss  0.005276520270854235  and accuracy  0.998\n","ep  998  training loss:  0.0042383109926963005\n","valid loss  0.05002039298415184  and accuracy  0.996\n","ep  999  training loss:  0.004725816334047324\n","valid loss  0.0036568744108080864  and accuracy  0.9992\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_QhURGDZmoW2"},"source":["15 min 14 sec"]},{"cell_type":"markdown","metadata":{"id":"lyj8CC2dr8P7"},"source":["### ***PREDICTION***"]},{"cell_type":"code","metadata":{"id":"1WL6v0uDgDJA"},"source":["\"\"\" Effettuiamo le predizioni sul dataset di test \"\"\"\n","\n","test_ds = CTU_Dataset(test, np.zeros(len(test)))\n","test_dl = DataLoader(test_ds, batch_size=batch_size)\n","test_dl = DeviceDataLoader(test_dl, device)\n","\n","# Utilizziamo la funzione softmax poiché siamo interessati alla probabilità per ogni classe\n","preds = []\n","model.eval()\n","with torch.no_grad():\n","    for x, y in test_dl:\n","        out = model(x)\n","        prob = F.softmax(out[0], dim=1)\n","        preds.append(prob)\n","\n","y_pred = []\n","for i in range(0, len(preds)):\n","  pred = preds[i].cpu()\n","  temp = np.argmax(pred, 1)\n","  temp = np.array(temp)\n","  y_pred = np.append(y_pred, temp)\n","\n","y_pred = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3hmjVkNmFkX","executionInfo":{"status":"ok","timestamp":1624967550511,"user_tz":-120,"elapsed":51,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"a77bc3d7-c132-441e-a486-74067a1ab05c"},"source":["y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, ..., 1, 1, 1])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"IsdRVjr9sFbO"},"source":["### ***EVALUATION***"]},{"cell_type":"code","metadata":{"id":"WOciCjPisC_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624967550512,"user_tz":-120,"elapsed":47,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"4baf4ef7-c703-4c1c-dca4-4834e72bba81"},"source":["print('Test:', Counter(y_test))\n","print('Pred:', Counter(y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: Counter({1: 15709, 0: 28})\n","Pred: Counter({1: 15686, 0: 51})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oqp05uc1-d4t"},"source":["# Matrice di confusione, accuracy, classification_report\n","from sklearn.metrics import *\n","\n","# y_test è la variabile che contiene i valori effettivi\n","# y_pred contiene i valori predetti dal modello\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","acc = accuracy_score(y_test, y_pred)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","# non presente nella libreria, calcolo mediante formula\n","f2 = (1+2**2)*((precision*recall)/((2**2*precision)+recall))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"8rOxIo2L-d4z","executionInfo":{"status":"ok","timestamp":1624967550515,"user_tz":-120,"elapsed":44,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"9f420046-e9bb-4958-8f26-185d31f46311"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","target_dict = {'BotNet' : 0,\n","               'normal' : 1}\n","\n","disp = ConfusionMatrixDisplay(cm, target_dict)\n","disp.plot()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7effbc84ee10>"]},"metadata":{"tags":[]},"execution_count":30},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELCQniZh-d40","executionInfo":{"status":"ok","timestamp":1624967550517,"user_tz":-120,"elapsed":33,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8fd9fc9b-502a-443f-e817-0b14b45b6f75"},"source":["mcm = multilabel_confusion_matrix(y_test, y_pred)\n","print(mcm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[15685    24]\n","  [    1    27]]\n","\n"," [[   27     1]\n","  [   24 15685]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7P4q-xyB7_n","executionInfo":{"status":"ok","timestamp":1624967550518,"user_tz":-120,"elapsed":29,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"d0c3a19f-bd99-4d0a-b307-ddda0de5d5a5"},"source":["FP = cm.sum (axis = 0) - np.diag (cm) \n","FN = cm.sum (axis = 1) - np.diag (cm) \n","TP = np.diag (cm) \n","TN = cm.sum () - (FP + FN + TP)\n","\n","print('True positive: ', TP)\n","print('True negative: ', TN)\n","print('False positive: ', FP)\n","print('False negative: ', FN)\n","\n","FP = FP.astype(float)\n","FN = FN.astype(float)\n","TP = TP.astype(float)\n","TN = TN.astype(float)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","# False negative rate\n","FNR = FN/(TP+FN)\n","\n","print('True positive rate: ', TPR)\n","print('True negative rate: ', TNR)\n","print('False positive rate: ', FPR)\n","print('False negative rate: ', FNR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True positive:  [   27 15685]\n","True negative:  [15685    27]\n","False positive:  [24  1]\n","False negative:  [ 1 24]\n","True positive rate:  [0.96428571 0.99847221]\n","True negative rate:  [0.99847221 0.96428571]\n","False positive rate:  [0.00152779 0.03571429]\n","False negative rate:  [0.03571429 0.00152779]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89Nap2dd-d40","executionInfo":{"status":"ok","timestamp":1624967550519,"user_tz":-120,"elapsed":25,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"a0aef85c-7952-4038-ce05-f94727546fab"},"source":["print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   27     1]\n"," [   24 15685]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wE1uRjas-d41","executionInfo":{"status":"ok","timestamp":1624967550989,"user_tz":-120,"elapsed":490,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"c054dc03-ce4d-415c-af1a-52848178394e"},"source":["print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.53      0.96      0.68        28\n","           1       1.00      1.00      1.00     15709\n","\n","    accuracy                           1.00     15737\n","   macro avg       0.76      0.98      0.84     15737\n","weighted avg       1.00      1.00      1.00     15737\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJICMuXI-d41","executionInfo":{"status":"ok","timestamp":1624967550990,"user_tz":-120,"elapsed":23,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"ad1d2a85-e4cd-41af-ab36-caa51f3412ab"},"source":["print('Accuracy: ', acc)\n","print('Precision_weighted: ', precision)\n","print('Recall_weighted: ', recall)\n","print('mcc: ', mcc)\n","print('f2: ', f2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy:  0.9984113871767173\n","Precision_weighted:  0.9990990699076129\n","Recall_weighted:  0.9984113871767173\n","mcc:  0.7138865432115648\n","f2:  0.9985488479790328\n"],"name":"stdout"}]}]}