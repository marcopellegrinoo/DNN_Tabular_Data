{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CTU_PyTorch(NoEmbed).ipynb","provenance":[],"collapsed_sections":["o66WXMYmq7UQ","ceUGliiDrULo","lyj8CC2dr8P7","IsdRVjr9sFbO"],"mount_file_id":"1IdeqAdU0WT_okUWdIBYLFJw-mc8Kj-L1","authorship_tag":"ABX9TyPQzlXzXYbYTEGc83QTxFr9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sHDsrAJqmeaY"},"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as torch_optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGYFahn6d0Ef","executionInfo":{"status":"ok","timestamp":1624964749871,"user_tz":-120,"elapsed":15,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"23e055bf-95b2-4bd1-c0d9-d1cbd208cf4f"},"source":["if torch.cuda.is_available():\n","  print(torch.cuda.device_count())            # Numero di GPU disponibili\n","  print(torch.cuda.get_device_name(0))        # Nome della prima GPU disponibile\n","  print(torch.cuda.current_device())          # Device in uso al momento\n","  print(torch.cuda.set_device(0))             # Imposta la prima GPU come default\n","  print(torch.cuda.get_device_capability(0))  # Verifica le capacità della prima GPU"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","Tesla T4\n","0\n","None\n","(7, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rg3vkXi5n2EG"},"source":["path = './drive/MyDrive/Materiale_Pellegrino_personal/CTU_Shuffled/CTU_Shuffled.csv'\n","dataset = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fMCEu_Uoe-0"},"source":["### ***PRE-ELABORAZIONE DATI***"]},{"cell_type":"code","metadata":{"id":"PHR55oIJaKA0"},"source":["dataset = dataset.drop('StartTime', axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"Hdkwvl91oGSc","executionInfo":{"status":"ok","timestamp":1624964750859,"user_tz":-120,"elapsed":43,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"4f8f75b9-8a06-48b8-fff4-1b2d399a88c3"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","      <th>multilabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000540</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>131</td>\n","      <td>71</td>\n","      <td>flow=Background-UDP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.014909</td>\n","      <td>2</td>\n","      <td>-&gt;</td>\n","      <td>SRPA_FSPA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11</td>\n","      <td>2882</td>\n","      <td>1504</td>\n","      <td>flow=Background-TCP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000798</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>244</td>\n","      <td>182</td>\n","      <td>flow=Background-UDP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15.302759</td>\n","      <td>1</td>\n","      <td>-&gt;</td>\n","      <td>INT</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>336</td>\n","      <td>336</td>\n","      <td>flow=Background-UDP-Attempt</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.843942</td>\n","      <td>2</td>\n","      <td>-&gt;</td>\n","      <td>FSPA_SRPA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>93</td>\n","      <td>11846</td>\n","      <td>4562</td>\n","      <td>flow=Background-TCP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31468</th>\n","      <td>8.917003</td>\n","      <td>2</td>\n","      <td>-&gt;</td>\n","      <td>S_SA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8</td>\n","      <td>512</td>\n","      <td>194</td>\n","      <td>flow=Background-TCP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31469</th>\n","      <td>0.038780</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>191</td>\n","      <td>68</td>\n","      <td>flow=To-Background-UDP-CVUT-DNS-Server</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31470</th>\n","      <td>603.106201</td>\n","      <td>1</td>\n","      <td>-&gt;</td>\n","      <td>INT</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>6</td>\n","      <td>552</td>\n","      <td>552</td>\n","      <td>flow=Background-Attempt-cmpgw-CVUT</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31471</th>\n","      <td>0.000550</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>128</td>\n","      <td>60</td>\n","      <td>flow=Background-UDP-Established</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>31472</th>\n","      <td>0.000186</td>\n","      <td>1</td>\n","      <td>&lt;-&gt;</td>\n","      <td>CON</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>flow=To-Background-UDP-CVUT-DNS-Server</td>\n","      <td>normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31473 rows × 11 columns</p>\n","</div>"],"text/plain":["              Dur  Proto  ...                                 Details multilabel\n","0        0.000540      1  ...         flow=Background-UDP-Established     normal\n","1        0.014909      2  ...         flow=Background-TCP-Established     normal\n","2        0.000798      1  ...         flow=Background-UDP-Established     normal\n","3       15.302759      1  ...             flow=Background-UDP-Attempt     normal\n","4        7.843942      2  ...         flow=Background-TCP-Established     normal\n","...           ...    ...  ...                                     ...        ...\n","31468    8.917003      2  ...         flow=Background-TCP-Established     normal\n","31469    0.038780      1  ...  flow=To-Background-UDP-CVUT-DNS-Server     normal\n","31470  603.106201      1  ...      flow=Background-Attempt-cmpgw-CVUT     normal\n","31471    0.000550      1  ...         flow=Background-UDP-Established     normal\n","31472    0.000186      1  ...  flow=To-Background-UDP-CVUT-DNS-Server     normal\n","\n","[31473 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mss4AVU-pWtg","executionInfo":{"status":"ok","timestamp":1624964750860,"user_tz":-120,"elapsed":38,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8ff5b449-920b-4894-b9f9-eec9e82e965d"},"source":["print(Counter(dataset['multilabel']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counter({'normal': 31406, 'BotNet': 67})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bqGImh5iNT_u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624964750861,"user_tz":-120,"elapsed":31,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"fd4d1e92-cb01-4ff2-93c3-87eab0bc1572"},"source":["dep_var = 'multilabel'\n","\n","cont_names = [col for col in dataset.columns if col != dep_var]\n","\n","print(cont_names, 'len: ', len(cont_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Dur', 'Proto', 'Dir', 'State', 'sTos', 'dTos', 'TotPkts', 'TotBytes', 'SrcBytes', 'Details'] len:  10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5pqCKXOoU-mA"},"source":["# LabelEncoding della variabile target \n","target_index = dataset.columns.get_loc(dep_var)\n","dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[dep_var])\n","\n","#LabelEncoding delle variabili categoriali\n","for col in [\"Dir\", \"State\", \"Details\"]:\n","  target_index = dataset.columns.get_loc(col)\n","  dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[col])\n","\n","# Fill NaN\n","\"\"\" Eliminiamo dalle colonne i valori nan \"\"\" \n","for col in dataset.columns:\n","  dataset[col] = dataset[col].fillna(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"fgf6kpUyNHjP","executionInfo":{"status":"ok","timestamp":1624964750864,"user_tz":-120,"elapsed":24,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8d306210-5684-4fed-8953-14c3cf6f0809"},"source":["dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","      <th>multilabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000540</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>131</td>\n","      <td>71</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.014909</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>89</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11</td>\n","      <td>2882</td>\n","      <td>1504</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000798</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>244</td>\n","      <td>182</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15.302759</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>336</td>\n","      <td>336</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.843942</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>43</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>93</td>\n","      <td>11846</td>\n","      <td>4562</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Dur  Proto  Dir  State  ...  TotBytes  SrcBytes  Details  multilabel\n","0   0.000540      1    3      4  ...       131        71        6           1\n","1   0.014909      2    0     89  ...      2882      1504        4           1\n","2   0.000798      1    3      4  ...       244       182        6           1\n","3  15.302759      1    0     53  ...       336       336        5           1\n","4   7.843942      2    0     43  ...     11846      4562        4           1\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"GL5xD37fnaia"},"source":["from sklearn.model_selection import train_test_split\n","\n","# train 50% e test 50%\n","train, test = train_test_split(dataset, test_size=0.50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ0TyYvfQLgW"},"source":["y_train = train[dep_var]\n","train = train.drop(dep_var, axis=1)\n","y_test = test[dep_var]\n","test = test.drop(dep_var, axis=1)\n","\n","# validation di 2500 righe da train\n","train, validation, y_train, y_val = train_test_split(train, y_train, test_size=(2500/len(train)), random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNTwI2TaaXlY"},"source":["\"\"\"Visto che nel dataset la variabile target è molto squilibrata lo amplio con una generazione\n"," randomica di dati mediante la tecnica chiamata Synthetic Minority Over-sampling Technique (SMOTE)\"\"\"\n","\n","from imblearn.over_sampling import SMOTE\n","sm = SMOTE(random_state=0)\n","x_sm, y_train = sm.fit_resample(train, y_train)\n","train = pd.DataFrame(x_sm,columns=train.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5xEsk56PSsW"},"source":["#y_train = y_train.values\n","y_test = y_test.values\n","y_val = y_val.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"IKp_gSloQTZO","executionInfo":{"status":"ok","timestamp":1624964751289,"user_tz":-120,"elapsed":45,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"ac341d18-0ec6-4cbe-d449-6f9adb7bd924"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>11058</th>\n","      <td>0.065865</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>109</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>16</td>\n","      <td>1270</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>18577</th>\n","      <td>0.000353</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>224</td>\n","      <td>87</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>11688</th>\n","      <td>0.000369</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>264</td>\n","      <td>83</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>20637</th>\n","      <td>0.118549</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>133</td>\n","      <td>73</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>30734</th>\n","      <td>0.000614</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>237</td>\n","      <td>175</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27164</th>\n","      <td>0.001956</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>522</td>\n","      <td>462</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3371</th>\n","      <td>0.000371</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>422</td>\n","      <td>72</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>31171</th>\n","      <td>0.020569</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>209</td>\n","      <td>72</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>2419</th>\n","      <td>0.000214</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>25384</th>\n","      <td>0.000305</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>208</td>\n","      <td>79</td>\n","      <td>46</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15737 rows × 10 columns</p>\n","</div>"],"text/plain":["            Dur  Proto  Dir  State  ...  TotPkts  TotBytes  SrcBytes  Details\n","11058  0.065865      2    0    109  ...       16      1270         0        4\n","18577  0.000353      1    3      4  ...        2       224        87       46\n","11688  0.000369      1    3      4  ...        2       264        83       46\n","20637  0.118549      1    3      4  ...        2       133        73        6\n","30734  0.000614      1    3      4  ...        2       237       175        6\n","...         ...    ...  ...    ...  ...      ...       ...       ...      ...\n","27164  0.001956      1    3      4  ...        2       522       462        6\n","3371   0.000371      1    3      4  ...        2       422        72       46\n","31171  0.020569      1    3      4  ...        2       209        72       46\n","2419   0.000214      1    3      4  ...        2       214        81       46\n","25384  0.000305      1    3      4  ...        2       208        79       46\n","\n","[15737 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"79A_sfwnQVqX","executionInfo":{"status":"ok","timestamp":1624964751290,"user_tz":-120,"elapsed":44,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"7a94888b-1d7b-4166-e1d0-3c8efc652d0c"},"source":["train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.190620</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>577.000000</td>\n","      <td>78.000000</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>756.997742</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.000000</td>\n","      <td>438.000000</td>\n","      <td>213.000000</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000866</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>320.000000</td>\n","      <td>64.000000</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000844</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>135.000000</td>\n","      <td>75.000000</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000359</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>368.000000</td>\n","      <td>90.000000</td>\n","      <td>37.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>26411</th>\n","      <td>79.045345</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>92.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.798394</td>\n","      <td>47535.545542</td>\n","      <td>2956.798180</td>\n","      <td>27.0</td>\n","    </tr>\n","    <tr>\n","      <th>26412</th>\n","      <td>0.663930</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>286.102429</td>\n","      <td>73.170553</td>\n","      <td>35.0</td>\n","    </tr>\n","    <tr>\n","      <th>26413</th>\n","      <td>1.442241</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>37.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.000000</td>\n","      <td>1329.000000</td>\n","      <td>842.845422</td>\n","      <td>28.0</td>\n","    </tr>\n","    <tr>\n","      <th>26414</th>\n","      <td>0.006586</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>274.388544</td>\n","      <td>74.018529</td>\n","      <td>35.0</td>\n","    </tr>\n","    <tr>\n","      <th>26415</th>\n","      <td>1.299126</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>37.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.000000</td>\n","      <td>1140.531483</td>\n","      <td>698.531483</td>\n","      <td>28.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>26416 rows × 10 columns</p>\n","</div>"],"text/plain":["              Dur  Proto  Dir  ...      TotBytes     SrcBytes  Details\n","0        0.190620    1.0  3.0  ...    577.000000    78.000000      2.0\n","1      756.997742    1.0  3.0  ...    438.000000   213.000000      6.0\n","2        0.000866    1.0  3.0  ...    320.000000    64.000000      6.0\n","3        0.000844    1.0  3.0  ...    135.000000    75.000000      6.0\n","4        0.000359    1.0  3.0  ...    368.000000    90.000000     37.0\n","...           ...    ...  ...  ...           ...          ...      ...\n","26411   79.045345    2.0  0.0  ...  47535.545542  2956.798180     27.0\n","26412    0.663930    1.0  3.0  ...    286.102429    73.170553     35.0\n","26413    1.442241    2.0  0.0  ...   1329.000000   842.845422     28.0\n","26414    0.006586    1.0  3.0  ...    274.388544    74.018529     35.0\n","26415    1.299126    2.0  0.0  ...   1140.531483   698.531483     28.0\n","\n","[26416 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"va0f2YIwqfM4","executionInfo":{"status":"ok","timestamp":1624964751291,"user_tz":-120,"elapsed":41,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"9076edcc-acd8-44d0-f5be-7f35c57368a3"},"source":["validation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dur</th>\n","      <th>Proto</th>\n","      <th>Dir</th>\n","      <th>State</th>\n","      <th>sTos</th>\n","      <th>dTos</th>\n","      <th>TotPkts</th>\n","      <th>TotBytes</th>\n","      <th>SrcBytes</th>\n","      <th>Details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>22555</th>\n","      <td>0.000588</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>132</td>\n","      <td>72</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>9717</th>\n","      <td>0.000642</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>237</td>\n","      <td>163</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>5155</th>\n","      <td>6.189429</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>300</td>\n","      <td>300</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>813</th>\n","      <td>0.000391</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>208</td>\n","      <td>79</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>19029</th>\n","      <td>0.000429</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>424</td>\n","      <td>75</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>26303</th>\n","      <td>0.000286</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>261</td>\n","      <td>84</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>14925</th>\n","      <td>0.000246</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>222</td>\n","      <td>75</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>3675</th>\n","      <td>1137.244873</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6</td>\n","      <td>660</td>\n","      <td>242</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>9314</th>\n","      <td>0.000280</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>214</td>\n","      <td>81</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>6091</th>\n","      <td>0.000000</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>103</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>70</td>\n","      <td>70</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2500 rows × 10 columns</p>\n","</div>"],"text/plain":["               Dur  Proto  Dir  State  ...  TotPkts  TotBytes  SrcBytes  Details\n","22555     0.000588      1    3      4  ...        2       132        72        6\n","9717      0.000642      1    3      4  ...        2       237       163        6\n","5155      6.189429      1    0     53  ...        3       300       300        1\n","813       0.000391      1    3      4  ...        2       208        79       46\n","19029     0.000429      1    3      4  ...        2       424        75       46\n","...            ...    ...  ...    ...  ...      ...       ...       ...      ...\n","26303     0.000286      1    3      4  ...        2       261        84       46\n","14925     0.000246      1    3      4  ...        2       222        75       46\n","3675   1137.244873      1    3      4  ...        6       660       242        6\n","9314      0.000280      1    3      4  ...        2       214        81       46\n","6091      0.000000      3    0    103  ...        1        70        70        0\n","\n","[2500 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"o66WXMYmq7UQ"},"source":["### ***MODEL***"]},{"cell_type":"code","metadata":{"id":"1Oa_SzfA6bq7"},"source":["\"\"\" Pytorch Dataset e DataLoader\n","Estendiamo la Datasetclasse (astratta) fornita da Pytorch per un accesso più facile al nostro set di dati durante l'addestramento \n","e per utilizzare efficacemente  il DataLoader modulo per gestire i batch. Ciò comporta la sovrascrittura dei metodi __len__e __getitem__\n","secondo il nostro particolare set di dati.\n","Poiché abbiamo solo bisogno di incorporare colonne categoriali, dividiamo il nostro input in due parti: numerico e categoriale. \"\"\" \n","\n","class CTU_Dataset(Dataset):\n","    def __init__(self, X, Y):\n","        X = X.copy()\n","        self.X = X.copy().values.astype(np.float32) #numerical columns\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","        \n","#creating train and valid datasets\n","train_ds = CTU_Dataset(train, y_train)\n","valid_ds = CTU_Dataset(validation, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oGGkh8o6icS","executionInfo":{"status":"ok","timestamp":1624964751294,"user_tz":-120,"elapsed":38,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"bb6edaa4-d84e-4f62-85b8-733070c0c19b"},"source":["\"\"\" Making device (GPU/CPU) compatible\n","\n","(borrowed from https://jovian.ml/aakashns/04-feedforward-nn)\n","\n","In order to make use of a GPU if available, we'll have to move our data and model to it. \"\"\" \n","\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"wZr4ZQflaCAw"},"source":["class CTUModel(nn.Module):\n","    def __init__(self, n_cont):\n","        super().__init__()\n","        self.n_cont =  n_cont\n","        self.lin1 = nn.Linear(self.n_cont, 200)\n","        self.lin2 = nn.Linear(200, 100)\n","        self.lin3 = nn.Linear(100, 2)\n","        self.bn1 = nn.BatchNorm1d(self.n_cont, momentum=1.0)\n","        self.bn2 = nn.BatchNorm1d(200, momentum=1.0)\n","        self.bn3 = nn.BatchNorm1d(100, momentum=1.0)\n","        self.drops = nn.Dropout(0.00001)\n","        \n","\n","    def forward(self, x_cont):\n","        x = self.bn1(x_cont)\n","        x = F.relu(self.lin1(x))\n","        x = self.drops(x)\n","        x = self.bn2(x)\n","        x = F.relu(self.lin2(x))\n","        x = self.drops(x)\n","        x = self.bn3(x)\n","        x = self.lin3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQc6Y5qBapPL"},"source":["\"\"\" Fase di preparazione per l'addestramento \"\"\"\n","\n","# Optimizer\n","def get_optimizer(model, lr = 0.001, wd = 0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim\n","\n","# Training function\n","def train_model(model, optim, train_dl):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x, y in train_dl:\n","        batch = y.shape[0]\n","        output = model(x)\n","        loss = F.cross_entropy(output, y)   \n","        optim.zero_grad()\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","# Evaluation function\n","def val_loss(model, valid_dl):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x, y in valid_dl:\n","        current_batch_size = y.shape[0]\n","        out = model(x)\n","        loss = F.cross_entropy(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.max(out, 1)[1]\n","        correct += (pred == y).float().sum().item()\n","    print('valid loss ', sum_loss/total, ' and accuracy ', correct/total)\n","    return sum_loss/total, correct/total\n","\n","# Funzione per l'addestramento \n","def train_loop(model, epochs, lr=0.01, wd=0.0):\n","    optim = get_optimizer(model, lr = lr, wd = wd)\n","    for i in range(epochs): \n","        loss = train_model(model, optim, train_dl)\n","        print('ep ', i, \" training loss: \", loss)\n","        val_loss(model, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceUGliiDrULo"},"source":["### ***TRAINING***"]},{"cell_type":"code","metadata":{"id":"qNNiIdczasqp"},"source":["\"\"\" Ora addestriamo il modello sul set di addestramento. Ho usato l'ottimizzatore Adam per ottimizzare la perdita di entropia incrociata. \n","L'addestramento è piuttosto semplice: iterare attraverso ogni batch, eseguire un passaggio in avanti, calcolare i gradienti, \n","eseguire una discesa del gradiente e ripetere questo processo per tutte le epoche necessarie. \"\"\" \n","\n","batch_size = 4096\n","train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIE4j_1iOOP4","executionInfo":{"status":"ok","timestamp":1624964763354,"user_tz":-120,"elapsed":11832,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"e8f23229-3ed3-4af8-a4e9-a8044bccccbb"},"source":["model = CTUModel(len(cont_names))\n","to_device(model, device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CTUModel(\n","  (lin1): Linear(in_features=10, out_features=200, bias=True)\n","  (lin2): Linear(in_features=200, out_features=100, bias=True)\n","  (lin3): Linear(in_features=100, out_features=2, bias=True)\n","  (bn1): BatchNorm1d(10, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","  (bn2): BatchNorm1d(200, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","  (bn3): BatchNorm1d(100, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n","  (drops): Dropout(p=1e-05, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b976OcWbQLBe","executionInfo":{"status":"ok","timestamp":1624964845434,"user_tz":-120,"elapsed":82101,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"7c2e2377-6b38-4fb3-a278-d6125ea6f257"},"source":["train_loop(model, epochs=500, lr=0.00008)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ep  0  training loss:  0.8499145677853612\n","valid loss  1.2395600080490112  and accuracy  0.4668\n","ep  1  training loss:  0.6552046706718362\n","valid loss  1.3511760234832764  and accuracy  0.8312\n","ep  2  training loss:  0.5256479240994537\n","valid loss  0.5532494187355042  and accuracy  0.876\n","ep  3  training loss:  0.4489174076169278\n","valid loss  0.4461265504360199  and accuracy  0.8932\n","ep  4  training loss:  0.3949977989307972\n","valid loss  0.3547188341617584  and accuracy  0.9672\n","ep  5  training loss:  0.3538499389143874\n","valid loss  0.2990839183330536  and accuracy  0.9732\n","ep  6  training loss:  0.32168781670709584\n","valid loss  0.27296939492225647  and accuracy  0.9704\n","ep  7  training loss:  0.29530577755999665\n","valid loss  0.2625962197780609  and accuracy  0.9744\n","ep  8  training loss:  0.266379399273628\n","valid loss  0.26685795187950134  and accuracy  0.9672\n","ep  9  training loss:  0.24917330736828025\n","valid loss  0.6821154952049255  and accuracy  0.9612\n","ep  10  training loss:  0.2354766972890989\n","valid loss  0.21717087924480438  and accuracy  0.9736\n","ep  11  training loss:  0.21586466319340059\n","valid loss  0.20672507584095  and accuracy  0.9688\n","ep  12  training loss:  0.20010681770773528\n","valid loss  0.2044370472431183  and accuracy  0.9716\n","ep  13  training loss:  0.18552624720793504\n","valid loss  0.18830756843090057  and accuracy  0.9712\n","ep  14  training loss:  0.17280374572971674\n","valid loss  0.20075350999832153  and accuracy  0.9712\n","ep  15  training loss:  0.15720316857947933\n","valid loss  0.16867919266223907  and accuracy  0.974\n","ep  16  training loss:  0.14887326374550722\n","valid loss  0.1460743397474289  and accuracy  0.974\n","ep  17  training loss:  0.1354153181732531\n","valid loss  0.13661086559295654  and accuracy  0.9712\n","ep  18  training loss:  0.1254907954576736\n","valid loss  0.13739439845085144  and accuracy  0.9704\n","ep  19  training loss:  0.11763038806197572\n","valid loss  0.1183025985956192  and accuracy  0.9756\n","ep  20  training loss:  0.10886589432307837\n","valid loss  0.11380872875452042  and accuracy  0.9756\n","ep  21  training loss:  0.10258913698574039\n","valid loss  0.13767345249652863  and accuracy  0.9708\n","ep  22  training loss:  0.09648486959219413\n","valid loss  0.10030648112297058  and accuracy  0.9752\n","ep  23  training loss:  0.09022727907877269\n","valid loss  0.08241657167673111  and accuracy  0.9904\n","ep  24  training loss:  0.08380166940314347\n","valid loss  0.08961528539657593  and accuracy  0.9904\n","ep  25  training loss:  0.0800993401063059\n","valid loss  0.10774626582860947  and accuracy  0.9856\n","ep  26  training loss:  0.07557630289835615\n","valid loss  0.07480466365814209  and accuracy  0.9896\n","ep  27  training loss:  0.0706126349467642\n","valid loss  0.08122007548809052  and accuracy  0.99\n","ep  28  training loss:  0.06990576793142407\n","valid loss  0.06061959266662598  and accuracy  0.9912\n","ep  29  training loss:  0.06558051831717927\n","valid loss  0.07855100929737091  and accuracy  0.99\n","ep  30  training loss:  0.06278647500897523\n","valid loss  0.07423193007707596  and accuracy  0.9892\n","ep  31  training loss:  0.06016771021501575\n","valid loss  0.07071617990732193  and accuracy  0.9904\n","ep  32  training loss:  0.0568913008830426\n","valid loss  0.05996325612068176  and accuracy  0.99\n","ep  33  training loss:  0.05497647268412187\n","valid loss  0.04624432325363159  and accuracy  0.9912\n","ep  34  training loss:  0.052310873486376905\n","valid loss  0.05788138508796692  and accuracy  0.99\n","ep  35  training loss:  0.05214940498654659\n","valid loss  0.0531664602458477  and accuracy  0.9912\n","ep  36  training loss:  0.04974359741433327\n","valid loss  0.05269099771976471  and accuracy  0.9904\n","ep  37  training loss:  0.048676279105521345\n","valid loss  0.04824208468198776  and accuracy  0.9912\n","ep  38  training loss:  0.04780707177765293\n","valid loss  0.04776685684919357  and accuracy  0.9908\n","ep  39  training loss:  0.045686464535601566\n","valid loss  0.04485982283949852  and accuracy  0.99\n","ep  40  training loss:  0.04453671269845341\n","valid loss  0.060447413474321365  and accuracy  0.9892\n","ep  41  training loss:  0.0430987584232598\n","valid loss  0.05162790045142174  and accuracy  0.9888\n","ep  42  training loss:  0.04152324445532135\n","valid loss  0.03939318284392357  and accuracy  0.992\n","ep  43  training loss:  0.03976975864917491\n","valid loss  0.04231951758265495  and accuracy  0.9908\n","ep  44  training loss:  0.03993821075509924\n","valid loss  0.03549490496516228  and accuracy  0.992\n","ep  45  training loss:  0.038835801831759664\n","valid loss  0.09596498310565948  and accuracy  0.9884\n","ep  46  training loss:  0.03739115727816041\n","valid loss  0.033227261155843735  and accuracy  0.9928\n","ep  47  training loss:  0.035932353521155126\n","valid loss  0.034350715577602386  and accuracy  0.9912\n","ep  48  training loss:  0.03586490441910214\n","valid loss  0.038702305406332016  and accuracy  0.9908\n","ep  49  training loss:  0.03443326743749472\n","valid loss  0.03445851430296898  and accuracy  0.992\n","ep  50  training loss:  0.0339456106892035\n","valid loss  0.03003927692770958  and accuracy  0.9924\n","ep  51  training loss:  0.033571307271547494\n","valid loss  0.040286991745233536  and accuracy  0.9916\n","ep  52  training loss:  0.03254769463599775\n","valid loss  0.03373042121529579  and accuracy  0.9904\n","ep  53  training loss:  0.03231829117141948\n","valid loss  0.03623040020465851  and accuracy  0.9908\n","ep  54  training loss:  0.03084296408221622\n","valid loss  0.038324348628520966  and accuracy  0.9904\n","ep  55  training loss:  0.03060348378635406\n","valid loss  0.03597234934568405  and accuracy  0.9896\n","ep  56  training loss:  0.029852488779348364\n","valid loss  0.05623321980237961  and accuracy  0.99\n","ep  57  training loss:  0.030566411220363094\n","valid loss  0.030705902725458145  and accuracy  0.992\n","ep  58  training loss:  0.02970157832421728\n","valid loss  0.033642109483480453  and accuracy  0.992\n","ep  59  training loss:  0.028689429281264778\n","valid loss  0.05574287846684456  and accuracy  0.99\n","ep  60  training loss:  0.028201274746416337\n","valid loss  0.03147819638252258  and accuracy  0.9908\n","ep  61  training loss:  0.02777686230229075\n","valid loss  0.03570892661809921  and accuracy  0.9908\n","ep  62  training loss:  0.02783888170347836\n","valid loss  0.0284702368080616  and accuracy  0.9924\n","ep  63  training loss:  0.027243734219493394\n","valid loss  0.031681835651397705  and accuracy  0.9924\n","ep  64  training loss:  0.026316975389490484\n","valid loss  0.026842858642339706  and accuracy  0.9924\n","ep  65  training loss:  0.026057792472106482\n","valid loss  0.03686821460723877  and accuracy  0.99\n","ep  66  training loss:  0.02568242916987384\n","valid loss  0.026921065524220467  and accuracy  0.9924\n","ep  67  training loss:  0.026032062642262748\n","valid loss  0.03368302807211876  and accuracy  0.9908\n","ep  68  training loss:  0.024700236273788454\n","valid loss  0.02783581241965294  and accuracy  0.9924\n","ep  69  training loss:  0.02476251488162271\n","valid loss  0.027225732803344727  and accuracy  0.9924\n","ep  70  training loss:  0.02447483949430923\n","valid loss  0.040925510227680206  and accuracy  0.9912\n","ep  71  training loss:  0.024492231127143195\n","valid loss  0.023147383704781532  and accuracy  0.9928\n","ep  72  training loss:  0.024059716946045463\n","valid loss  0.04815980792045593  and accuracy  0.9916\n","ep  73  training loss:  0.023536708595164612\n","valid loss  0.02820771560072899  and accuracy  0.9924\n","ep  74  training loss:  0.023042989403984172\n","valid loss  0.028157446533441544  and accuracy  0.9924\n","ep  75  training loss:  0.022963114620960894\n","valid loss  0.022068118676543236  and accuracy  0.9924\n","ep  76  training loss:  0.02306553020143711\n","valid loss  0.03792770206928253  and accuracy  0.9916\n","ep  77  training loss:  0.02305182221507499\n","valid loss  0.025572961196303368  and accuracy  0.9924\n","ep  78  training loss:  0.02258268573960523\n","valid loss  0.027929699048399925  and accuracy  0.9924\n","ep  79  training loss:  0.021821741083538727\n","valid loss  0.02504929155111313  and accuracy  0.9924\n","ep  80  training loss:  0.02170884864306645\n","valid loss  0.029619550332427025  and accuracy  0.9908\n","ep  81  training loss:  0.021877758171091218\n","valid loss  0.026681043207645416  and accuracy  0.9924\n","ep  82  training loss:  0.02066661675727454\n","valid loss  0.026656560599803925  and accuracy  0.9924\n","ep  83  training loss:  0.020827123616624067\n","valid loss  0.026284584775567055  and accuracy  0.9924\n","ep  84  training loss:  0.02069557541028017\n","valid loss  0.02431081049144268  and accuracy  0.9924\n","ep  85  training loss:  0.020117464414975413\n","valid loss  0.027617212384939194  and accuracy  0.9916\n","ep  86  training loss:  0.019762838390773713\n","valid loss  0.024265091866254807  and accuracy  0.9924\n","ep  87  training loss:  0.01956427375984004\n","valid loss  0.02844948135316372  and accuracy  0.9924\n","ep  88  training loss:  0.01939350503329289\n","valid loss  0.022739684209227562  and accuracy  0.9924\n","ep  89  training loss:  0.018851951457366012\n","valid loss  0.026070090010762215  and accuracy  0.9924\n","ep  90  training loss:  0.018710110855510494\n","valid loss  0.024861745536327362  and accuracy  0.9924\n","ep  91  training loss:  0.018230622880237527\n","valid loss  0.024433018639683723  and accuracy  0.992\n","ep  92  training loss:  0.018656692759267565\n","valid loss  0.01920170523226261  and accuracy  0.9924\n","ep  93  training loss:  0.01819240990157455\n","valid loss  0.019679013639688492  and accuracy  0.9924\n","ep  94  training loss:  0.018027328066460152\n","valid loss  0.02008834294974804  and accuracy  0.9924\n","ep  95  training loss:  0.017303611356640932\n","valid loss  0.02035553939640522  and accuracy  0.9924\n","ep  96  training loss:  0.017658325071265524\n","valid loss  0.02197551354765892  and accuracy  0.9924\n","ep  97  training loss:  0.016857363152199805\n","valid loss  0.018999427556991577  and accuracy  0.9924\n","ep  98  training loss:  0.01720444204850965\n","valid loss  0.02016761712729931  and accuracy  0.9924\n","ep  99  training loss:  0.016494649178661554\n","valid loss  0.019952919334173203  and accuracy  0.9928\n","ep  100  training loss:  0.0160269307413746\n","valid loss  0.01876194216310978  and accuracy  0.9924\n","ep  101  training loss:  0.016237880882870818\n","valid loss  0.020109476521611214  and accuracy  0.9924\n","ep  102  training loss:  0.016297372942536546\n","valid loss  0.0605018250644207  and accuracy  0.9916\n","ep  103  training loss:  0.01640066667953489\n","valid loss  0.020735416561365128  and accuracy  0.9924\n","ep  104  training loss:  0.01557937665063457\n","valid loss  0.018613409250974655  and accuracy  0.9924\n","ep  105  training loss:  0.015890785800388985\n","valid loss  0.019717954099178314  and accuracy  0.9924\n","ep  106  training loss:  0.015538275437029039\n","valid loss  0.018748844042420387  and accuracy  0.9924\n","ep  107  training loss:  0.015546011189613683\n","valid loss  0.01784018613398075  and accuracy  0.9924\n","ep  108  training loss:  0.015608085571550767\n","valid loss  0.053369440138339996  and accuracy  0.9916\n","ep  109  training loss:  0.015014562667824152\n","valid loss  0.014254806563258171  and accuracy  0.9924\n","ep  110  training loss:  0.014512167912403033\n","valid loss  0.013617727905511856  and accuracy  0.9976\n","ep  111  training loss:  0.014918003330567907\n","valid loss  0.015653664246201515  and accuracy  0.9924\n","ep  112  training loss:  0.014418982682938036\n","valid loss  0.014756768941879272  and accuracy  0.9924\n","ep  113  training loss:  0.013904575846845601\n","valid loss  0.01597742550075054  and accuracy  0.9924\n","ep  114  training loss:  0.014563482562740019\n","valid loss  0.014846459962427616  and accuracy  0.9924\n","ep  115  training loss:  0.01349381901163592\n","valid loss  0.016031211242079735  and accuracy  0.9928\n","ep  116  training loss:  0.01375154948831146\n","valid loss  0.0733121857047081  and accuracy  0.992\n","ep  117  training loss:  0.013577592928641713\n","valid loss  0.01550352293998003  and accuracy  0.9928\n","ep  118  training loss:  0.013390617164191442\n","valid loss  0.018779238685965538  and accuracy  0.9924\n","ep  119  training loss:  0.013410625575411544\n","valid loss  0.012417863123118877  and accuracy  0.9928\n","ep  120  training loss:  0.013146454238218296\n","valid loss  0.013381972908973694  and accuracy  0.998\n","ep  121  training loss:  0.012869074817708518\n","valid loss  0.013971976935863495  and accuracy  0.9976\n","ep  122  training loss:  0.012723130881357634\n","valid loss  0.015917208045721054  and accuracy  0.9924\n","ep  123  training loss:  0.012796685330881125\n","valid loss  0.0155784972012043  and accuracy  0.9924\n","ep  124  training loss:  0.012527420661039313\n","valid loss  0.014882965013384819  and accuracy  0.9976\n","ep  125  training loss:  0.012464263216383092\n","valid loss  0.01788482442498207  and accuracy  0.9928\n","ep  126  training loss:  0.011973878282992715\n","valid loss  0.014310607686638832  and accuracy  0.9924\n","ep  127  training loss:  0.012280824657378415\n","valid loss  0.01599946618080139  and accuracy  0.9932\n","ep  128  training loss:  0.011993738753819271\n","valid loss  0.012877036817371845  and accuracy  0.9928\n","ep  129  training loss:  0.011690109154758923\n","valid loss  0.014236340299248695  and accuracy  0.9928\n","ep  130  training loss:  0.01196241689149825\n","valid loss  0.015862688422203064  and accuracy  0.9928\n","ep  131  training loss:  0.011784052181828751\n","valid loss  0.012779162265360355  and accuracy  0.9924\n","ep  132  training loss:  0.011070525035632155\n","valid loss  0.01196342147886753  and accuracy  0.9928\n","ep  133  training loss:  0.011114243936382554\n","valid loss  0.020553581416606903  and accuracy  0.9924\n","ep  134  training loss:  0.011340228744812405\n","valid loss  0.08036880940198898  and accuracy  0.992\n","ep  135  training loss:  0.010887361858728292\n","valid loss  0.10794239491224289  and accuracy  0.992\n","ep  136  training loss:  0.010522822977502299\n","valid loss  0.01717197336256504  and accuracy  0.9928\n","ep  137  training loss:  0.01084502946544802\n","valid loss  0.023413212969899178  and accuracy  0.9972\n","ep  138  training loss:  0.010380273573168258\n","valid loss  0.012704689987003803  and accuracy  0.998\n","ep  139  training loss:  0.011271455576208993\n","valid loss  0.01796049252152443  and accuracy  0.9928\n","ep  140  training loss:  0.010077889233055908\n","valid loss  0.012134897522628307  and accuracy  0.998\n","ep  141  training loss:  0.010744266387984508\n","valid loss  0.026084311306476593  and accuracy  0.9924\n","ep  142  training loss:  0.010183124320140432\n","valid loss  0.011944262310862541  and accuracy  0.998\n","ep  143  training loss:  0.009991673226928004\n","valid loss  0.011070266366004944  and accuracy  0.998\n","ep  144  training loss:  0.009630154704913126\n","valid loss  0.015255698002874851  and accuracy  0.9928\n","ep  145  training loss:  0.009238085408284944\n","valid loss  0.011654546484351158  and accuracy  0.998\n","ep  146  training loss:  0.009499256341164759\n","valid loss  0.012506472878158092  and accuracy  0.9928\n","ep  147  training loss:  0.009631460208190817\n","valid loss  0.019648168236017227  and accuracy  0.9976\n","ep  148  training loss:  0.009600636109863173\n","valid loss  0.0869087427854538  and accuracy  0.9972\n","ep  149  training loss:  0.009142263333563062\n","valid loss  0.05689471960067749  and accuracy  0.9972\n","ep  150  training loss:  0.009133428872209257\n","valid loss  0.054007917642593384  and accuracy  0.992\n","ep  151  training loss:  0.008997786318548227\n","valid loss  0.022677581757307053  and accuracy  0.998\n","ep  152  training loss:  0.008957171957062129\n","valid loss  0.03404783457517624  and accuracy  0.9976\n","ep  153  training loss:  0.009732536902749344\n","valid loss  0.02304687723517418  and accuracy  0.9928\n","ep  154  training loss:  0.009210928040598917\n","valid loss  0.012881986796855927  and accuracy  0.9928\n","ep  155  training loss:  0.009198519779196621\n","valid loss  0.12685847282409668  and accuracy  0.992\n","ep  156  training loss:  0.008631009640096535\n","valid loss  0.041036102920770645  and accuracy  0.9976\n","ep  157  training loss:  0.0081201949281486\n","valid loss  0.011194304563105106  and accuracy  0.9928\n","ep  158  training loss:  0.008802815311126352\n","valid loss  0.010250301100313663  and accuracy  0.9932\n","ep  159  training loss:  0.009015102953684467\n","valid loss  0.012303108349442482  and accuracy  0.9928\n","ep  160  training loss:  0.008625646419078562\n","valid loss  0.046945638954639435  and accuracy  0.9976\n","ep  161  training loss:  0.008189830160177996\n","valid loss  0.00889524258673191  and accuracy  0.9984\n","ep  162  training loss:  0.00845063043005769\n","valid loss  0.10104523599147797  and accuracy  0.9976\n","ep  163  training loss:  0.008092357570035829\n","valid loss  0.015976695343852043  and accuracy  0.9972\n","ep  164  training loss:  0.007911795605287668\n","valid loss  0.025011084973812103  and accuracy  0.9976\n","ep  165  training loss:  0.007958952065609138\n","valid loss  0.009308448061347008  and accuracy  0.998\n","ep  166  training loss:  0.007991916742593249\n","valid loss  0.03087669610977173  and accuracy  0.9976\n","ep  167  training loss:  0.008418539673798485\n","valid loss  0.14721958339214325  and accuracy  0.9972\n","ep  168  training loss:  0.00778116017178396\n","valid loss  0.019526949152350426  and accuracy  0.998\n","ep  169  training loss:  0.00793223932886362\n","valid loss  0.010301106609404087  and accuracy  0.998\n","ep  170  training loss:  0.007370361261214183\n","valid loss  0.03114004246890545  and accuracy  0.9972\n","ep  171  training loss:  0.007696606093585527\n","valid loss  0.01502616424113512  and accuracy  0.9972\n","ep  172  training loss:  0.007822314577390395\n","valid loss  0.03936240077018738  and accuracy  0.9976\n","ep  173  training loss:  0.0071268754778288845\n","valid loss  0.010561894625425339  and accuracy  0.998\n","ep  174  training loss:  0.008143399925569669\n","valid loss  0.01952417381107807  and accuracy  0.9928\n","ep  175  training loss:  0.007073414659207695\n","valid loss  0.011233372613787651  and accuracy  0.998\n","ep  176  training loss:  0.006947741763094279\n","valid loss  0.013633624650537968  and accuracy  0.9932\n","ep  177  training loss:  0.0074133184664560625\n","valid loss  0.04465237259864807  and accuracy  0.9976\n","ep  178  training loss:  0.007284208446817821\n","valid loss  0.008962510153651237  and accuracy  0.998\n","ep  179  training loss:  0.00689655638608443\n","valid loss  0.008692207746207714  and accuracy  0.998\n","ep  180  training loss:  0.006904453668982549\n","valid loss  0.008514435030519962  and accuracy  0.9984\n","ep  181  training loss:  0.0070749170855951265\n","valid loss  0.009281480684876442  and accuracy  0.9932\n","ep  182  training loss:  0.006765607030309742\n","valid loss  0.2026112973690033  and accuracy  0.9972\n","ep  183  training loss:  0.007354545411227924\n","valid loss  0.05497272312641144  and accuracy  0.9968\n","ep  184  training loss:  0.006704036180248122\n","valid loss  0.006397980730980635  and accuracy  0.9984\n","ep  185  training loss:  0.006551569231251635\n","valid loss  0.007975836284458637  and accuracy  0.998\n","ep  186  training loss:  0.006620813676154412\n","valid loss  0.02316507324576378  and accuracy  0.9932\n","ep  187  training loss:  0.006343087947209321\n","valid loss  0.007239689584821463  and accuracy  0.998\n","ep  188  training loss:  0.006258553296196721\n","valid loss  0.06620011478662491  and accuracy  0.9928\n","ep  189  training loss:  0.007350036253907838\n","valid loss  0.01084370817989111  and accuracy  0.998\n","ep  190  training loss:  0.006306997867022443\n","valid loss  0.05361165851354599  and accuracy  0.9972\n","ep  191  training loss:  0.006307614028571159\n","valid loss  0.20846113562583923  and accuracy  0.996\n","ep  192  training loss:  0.006355381920495985\n","valid loss  0.03626082465052605  and accuracy  0.9976\n","ep  193  training loss:  0.006436781101447752\n","valid loss  0.12949615716934204  and accuracy  0.9972\n","ep  194  training loss:  0.0060980869806865215\n","valid loss  0.15639972686767578  and accuracy  0.9976\n","ep  195  training loss:  0.005952105214644442\n","valid loss  0.007643731776624918  and accuracy  0.9988\n","ep  196  training loss:  0.006316810461051135\n","valid loss  0.007204567547887564  and accuracy  0.998\n","ep  197  training loss:  0.006203338142534067\n","valid loss  0.016395073384046555  and accuracy  0.9976\n","ep  198  training loss:  0.005769489467006373\n","valid loss  0.03868522122502327  and accuracy  0.9976\n","ep  199  training loss:  0.005730892618021492\n","valid loss  0.006694337353110313  and accuracy  0.998\n","ep  200  training loss:  0.006635454325947453\n","valid loss  0.007765857502818108  and accuracy  0.998\n","ep  201  training loss:  0.005842559429383112\n","valid loss  0.008965852670371532  and accuracy  0.9936\n","ep  202  training loss:  0.005907142556287678\n","valid loss  0.04056253284215927  and accuracy  0.9976\n","ep  203  training loss:  0.005549429476978669\n","valid loss  0.1821550577878952  and accuracy  0.9972\n","ep  204  training loss:  0.005583160097396587\n","valid loss  0.006547458469867706  and accuracy  0.998\n","ep  205  training loss:  0.00555835362029646\n","valid loss  0.006911446340382099  and accuracy  0.998\n","ep  206  training loss:  0.005380049402594819\n","valid loss  0.008021656423807144  and accuracy  0.9988\n","ep  207  training loss:  0.00543048767867114\n","valid loss  0.008329407311975956  and accuracy  0.9984\n","ep  208  training loss:  0.005685842738710772\n","valid loss  0.07704972475767136  and accuracy  0.9972\n","ep  209  training loss:  0.005173170816679025\n","valid loss  0.005433341953903437  and accuracy  0.998\n","ep  210  training loss:  0.005076694792742389\n","valid loss  0.041459519416093826  and accuracy  0.9968\n","ep  211  training loss:  0.005325181468382595\n","valid loss  0.031665705144405365  and accuracy  0.9984\n","ep  212  training loss:  0.005237788447223464\n","valid loss  0.01238342560827732  and accuracy  0.9984\n","ep  213  training loss:  0.005193239612688735\n","valid loss  0.20542795956134796  and accuracy  0.9972\n","ep  214  training loss:  0.005218314268665195\n","valid loss  0.05386389046907425  and accuracy  0.998\n","ep  215  training loss:  0.004915170520453796\n","valid loss  0.03607810288667679  and accuracy  0.9968\n","ep  216  training loss:  0.005554029799835347\n","valid loss  0.03141027316451073  and accuracy  0.9972\n","ep  217  training loss:  0.004856204561300256\n","valid loss  0.009098167531192303  and accuracy  0.9984\n","ep  218  training loss:  0.005141847849857944\n","valid loss  0.0366818830370903  and accuracy  0.9976\n","ep  219  training loss:  0.005228623437290749\n","valid loss  0.006156643852591515  and accuracy  0.9988\n","ep  220  training loss:  0.00561030281804669\n","valid loss  0.028149297460913658  and accuracy  0.9976\n","ep  221  training loss:  0.005031966708086065\n","valid loss  0.37515100836753845  and accuracy  0.9932\n","ep  222  training loss:  0.004974697919044619\n","valid loss  0.9778699278831482  and accuracy  0.9956\n","ep  223  training loss:  0.004900754052019712\n","valid loss  0.004520973656326532  and accuracy  0.9988\n","ep  224  training loss:  0.004829955428019663\n","valid loss  0.005903755780309439  and accuracy  0.9984\n","ep  225  training loss:  0.004637191854789628\n","valid loss  0.007274509407579899  and accuracy  0.998\n","ep  226  training loss:  0.005242505380438177\n","valid loss  0.013272427022457123  and accuracy  0.998\n","ep  227  training loss:  0.004500918686875046\n","valid loss  0.005664977245032787  and accuracy  0.9988\n","ep  228  training loss:  0.004730346619683475\n","valid loss  0.08495406806468964  and accuracy  0.9976\n","ep  229  training loss:  0.004380129908224503\n","valid loss  0.1437433660030365  and accuracy  0.998\n","ep  230  training loss:  0.0050354513984460095\n","valid loss  0.036605507135391235  and accuracy  0.9972\n","ep  231  training loss:  0.004768515292268533\n","valid loss  0.005930311977863312  and accuracy  0.9984\n","ep  232  training loss:  0.004479193221721268\n","valid loss  0.003683355636894703  and accuracy  0.9988\n","ep  233  training loss:  0.004560823305800494\n","valid loss  0.0496939942240715  and accuracy  0.9968\n","ep  234  training loss:  0.0046355041565552925\n","valid loss  0.010639478452503681  and accuracy  0.9984\n","ep  235  training loss:  0.004527225557360449\n","valid loss  0.032537322491407394  and accuracy  0.9976\n","ep  236  training loss:  0.004297583888559576\n","valid loss  0.00500422203913331  and accuracy  0.9984\n","ep  237  training loss:  0.004382817427780612\n","valid loss  0.004306319635361433  and accuracy  0.9984\n","ep  238  training loss:  0.004703955380736807\n","valid loss  0.004287806339561939  and accuracy  0.9988\n","ep  239  training loss:  0.004283187164051254\n","valid loss  0.16342370212078094  and accuracy  0.9976\n","ep  240  training loss:  0.0043546216507955055\n","valid loss  0.014409858733415604  and accuracy  0.998\n","ep  241  training loss:  0.004244391095944692\n","valid loss  0.006314108148217201  and accuracy  0.9992\n","ep  242  training loss:  0.004055722941955307\n","valid loss  0.0043939766474068165  and accuracy  0.9992\n","ep  243  training loss:  0.003917051516168681\n","valid loss  0.004439678508788347  and accuracy  0.9988\n","ep  244  training loss:  0.004428064425237283\n","valid loss  0.007046373561024666  and accuracy  0.9976\n","ep  245  training loss:  0.004264654702806494\n","valid loss  0.008958756923675537  and accuracy  0.998\n","ep  246  training loss:  0.0040804405108902\n","valid loss  0.36597105860710144  and accuracy  0.998\n","ep  247  training loss:  0.004223491567249626\n","valid loss  0.05157020315527916  and accuracy  0.9972\n","ep  248  training loss:  0.00415565985605247\n","valid loss  0.3733235001564026  and accuracy  0.9932\n","ep  249  training loss:  0.003937163692010597\n","valid loss  0.1598219871520996  and accuracy  0.9976\n","ep  250  training loss:  0.00486792112135981\n","valid loss  0.27124908566474915  and accuracy  0.998\n","ep  251  training loss:  0.004095293056876424\n","valid loss  0.003854320617392659  and accuracy  0.9984\n","ep  252  training loss:  0.003982464029261338\n","valid loss  0.1153690293431282  and accuracy  0.9972\n","ep  253  training loss:  0.003868400006707891\n","valid loss  0.3359520435333252  and accuracy  0.9972\n","ep  254  training loss:  0.003964970519835031\n","valid loss  0.003963914699852467  and accuracy  0.9988\n","ep  255  training loss:  0.003863164676276618\n","valid loss  0.03302939236164093  and accuracy  0.9968\n","ep  256  training loss:  0.004042378149538564\n","valid loss  0.004009007941931486  and accuracy  0.9988\n","ep  257  training loss:  0.004537808560207261\n","valid loss  0.14266546070575714  and accuracy  0.9976\n","ep  258  training loss:  0.004180365098501555\n","valid loss  0.40177398920059204  and accuracy  0.9984\n","ep  259  training loss:  0.003582893657542625\n","valid loss  0.0040388149209320545  and accuracy  0.9992\n","ep  260  training loss:  0.0040742271450906085\n","valid loss  0.004697589203715324  and accuracy  0.9988\n","ep  261  training loss:  0.0036885883499203145\n","valid loss  0.028741376474499702  and accuracy  0.9988\n","ep  262  training loss:  0.003514168779791611\n","valid loss  0.004089696798473597  and accuracy  0.9992\n","ep  263  training loss:  0.0038690489820057444\n","valid loss  0.0032365138176828623  and accuracy  0.9992\n","ep  264  training loss:  0.00396650502142885\n","valid loss  0.0962914526462555  and accuracy  0.998\n","ep  265  training loss:  0.0035595335263752127\n","valid loss  0.2888529598712921  and accuracy  0.9984\n","ep  266  training loss:  0.0032963556596283874\n","valid loss  1.2173287868499756  and accuracy  0.9956\n","ep  267  training loss:  0.0034788519201596024\n","valid loss  0.10395973175764084  and accuracy  0.998\n","ep  268  training loss:  0.003650156120520913\n","valid loss  0.3089144229888916  and accuracy  0.9984\n","ep  269  training loss:  0.003550916241012572\n","valid loss  0.1821954846382141  and accuracy  0.9976\n","ep  270  training loss:  0.003697683593300005\n","valid loss  0.011637876741588116  and accuracy  0.9988\n","ep  271  training loss:  0.0034712889991296756\n","valid loss  0.2126714289188385  and accuracy  0.9984\n","ep  272  training loss:  0.0031660932644627434\n","valid loss  0.03829479217529297  and accuracy  0.9984\n","ep  273  training loss:  0.0032685862810123622\n","valid loss  0.1724054515361786  and accuracy  0.998\n","ep  274  training loss:  0.00384619450667644\n","valid loss  0.004466771148145199  and accuracy  0.9984\n","ep  275  training loss:  0.0032682147115098933\n","valid loss  0.19147750735282898  and accuracy  0.998\n","ep  276  training loss:  0.0035658121824846674\n","valid loss  0.11461599171161652  and accuracy  0.998\n","ep  277  training loss:  0.0033570965351497383\n","valid loss  0.0026716222055256367  and accuracy  0.9988\n","ep  278  training loss:  0.0032854515803624725\n","valid loss  0.3243549168109894  and accuracy  0.9984\n","ep  279  training loss:  0.0032013290201278258\n","valid loss  0.027925677597522736  and accuracy  0.9984\n","ep  280  training loss:  0.0031072223661405382\n","valid loss  0.3201914131641388  and accuracy  0.9984\n","ep  281  training loss:  0.0030704022493486436\n","valid loss  0.0028217616491019726  and accuracy  0.9988\n","ep  282  training loss:  0.0030210299619322905\n","valid loss  0.002245567971840501  and accuracy  0.9992\n","ep  283  training loss:  0.003065094083963895\n","valid loss  0.0028236156795173883  and accuracy  0.9992\n","ep  284  training loss:  0.0035658039999894487\n","valid loss  0.015861734747886658  and accuracy  0.998\n","ep  285  training loss:  0.003805439404777727\n","valid loss  0.4101646840572357  and accuracy  0.9972\n","ep  286  training loss:  0.003517855736520085\n","valid loss  0.11767790466547012  and accuracy  0.998\n","ep  287  training loss:  0.0038302546891094444\n","valid loss  0.003191794501617551  and accuracy  0.9988\n","ep  288  training loss:  0.003478794411131644\n","valid loss  0.011338653042912483  and accuracy  0.9988\n","ep  289  training loss:  0.0030831885095381107\n","valid loss  0.004742912482470274  and accuracy  0.9992\n","ep  290  training loss:  0.003056607700614704\n","valid loss  0.004283171612769365  and accuracy  0.9992\n","ep  291  training loss:  0.0031106716751271597\n","valid loss  0.002644397085532546  and accuracy  0.9992\n","ep  292  training loss:  0.003224465134652328\n","valid loss  0.0029952151235193014  and accuracy  0.9992\n","ep  293  training loss:  0.003782027438542451\n","valid loss  0.3092665374279022  and accuracy  0.9984\n","ep  294  training loss:  0.0032198095726901875\n","valid loss  0.037211284041404724  and accuracy  0.9988\n","ep  295  training loss:  0.0028056109333687265\n","valid loss  0.002072735922411084  and accuracy  0.9992\n","ep  296  training loss:  0.00330014924418919\n","valid loss  0.0044700768776237965  and accuracy  0.9992\n","ep  297  training loss:  0.003030092431754686\n","valid loss  0.025634612888097763  and accuracy  0.9996\n","ep  298  training loss:  0.0030633940599488715\n","valid loss  0.22481948137283325  and accuracy  0.9984\n","ep  299  training loss:  0.0029294015781876083\n","valid loss  0.0023304689675569534  and accuracy  0.9988\n","ep  300  training loss:  0.0029500779049354818\n","valid loss  0.0019774280954152346  and accuracy  0.9992\n","ep  301  training loss:  0.002795961792656936\n","valid loss  0.17389222979545593  and accuracy  0.9976\n","ep  302  training loss:  0.0031610077429226604\n","valid loss  0.016815457493066788  and accuracy  0.9988\n","ep  303  training loss:  0.003266930425365307\n","valid loss  0.08542191237211227  and accuracy  0.9976\n","ep  304  training loss:  0.002781249622818306\n","valid loss  0.4170161187648773  and accuracy  0.9972\n","ep  305  training loss:  0.002727690665587232\n","valid loss  0.0026474660262465477  and accuracy  0.9992\n","ep  306  training loss:  0.0027640835471735587\n","valid loss  0.0022891934495419264  and accuracy  0.9988\n","ep  307  training loss:  0.0026667827557262454\n","valid loss  0.046477437019348145  and accuracy  0.9984\n","ep  308  training loss:  0.0033459911614527834\n","valid loss  0.20000693202018738  and accuracy  0.998\n","ep  309  training loss:  0.0036419076401670364\n","valid loss  0.004909995943307877  and accuracy  0.998\n","ep  310  training loss:  0.0036098833383709137\n","valid loss  0.16099929809570312  and accuracy  0.9988\n","ep  311  training loss:  0.0046016722426793204\n","valid loss  0.20679667592048645  and accuracy  0.998\n","ep  312  training loss:  0.003995870375133676\n","valid loss  0.015716135501861572  and accuracy  0.9996\n","ep  313  training loss:  0.002723085299993937\n","valid loss  0.004074319265782833  and accuracy  0.9984\n","ep  314  training loss:  0.0031450044774540054\n","valid loss  0.05137285590171814  and accuracy  0.9988\n","ep  315  training loss:  0.0028174270563806857\n","valid loss  0.18804919719696045  and accuracy  0.998\n","ep  316  training loss:  0.0028591086249742125\n","valid loss  0.05664266273379326  and accuracy  0.9984\n","ep  317  training loss:  0.0024656203906415655\n","valid loss  0.002593603450804949  and accuracy  0.9984\n","ep  318  training loss:  0.0029509727328932245\n","valid loss  0.11306457221508026  and accuracy  0.9976\n","ep  319  training loss:  0.002565959969189679\n","valid loss  0.06937599927186966  and accuracy  0.9984\n","ep  320  training loss:  0.002372544714904586\n","valid loss  0.002008169423788786  and accuracy  0.9992\n","ep  321  training loss:  0.00260755809098437\n","valid loss  0.18874572217464447  and accuracy  0.9984\n","ep  322  training loss:  0.0025763520839788257\n","valid loss  0.09655863791704178  and accuracy  0.9984\n","ep  323  training loss:  0.002797614001211508\n","valid loss  0.0021371017210185528  and accuracy  0.9992\n","ep  324  training loss:  0.0026360521000499152\n","valid loss  0.15324276685714722  and accuracy  0.9984\n","ep  325  training loss:  0.002632559331129449\n","valid loss  0.0017718580784276128  and accuracy  0.9992\n","ep  326  training loss:  0.002520079242752618\n","valid loss  0.002138051437214017  and accuracy  1.0\n","ep  327  training loss:  0.0024380621842719583\n","valid loss  0.15131784975528717  and accuracy  0.9972\n","ep  328  training loss:  0.003080571548102806\n","valid loss  0.19178363680839539  and accuracy  0.9976\n","ep  329  training loss:  0.002684885968506643\n","valid loss  0.04134799912571907  and accuracy  0.9988\n","ep  330  training loss:  0.003288080553376967\n","valid loss  0.00310704973526299  and accuracy  1.0\n","ep  331  training loss:  0.0029629626447308936\n","valid loss  0.07369773834943771  and accuracy  0.9976\n","ep  332  training loss:  0.0027205575841943255\n","valid loss  0.03168453648686409  and accuracy  0.998\n","ep  333  training loss:  0.00338147020462149\n","valid loss  0.0611581914126873  and accuracy  0.9992\n","ep  334  training loss:  0.0023530230525743412\n","valid loss  0.0018509391229599714  and accuracy  0.9992\n","ep  335  training loss:  0.0024522322888341115\n","valid loss  0.05992360785603523  and accuracy  0.9988\n","ep  336  training loss:  0.002223661549681\n","valid loss  0.001859620912000537  and accuracy  0.9992\n","ep  337  training loss:  0.002301715810228701\n","valid loss  0.13609729707241058  and accuracy  0.9988\n","ep  338  training loss:  0.002182837598943019\n","valid loss  0.035278741270303726  and accuracy  0.9996\n","ep  339  training loss:  0.0022892289301961535\n","valid loss  0.0015587315429002047  and accuracy  1.0\n","ep  340  training loss:  0.002491565262571627\n","valid loss  0.0030016431119292974  and accuracy  1.0\n","ep  341  training loss:  0.002133286137417013\n","valid loss  0.271984726190567  and accuracy  0.9988\n","ep  342  training loss:  0.002560328769478958\n","valid loss  0.0012124673230573535  and accuracy  1.0\n","ep  343  training loss:  0.0023854765813991763\n","valid loss  0.024399837478995323  and accuracy  0.9996\n","ep  344  training loss:  0.0025106415486494606\n","valid loss  0.0015350437024608254  and accuracy  1.0\n","ep  345  training loss:  0.0025272183515117935\n","valid loss  0.0016419931780546904  and accuracy  0.9992\n","ep  346  training loss:  0.002632265278436627\n","valid loss  0.0021952036768198013  and accuracy  0.9992\n","ep  347  training loss:  0.0027278232580555126\n","valid loss  0.2015836089849472  and accuracy  0.994\n","ep  348  training loss:  0.002212643378534366\n","valid loss  0.09562505781650543  and accuracy  0.9984\n","ep  349  training loss:  0.0021926589079735983\n","valid loss  0.0018130411626771092  and accuracy  1.0\n","ep  350  training loss:  0.0026660298227447043\n","valid loss  0.0014670881209895015  and accuracy  0.9992\n","ep  351  training loss:  0.0020508356865871695\n","valid loss  0.12008973211050034  and accuracy  0.9988\n","ep  352  training loss:  0.0022314824291540455\n","valid loss  0.00175322440918535  and accuracy  0.9992\n","ep  353  training loss:  0.0022113588873408667\n","valid loss  0.0014060157118365169  and accuracy  1.0\n","ep  354  training loss:  0.002323285079584302\n","valid loss  0.007949253544211388  and accuracy  0.9992\n","ep  355  training loss:  0.002197146982062881\n","valid loss  0.0017284982604905963  and accuracy  1.0\n","ep  356  training loss:  0.002047631249614714\n","valid loss  0.001910335966385901  and accuracy  0.9996\n","ep  357  training loss:  0.0021000861224880854\n","valid loss  0.0011765717063099146  and accuracy  1.0\n","ep  358  training loss:  0.0025158431454319653\n","valid loss  0.1415712982416153  and accuracy  0.9992\n","ep  359  training loss:  0.001870058060269453\n","valid loss  0.27634483575820923  and accuracy  0.9992\n","ep  360  training loss:  0.0022065285120840814\n","valid loss  0.001192786148749292  and accuracy  0.9996\n","ep  361  training loss:  0.0020263852607424982\n","valid loss  0.0722794160246849  and accuracy  0.9988\n","ep  362  training loss:  0.0019153107462822037\n","valid loss  0.06927282363176346  and accuracy  0.9988\n","ep  363  training loss:  0.0020397557821440686\n","valid loss  0.1613851934671402  and accuracy  0.9992\n","ep  364  training loss:  0.002133666696777169\n","valid loss  0.0010373310651630163  and accuracy  1.0\n","ep  365  training loss:  0.0020590131483415874\n","valid loss  0.0013494372833520174  and accuracy  1.0\n","ep  366  training loss:  0.002087941459537473\n","valid loss  0.001610754756256938  and accuracy  0.9996\n","ep  367  training loss:  0.002151253227898112\n","valid loss  0.0016177110373973846  and accuracy  0.9996\n","ep  368  training loss:  0.002222667774411177\n","valid loss  0.015378325246274471  and accuracy  0.9992\n","ep  369  training loss:  0.002259655815122174\n","valid loss  0.001745590940117836  and accuracy  1.0\n","ep  370  training loss:  0.001898991575251378\n","valid loss  0.07703708112239838  and accuracy  0.9992\n","ep  371  training loss:  0.001970696533217277\n","valid loss  0.0014255198184400797  and accuracy  1.0\n","ep  372  training loss:  0.0018850320760543705\n","valid loss  0.001279825926758349  and accuracy  1.0\n","ep  373  training loss:  0.0017166024891615617\n","valid loss  0.0010361064923927188  and accuracy  1.0\n","ep  374  training loss:  0.0021258047214968144\n","valid loss  0.07440046966075897  and accuracy  0.994\n","ep  375  training loss:  0.001875491471899654\n","valid loss  0.0012148012174293399  and accuracy  1.0\n","ep  376  training loss:  0.0027075177899080647\n","valid loss  0.0013233597856014967  and accuracy  1.0\n","ep  377  training loss:  0.0035588557322564305\n","valid loss  0.0036723236553370953  and accuracy  1.0\n","ep  378  training loss:  0.003652293956401374\n","valid loss  0.014053828082978725  and accuracy  0.998\n","ep  379  training loss:  0.002630536072745892\n","valid loss  0.06966326385736465  and accuracy  0.9984\n","ep  380  training loss:  0.0024970791255801393\n","valid loss  0.2074916958808899  and accuracy  0.9992\n","ep  381  training loss:  0.0024441463416154585\n","valid loss  0.006917102728039026  and accuracy  0.9996\n","ep  382  training loss:  0.0020479015534408595\n","valid loss  0.1545313596725464  and accuracy  0.9992\n","ep  383  training loss:  0.0020422130544048857\n","valid loss  0.0012092448305338621  and accuracy  1.0\n","ep  384  training loss:  0.0019955157684777122\n","valid loss  0.25935813784599304  and accuracy  0.9992\n","ep  385  training loss:  0.0018114324854850445\n","valid loss  0.005680851172655821  and accuracy  0.9996\n","ep  386  training loss:  0.0019346277568227572\n","valid loss  0.009312883950769901  and accuracy  0.9992\n","ep  387  training loss:  0.0020226618544543235\n","valid loss  0.5636872053146362  and accuracy  0.9968\n","ep  388  training loss:  0.001975033629960919\n","valid loss  0.0014384955866262317  and accuracy  1.0\n","ep  389  training loss:  0.0019287822295943763\n","valid loss  0.10685424506664276  and accuracy  0.9992\n","ep  390  training loss:  0.00172393221626998\n","valid loss  0.009750396013259888  and accuracy  0.9992\n","ep  391  training loss:  0.0018218427313083296\n","valid loss  0.0013733155792579055  and accuracy  1.0\n","ep  392  training loss:  0.0016884299087225893\n","valid loss  0.059845417737960815  and accuracy  0.9988\n","ep  393  training loss:  0.0021343508158933973\n","valid loss  0.002092879032716155  and accuracy  0.9996\n","ep  394  training loss:  0.0018542794150264786\n","valid loss  0.21797871589660645  and accuracy  0.9992\n","ep  395  training loss:  0.0019892359920735073\n","valid loss  0.14903366565704346  and accuracy  0.9984\n","ep  396  training loss:  0.002194370158628162\n","valid loss  0.0006807668250985444  and accuracy  1.0\n","ep  397  training loss:  0.001663674347249025\n","valid loss  0.0005079028196632862  and accuracy  1.0\n","ep  398  training loss:  0.0016638781300840108\n","valid loss  0.0013124970719218254  and accuracy  1.0\n","ep  399  training loss:  0.001867356901505521\n","valid loss  0.0013528051786124706  and accuracy  1.0\n","ep  400  training loss:  0.002041376939538115\n","valid loss  0.11381305009126663  and accuracy  0.9992\n","ep  401  training loss:  0.001676562702230238\n","valid loss  0.026745233684778214  and accuracy  0.9992\n","ep  402  training loss:  0.0017053555654331382\n","valid loss  0.15288503468036652  and accuracy  0.9992\n","ep  403  training loss:  0.0016269265722496371\n","valid loss  0.0009902145247906446  and accuracy  1.0\n","ep  404  training loss:  0.001720189451263131\n","valid loss  0.043066490441560745  and accuracy  0.9988\n","ep  405  training loss:  0.0016563460464222072\n","valid loss  0.000947437307331711  and accuracy  1.0\n","ep  406  training loss:  0.0019641711088495364\n","valid loss  0.043146129697561264  and accuracy  0.9992\n","ep  407  training loss:  0.0016141201521488282\n","valid loss  0.0005418400978669524  and accuracy  1.0\n","ep  408  training loss:  0.0018318444565175466\n","valid loss  0.0161544568836689  and accuracy  0.9996\n","ep  409  training loss:  0.0014178592644771195\n","valid loss  0.0009796954691410065  and accuracy  1.0\n","ep  410  training loss:  0.0018559543617286333\n","valid loss  0.025334419682621956  and accuracy  0.9996\n","ep  411  training loss:  0.0016811887938405152\n","valid loss  0.000747834041249007  and accuracy  1.0\n","ep  412  training loss:  0.0015129370611783755\n","valid loss  0.000915428448934108  and accuracy  1.0\n","ep  413  training loss:  0.0014168156359498718\n","valid loss  0.0006578500033356249  and accuracy  1.0\n","ep  414  training loss:  0.0015360465719461043\n","valid loss  0.0013375190319493413  and accuracy  0.9996\n","ep  415  training loss:  0.0014978803933706972\n","valid loss  0.037802476435899734  and accuracy  0.9996\n","ep  416  training loss:  0.0014573310596077533\n","valid loss  0.03543967381119728  and accuracy  0.9992\n","ep  417  training loss:  0.001568242244570229\n","valid loss  0.0006315077189356089  and accuracy  1.0\n","ep  418  training loss:  0.0014236279668654232\n","valid loss  0.0008940278785303235  and accuracy  1.0\n","ep  419  training loss:  0.0013987862551274586\n","valid loss  0.15672284364700317  and accuracy  0.9988\n","ep  420  training loss:  0.0014855809702110933\n","valid loss  0.01822584494948387  and accuracy  0.9992\n","ep  421  training loss:  0.0016028881665298897\n","valid loss  0.000606191752012819  and accuracy  1.0\n","ep  422  training loss:  0.001411785401357648\n","valid loss  0.07335786521434784  and accuracy  0.9992\n","ep  423  training loss:  0.001932766575643794\n","valid loss  0.006296500097960234  and accuracy  0.9996\n","ep  424  training loss:  0.0016042636295635004\n","valid loss  0.0018176467856392264  and accuracy  1.0\n","ep  425  training loss:  0.0016217480303899023\n","valid loss  0.0007795343408361077  and accuracy  1.0\n","ep  426  training loss:  0.0014969391431287158\n","valid loss  0.0009696686756797135  and accuracy  1.0\n","ep  427  training loss:  0.0017947867966236165\n","valid loss  0.008791360072791576  and accuracy  0.9996\n","ep  428  training loss:  0.001355096494380175\n","valid loss  0.0005048387101851404  and accuracy  1.0\n","ep  429  training loss:  0.00175598304496479\n","valid loss  0.0012703373795375228  and accuracy  1.0\n","ep  430  training loss:  0.0013557894557724617\n","valid loss  0.0011718729510903358  and accuracy  1.0\n","ep  431  training loss:  0.001503623613319402\n","valid loss  0.00038036538171581924  and accuracy  1.0\n","ep  432  training loss:  0.0018178638968298574\n","valid loss  0.02519552782177925  and accuracy  0.9992\n","ep  433  training loss:  0.001972617866637254\n","valid loss  0.0016378456493839622  and accuracy  1.0\n","ep  434  training loss:  0.0025961974594541242\n","valid loss  0.0020883926190435886  and accuracy  1.0\n","ep  435  training loss:  0.001787632610846304\n","valid loss  0.008916378021240234  and accuracy  0.9996\n","ep  436  training loss:  0.0017964105095842945\n","valid loss  0.0009137839078903198  and accuracy  1.0\n","ep  437  training loss:  0.0014257211689986762\n","valid loss  0.001115805353038013  and accuracy  0.9996\n","ep  438  training loss:  0.0015580592920122582\n","valid loss  0.0017219050787389278  and accuracy  0.9996\n","ep  439  training loss:  0.0014046617523205214\n","valid loss  0.000882817548699677  and accuracy  1.0\n","ep  440  training loss:  0.0014510513644333792\n","valid loss  0.0011851583840325475  and accuracy  1.0\n","ep  441  training loss:  0.0017030325697846079\n","valid loss  0.0007036345778033137  and accuracy  1.0\n","ep  442  training loss:  0.0014166753339809087\n","valid loss  0.03776905685663223  and accuracy  0.9996\n","ep  443  training loss:  0.0013076206051526216\n","valid loss  0.0004441062919795513  and accuracy  1.0\n","ep  444  training loss:  0.0015218066353957845\n","valid loss  0.00048801559023559093  and accuracy  1.0\n","ep  445  training loss:  0.0013698124090409294\n","valid loss  0.003957692068070173  and accuracy  1.0\n","ep  446  training loss:  0.001253648158749492\n","valid loss  0.0008011636673472822  and accuracy  0.9996\n","ep  447  training loss:  0.0012654834027136413\n","valid loss  0.0004576527571771294  and accuracy  1.0\n","ep  448  training loss:  0.0011443978320493581\n","valid loss  0.0009637848124839365  and accuracy  1.0\n","ep  449  training loss:  0.0014001843716589895\n","valid loss  0.00041561125544831157  and accuracy  1.0\n","ep  450  training loss:  0.0014389316550823214\n","valid loss  0.0006998208700679243  and accuracy  1.0\n","ep  451  training loss:  0.0012093392267895423\n","valid loss  0.0005369954742491245  and accuracy  1.0\n","ep  452  training loss:  0.0010736356486539852\n","valid loss  0.0007656709058210254  and accuracy  1.0\n","ep  453  training loss:  0.0011913086685929098\n","valid loss  0.0004989150329492986  and accuracy  1.0\n","ep  454  training loss:  0.001065602744162322\n","valid loss  0.00028618628857657313  and accuracy  1.0\n","ep  455  training loss:  0.001234253606297017\n","valid loss  0.000736167945433408  and accuracy  1.0\n","ep  456  training loss:  0.001157397553340212\n","valid loss  0.0006673845346085727  and accuracy  1.0\n","ep  457  training loss:  0.0010993000921505028\n","valid loss  0.003014323767274618  and accuracy  0.9996\n","ep  458  training loss:  0.0012151560534456286\n","valid loss  0.08422738313674927  and accuracy  0.9992\n","ep  459  training loss:  0.0010845548442837364\n","valid loss  0.0005316631286405027  and accuracy  1.0\n","ep  460  training loss:  0.001017433411028825\n","valid loss  0.0005979792331345379  and accuracy  1.0\n","ep  461  training loss:  0.0009771196945087544\n","valid loss  0.0010732875671237707  and accuracy  1.0\n","ep  462  training loss:  0.0015413290927841525\n","valid loss  0.005111059173941612  and accuracy  0.9996\n","ep  463  training loss:  0.001320288131162434\n","valid loss  0.0023007818963378668  and accuracy  1.0\n","ep  464  training loss:  0.0009776872209830887\n","valid loss  0.0003493413678370416  and accuracy  1.0\n","ep  465  training loss:  0.001064768914237104\n","valid loss  0.05301736295223236  and accuracy  0.9992\n","ep  466  training loss:  0.0009801759389367322\n","valid loss  0.00028362759621813893  and accuracy  1.0\n","ep  467  training loss:  0.0010747137917504663\n","valid loss  0.002574981888756156  and accuracy  0.9996\n","ep  468  training loss:  0.0009219857811479903\n","valid loss  0.0006552160484716296  and accuracy  1.0\n","ep  469  training loss:  0.0009782461927998372\n","valid loss  0.00030242252978496253  and accuracy  1.0\n","ep  470  training loss:  0.0010479744882669767\n","valid loss  0.07030311226844788  and accuracy  0.9992\n","ep  471  training loss:  0.0010410093395719175\n","valid loss  0.025962451472878456  and accuracy  0.9992\n","ep  472  training loss:  0.0010322087557466282\n","valid loss  0.0005698464228771627  and accuracy  1.0\n","ep  473  training loss:  0.0009293692457381927\n","valid loss  0.07456449419260025  and accuracy  0.9992\n","ep  474  training loss:  0.0010023615387878152\n","valid loss  0.0007361638126894832  and accuracy  1.0\n","ep  475  training loss:  0.0012250465729715544\n","valid loss  0.0004918720806017518  and accuracy  1.0\n","ep  476  training loss:  0.00101357985442044\n","valid loss  0.008689835667610168  and accuracy  0.9996\n","ep  477  training loss:  0.0009061072408058074\n","valid loss  0.0003896623966284096  and accuracy  1.0\n","ep  478  training loss:  0.0008966478910553819\n","valid loss  0.0002953123184852302  and accuracy  1.0\n","ep  479  training loss:  0.0011731500502987153\n","valid loss  0.0007235184893943369  and accuracy  1.0\n","ep  480  training loss:  0.0009986824861731604\n","valid loss  0.5999824404716492  and accuracy  0.9976\n","ep  481  training loss:  0.0015981739684758874\n","valid loss  0.0025986596010625362  and accuracy  1.0\n","ep  482  training loss:  0.00090344241204847\n","valid loss  0.014839748851954937  and accuracy  0.9996\n","ep  483  training loss:  0.0009132470658964516\n","valid loss  0.0003339177055750042  and accuracy  1.0\n","ep  484  training loss:  0.0008912531977791851\n","valid loss  0.0005033907946199179  and accuracy  1.0\n","ep  485  training loss:  0.001424554150172485\n","valid loss  0.000964667706284672  and accuracy  1.0\n","ep  486  training loss:  0.0011538926974808497\n","valid loss  0.0006540701724588871  and accuracy  1.0\n","ep  487  training loss:  0.0009063055099897438\n","valid loss  0.2189147025346756  and accuracy  0.998\n","ep  488  training loss:  0.0010220955099475015\n","valid loss  0.0007663547876290977  and accuracy  1.0\n","ep  489  training loss:  0.0009316844369063683\n","valid loss  0.0004308252828195691  and accuracy  1.0\n","ep  490  training loss:  0.0008712790005885206\n","valid loss  0.0003973652492277324  and accuracy  1.0\n","ep  491  training loss:  0.0008766642696001899\n","valid loss  0.0004864464863203466  and accuracy  1.0\n","ep  492  training loss:  0.0009281728191042871\n","valid loss  0.0014800982316955924  and accuracy  1.0\n","ep  493  training loss:  0.0009384536079871724\n","valid loss  0.0013241064734756947  and accuracy  1.0\n","ep  494  training loss:  0.0009210661585600105\n","valid loss  0.000396584888221696  and accuracy  1.0\n","ep  495  training loss:  0.0009212251862966265\n","valid loss  0.0005648279911838472  and accuracy  1.0\n","ep  496  training loss:  0.0008716422337914581\n","valid loss  1.5404714345932007  and accuracy  0.9968\n","ep  497  training loss:  0.0007542868907820616\n","valid loss  0.0005502304993569851  and accuracy  1.0\n","ep  498  training loss:  0.0007742507135765464\n","valid loss  0.00033134969999082386  and accuracy  1.0\n","ep  499  training loss:  0.0010092504358701498\n","valid loss  0.0002936194359790534  and accuracy  1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lyj8CC2dr8P7"},"source":["### ***PREDICTION***"]},{"cell_type":"code","metadata":{"id":"1WL6v0uDgDJA"},"source":["\"\"\" Effettuiamo le predizioni sul dataset di test \"\"\"\n","\n","test_ds = CTU_Dataset(test, np.zeros(len(test)))\n","test_dl = DataLoader(test_ds, batch_size=batch_size)\n","test_dl = DeviceDataLoader(test_dl, device)\n","\n","# Utilizziamo la funzione softmax poiché siamo interessati alla probabilità per ogni classe\n","preds = []\n","model.eval()\n","with torch.no_grad():\n","    for x, y in test_dl:\n","        out = model(x)\n","        prob = F.softmax(out, dim=1)\n","        preds.append(prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syhoruAUlMAX"},"source":["y_pred = []\n","for i in range(0, len(preds)):\n","  pred = preds[i].cpu()\n","  temp = np.argmax(pred, 1)\n","  temp = np.array(temp)\n","  y_pred = np.append(y_pred, temp)\n","\n","y_pred = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3hmjVkNmFkX","executionInfo":{"status":"ok","timestamp":1624964845438,"user_tz":-120,"elapsed":35,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"9dc2a3da-84dc-4b8d-c8a1-d60a84a9c7c1"},"source":["y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, ..., 1, 1, 1])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"IsdRVjr9sFbO"},"source":["### ***EVALUATION***"]},{"cell_type":"code","metadata":{"id":"WOciCjPisC_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624964845439,"user_tz":-120,"elapsed":33,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"3b564751-453d-4995-bdfc-ad3100a8c622"},"source":["print('Test:', Counter(y_test))\n","print('Pred:', Counter(y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: Counter({1: 15702, 0: 35})\n","Pred: Counter({1: 15695, 0: 42})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oqp05uc1-d4t"},"source":["# Matrice di confusione, accuracy, classification_report\n","from sklearn.metrics import *\n","\n","# y_test è la variabile che contiene i valori effettivi\n","# y_pred contiene i valori predetti dal modello\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","acc = accuracy_score(y_test, y_pred)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","# non presente nella libreria, calcolo mediante formula\n","f2 = (1+2**2)*((precision*recall)/((2**2*precision)+recall))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"8rOxIo2L-d4z","executionInfo":{"status":"ok","timestamp":1624964845824,"user_tz":-120,"elapsed":411,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"a021cd39-6f5b-4139-8a26-bbd6729a0f6c"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","target_dict = {'BotNet' : 0,\n","               'normal' : 1}\n","\n","disp = ConfusionMatrixDisplay(cm, target_dict)\n","disp.plot()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f862309ff50>"]},"metadata":{"tags":[]},"execution_count":29},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELCQniZh-d40","executionInfo":{"status":"ok","timestamp":1624964845826,"user_tz":-120,"elapsed":40,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"e479b202-9cec-4d26-9bcc-387fb9ee86ac"},"source":["mcm = multilabel_confusion_matrix(y_test, y_pred)\n","print(mcm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[15688    14]\n","  [    7    28]]\n","\n"," [[   28     7]\n","  [   14 15688]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7P4q-xyB7_n","executionInfo":{"status":"ok","timestamp":1624964845827,"user_tz":-120,"elapsed":36,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"2423b073-d3f1-4fca-84a3-9cba317746e7"},"source":["FP = cm.sum (axis = 0) - np.diag (cm) \n","FN = cm.sum (axis = 1) - np.diag (cm) \n","TP = np.diag (cm) \n","TN = cm.sum () - (FP + FN + TP)\n","\n","print('True positive: ', TP)\n","print('True negative: ', TN)\n","print('False positive: ', FP)\n","print('False negative: ', FN)\n","\n","FP = FP.astype(float)\n","FN = FN.astype(float)\n","TP = TP.astype(float)\n","TN = TN.astype(float)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","# False negative rate\n","FNR = FN/(TP+FN)\n","\n","print('True positive rate: ', TPR)\n","print('True negative rate: ', TNR)\n","print('False positive rate: ', FPR)\n","print('False negative rate: ', FNR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True positive:  [   28 15688]\n","True negative:  [15688    28]\n","False positive:  [14  7]\n","False negative:  [ 7 14]\n","True positive rate:  [0.8        0.99910839]\n","True negative rate:  [0.99910839 0.8       ]\n","False positive rate:  [0.00089161 0.2       ]\n","False negative rate:  [0.2        0.00089161]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89Nap2dd-d40","executionInfo":{"status":"ok","timestamp":1624964845828,"user_tz":-120,"elapsed":32,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"6ed200d8-0a3c-48cd-ad4f-71ed0c56f998"},"source":["print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   28     7]\n"," [   14 15688]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wE1uRjas-d41","executionInfo":{"status":"ok","timestamp":1624964845830,"user_tz":-120,"elapsed":30,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"24de78cc-cbd6-41dc-ed29-03b5f995d334"},"source":["print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.80      0.73        35\n","           1       1.00      1.00      1.00     15702\n","\n","    accuracy                           1.00     15737\n","   macro avg       0.83      0.90      0.86     15737\n","weighted avg       1.00      1.00      1.00     15737\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJICMuXI-d41","executionInfo":{"status":"ok","timestamp":1624964845831,"user_tz":-120,"elapsed":26,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"85e03358-1298-448d-effc-d840ddd29b05"},"source":["print('Accuracy: ', acc)\n","print('Precision_weighted: ', precision)\n","print('Recall_weighted: ', recall)\n","print('mcc: ', mcc)\n","print('f2: ', f2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy:  0.9986655652284425\n","Precision_weighted:  0.9988136373717961\n","Recall_weighted:  0.9986655652284425\n","mcc:  0.7296454792242311\n","f2:  0.9986951761447848\n"],"name":"stdout"}]}]}