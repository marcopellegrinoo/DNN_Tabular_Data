{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NSLKDD_PyTorch.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-gGAofDSdBzR7-7d7FdrhgveQKXZsV-J","authorship_tag":"ABX9TyMv2A7o1eoa0OHN7/Lcekop"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sHDsrAJqmeaY"},"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as torch_optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGYFahn6d0Ef","executionInfo":{"status":"ok","timestamp":1621269466613,"user_tz":-120,"elapsed":982,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"dd99a7e8-85f9-4cda-9ffb-a08ab5b4f502"},"source":["print(torch.cuda.device_count())            # Numero di GPU disponibili\n","print(torch.cuda.get_device_name(0))        # Nome della prima GPU disponibile\n","print(torch.cuda.current_device())        # Device in uso al momento\n","print(torch.cuda.set_device(0))             # Imposta la prima GPU come default\n","print(torch.cuda.get_device_capability(0))  # Verifica le capacit√† della prima GPU"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","Tesla T4\n","0\n","None\n","(7, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04ZXjd9MhmlY"},"source":["path = './drive/MyDrive/Materiale_Pellegrino_personal/NSLKDD/NSLKDD_Full.csv'\n","dataset = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"id":"WIqtM3qQMThm","executionInfo":{"status":"ok","timestamp":1621269468726,"user_tz":-120,"elapsed":697,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"f6d49504-e7fe-40d6-f06c-900c6b20df5a"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>protocol_type</th>\n","      <th>service</th>\n","      <th>flag</th>\n","      <th>symbolic</th>\n","      <th>src_bytes</th>\n","      <th>dst_bytes</th>\n","      <th>DoS</th>\n","      <th>wrong_fragment</th>\n","      <th>urgent</th>\n","      <th>hot</th>\n","      <th>num_failed_logins</th>\n","      <th>logged_in</th>\n","      <th>num_compromised</th>\n","      <th>root_shell</th>\n","      <th>su_attempted</th>\n","      <th>num_root</th>\n","      <th>num_file_creations</th>\n","      <th>num_shells</th>\n","      <th>num_access_files</th>\n","      <th>num_outbound_cmds</th>\n","      <th>is_host_login</th>\n","      <th>is_guest_login</th>\n","      <th>count</th>\n","      <th>srv_count</th>\n","      <th>serror_rate</th>\n","      <th>srv_serror_rate</th>\n","      <th>rerror_rate</th>\n","      <th>srv_rerror_rate</th>\n","      <th>same_srv_rate</th>\n","      <th>diff_srv_rate</th>\n","      <th>srv_diff_host_rate</th>\n","      <th>dst_host_count</th>\n","      <th>dst_host_srv_count</th>\n","      <th>dst_host_same_srv_rate</th>\n","      <th>dst_host_diff_srv_rate</th>\n","      <th>dst_host_same_src_port_rate</th>\n","      <th>dst_host_srv_diff_host_rate</th>\n","      <th>dst_host_serror_rate</th>\n","      <th>dst_host_srv_serror_rate</th>\n","      <th>dst_host_rerror_rate</th>\n","      <th>dst_host_srv_rerror_rate</th>\n","      <th>binlabel</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>ftp_data</td>\n","      <td>SF</td>\n","      <td>491</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>150</td>\n","      <td>25</td>\n","      <td>0.17</td>\n","      <td>0.03</td>\n","      <td>0.17</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>20</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>udp</td>\n","      <td>other</td>\n","      <td>SF</td>\n","      <td>146</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.08</td>\n","      <td>0.15</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","      <td>0.88</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>15</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>private</td>\n","      <td>S0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>123</td>\n","      <td>6</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.05</td>\n","      <td>0.07</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>26</td>\n","      <td>0.10</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>19</td>\n","      <td>anomaly</td>\n","      <td>DoS</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>http</td>\n","      <td>SF</td>\n","      <td>232</td>\n","      <td>8153</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0.2</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>30</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.03</td>\n","      <td>0.04</td>\n","      <td>0.03</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>21</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>http</td>\n","      <td>SF</td>\n","      <td>199</td>\n","      <td>420</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.09</td>\n","      <td>255</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>21</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>148511</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>smtp</td>\n","      <td>SF</td>\n","      <td>794</td>\n","      <td>333</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>100</td>\n","      <td>141</td>\n","      <td>0.72</td>\n","      <td>0.06</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>21</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>148512</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>http</td>\n","      <td>SF</td>\n","      <td>317</td>\n","      <td>938</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.18</td>\n","      <td>197</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>21</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>148513</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>http</td>\n","      <td>SF</td>\n","      <td>54540</td>\n","      <td>8314</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.20</td>\n","      <td>255</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.07</td>\n","      <td>0.07</td>\n","      <td>15</td>\n","      <td>anomaly</td>\n","      <td>DoS</td>\n","    </tr>\n","    <tr>\n","      <th>148514</th>\n","      <td>0</td>\n","      <td>udp</td>\n","      <td>domain_u</td>\n","      <td>SF</td>\n","      <td>42</td>\n","      <td>42</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.33</td>\n","      <td>255</td>\n","      <td>252</td>\n","      <td>0.99</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>21</td>\n","      <td>normal</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>148515</th>\n","      <td>0</td>\n","      <td>tcp</td>\n","      <td>sunrpc</td>\n","      <td>REJ</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>255</td>\n","      <td>21</td>\n","      <td>0.08</td>\n","      <td>0.03</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.44</td>\n","      <td>1.00</td>\n","      <td>14</td>\n","      <td>normal</td>\n","      <td>Probe</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>148516 rows √ó 44 columns</p>\n","</div>"],"text/plain":["        duration protocol_type  ... binlabel   label\n","0              0           tcp  ...   normal  normal\n","1              0           udp  ...   normal  normal\n","2              0           tcp  ...  anomaly     DoS\n","3              0           tcp  ...   normal  normal\n","4              0           tcp  ...   normal  normal\n","...          ...           ...  ...      ...     ...\n","148511         0           tcp  ...   normal  normal\n","148512         0           tcp  ...   normal  normal\n","148513         0           tcp  ...  anomaly     DoS\n","148514         0           udp  ...   normal  normal\n","148515         0           tcp  ...   normal   Probe\n","\n","[148516 rows x 44 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"1BkHPKMrOBAf"},"source":["# Elimino la colonna 'num_access_files' poich√© inutile\n","dataset = dataset.drop('num_access_files', axis=1)\n","\n","# Elimino la colonna 'binlabel' dal dataset\n","dataset = dataset.drop('binlabel', axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqGImh5iNT_u"},"source":["dep_var = 'label'\n","cat_names = [\"protocol_type\", \"service\", \"flag\"]\n","cont_names = [col for col in dataset.columns if col not in cat_names and col != dep_var]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"An02-kHCNdnW","executionInfo":{"status":"ok","timestamp":1620317958438,"user_tz":-120,"elapsed":636,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"69b46d6c-1430-4e63-d040-e2a0b805c7fd"},"source":["len(cont_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"ghwOUeUlMegO"},"source":["# LabelEncoding della variabile target \n","target_index = dataset.columns.get_loc(dep_var)\n","dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[dep_var])\n","\n","#LabelEncoding delle variabili categoriali\n","for col in cat_names:\n","  target_index = dataset.columns.get_loc(col)\n","  dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[col])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"fgf6kpUyNHjP","executionInfo":{"status":"ok","timestamp":1621269480126,"user_tz":-120,"elapsed":434,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8ef3de25-2112-4139-93c6-35997a8b6332"},"source":["dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>protocol_type</th>\n","      <th>service</th>\n","      <th>flag</th>\n","      <th>symbolic</th>\n","      <th>src_bytes</th>\n","      <th>dst_bytes</th>\n","      <th>DoS</th>\n","      <th>wrong_fragment</th>\n","      <th>urgent</th>\n","      <th>hot</th>\n","      <th>num_failed_logins</th>\n","      <th>logged_in</th>\n","      <th>num_compromised</th>\n","      <th>root_shell</th>\n","      <th>su_attempted</th>\n","      <th>num_root</th>\n","      <th>num_file_creations</th>\n","      <th>num_shells</th>\n","      <th>num_outbound_cmds</th>\n","      <th>is_host_login</th>\n","      <th>is_guest_login</th>\n","      <th>count</th>\n","      <th>srv_count</th>\n","      <th>serror_rate</th>\n","      <th>srv_serror_rate</th>\n","      <th>rerror_rate</th>\n","      <th>srv_rerror_rate</th>\n","      <th>same_srv_rate</th>\n","      <th>diff_srv_rate</th>\n","      <th>srv_diff_host_rate</th>\n","      <th>dst_host_count</th>\n","      <th>dst_host_srv_count</th>\n","      <th>dst_host_same_srv_rate</th>\n","      <th>dst_host_diff_srv_rate</th>\n","      <th>dst_host_same_src_port_rate</th>\n","      <th>dst_host_srv_diff_host_rate</th>\n","      <th>dst_host_serror_rate</th>\n","      <th>dst_host_srv_serror_rate</th>\n","      <th>dst_host_rerror_rate</th>\n","      <th>dst_host_srv_rerror_rate</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>9</td>\n","      <td>491</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>150</td>\n","      <td>25</td>\n","      <td>0.17</td>\n","      <td>0.03</td>\n","      <td>0.17</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>20</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>44</td>\n","      <td>9</td>\n","      <td>146</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.08</td>\n","      <td>0.15</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","      <td>0.88</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>15</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>49</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>123</td>\n","      <td>6</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.05</td>\n","      <td>0.07</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>26</td>\n","      <td>0.10</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>19</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>232</td>\n","      <td>8153</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0.2</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>30</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.03</td>\n","      <td>0.04</td>\n","      <td>0.03</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>21</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>199</td>\n","      <td>420</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.09</td>\n","      <td>255</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>21</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   duration  protocol_type  ...  dst_host_srv_rerror_rate  label\n","0         0              1  ...                        20      2\n","1         0              2  ...                        15      2\n","2         0              1  ...                        19      0\n","3         0              1  ...                        21      2\n","4         0              1  ...                        21      2\n","\n","[5 rows x 42 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"GL5xD37fnaia"},"source":["from sklearn.model_selection import train_test_split\n","\n","# train 50% e test 50%\n","train, test = train_test_split(dataset, test_size=0.50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ0TyYvfQLgW"},"source":["y_train = train[dep_var]\n","train = train.drop(dep_var, axis=1)\n","y_test = test[dep_var]\n","test = test.drop(dep_var, axis=1)\n","\n","# validation di 2500 righe da train\n","train, validation, y_train, y_val = train_test_split(train, y_train, test_size=0.033666, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5xEsk56PSsW"},"source":["y_train = y_train.values\n","y_test = y_test.values\n","y_val = y_val.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLigb6pKPYk2","executionInfo":{"status":"ok","timestamp":1621269496365,"user_tz":-120,"elapsed":377,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"e74db320-de6f-4af7-9a5a-497d38cdc913"},"source":["y_val"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 2, 2, ..., 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"id":"IKp_gSloQTZO","executionInfo":{"status":"ok","timestamp":1621269498862,"user_tz":-120,"elapsed":1073,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"3a3ea51b-c942-4994-d1d4-0c99fe7be635"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>protocol_type</th>\n","      <th>service</th>\n","      <th>flag</th>\n","      <th>symbolic</th>\n","      <th>src_bytes</th>\n","      <th>dst_bytes</th>\n","      <th>DoS</th>\n","      <th>wrong_fragment</th>\n","      <th>urgent</th>\n","      <th>hot</th>\n","      <th>num_failed_logins</th>\n","      <th>logged_in</th>\n","      <th>num_compromised</th>\n","      <th>root_shell</th>\n","      <th>su_attempted</th>\n","      <th>num_root</th>\n","      <th>num_file_creations</th>\n","      <th>num_shells</th>\n","      <th>num_outbound_cmds</th>\n","      <th>is_host_login</th>\n","      <th>is_guest_login</th>\n","      <th>count</th>\n","      <th>srv_count</th>\n","      <th>serror_rate</th>\n","      <th>srv_serror_rate</th>\n","      <th>rerror_rate</th>\n","      <th>srv_rerror_rate</th>\n","      <th>same_srv_rate</th>\n","      <th>diff_srv_rate</th>\n","      <th>srv_diff_host_rate</th>\n","      <th>dst_host_count</th>\n","      <th>dst_host_srv_count</th>\n","      <th>dst_host_same_srv_rate</th>\n","      <th>dst_host_diff_srv_rate</th>\n","      <th>dst_host_same_src_port_rate</th>\n","      <th>dst_host_srv_diff_host_rate</th>\n","      <th>dst_host_serror_rate</th>\n","      <th>dst_host_srv_serror_rate</th>\n","      <th>dst_host_rerror_rate</th>\n","      <th>dst_host_srv_rerror_rate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>51891</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>49</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>103</td>\n","      <td>14</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.14</td>\n","      <td>0.06</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>14</td>\n","      <td>0.05</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>102171</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>49</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>73</td>\n","      <td>3</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.04</td>\n","      <td>0.08</td>\n","      <td>0.00</td>\n","      <td>180</td>\n","      <td>3</td>\n","      <td>0.02</td>\n","      <td>0.07</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>44390</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>49</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>258</td>\n","      <td>20</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.08</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>5</td>\n","      <td>0.02</td>\n","      <td>0.08</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>127542</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>328</td>\n","      <td>2375</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>19</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.21</td>\n","      <td>24</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.04</td>\n","      <td>0.02</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>11713</th>\n","      <td>23120</td>\n","      <td>2</td>\n","      <td>44</td>\n","      <td>9</td>\n","      <td>146</td>\n","      <td>105</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>0.17</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.33</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>2</td>\n","      <td>0.01</td>\n","      <td>0.56</td>\n","      <td>0.88</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>65312</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>67</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>65</td>\n","      <td>19</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.29</td>\n","      <td>0.08</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>19</td>\n","      <td>0.07</td>\n","      <td>0.08</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>48932</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>218</td>\n","      <td>306</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>36</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.03</td>\n","      <td>0.04</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>64242</th>\n","      <td>24</td>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>9</td>\n","      <td>239</td>\n","      <td>774</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>0.33</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>94316</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>274</td>\n","      <td>17</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.06</td>\n","      <td>0.06</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>17</td>\n","      <td>0.07</td>\n","      <td>0.07</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>79230</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>9</td>\n","      <td>1032</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>155</td>\n","      <td>155</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>155</td>\n","      <td>0.61</td>\n","      <td>0.01</td>\n","      <td>0.61</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>74258 rows √ó 41 columns</p>\n","</div>"],"text/plain":["        duration  protocol_type  ...  dst_host_rerror_rate  dst_host_srv_rerror_rate\n","51891          0              1  ...                   0.0                        21\n","102171         0              1  ...                   0.0                        18\n","44390          0              1  ...                   0.0                        21\n","127542         0              1  ...                   0.0                        21\n","11713      23120              2  ...                   0.0                        19\n","...          ...            ...  ...                   ...                       ...\n","65312          0              1  ...                   0.0                        18\n","48932          0              1  ...                   0.0                        21\n","64242         24              1  ...                   0.0                        21\n","94316          0              1  ...                   1.0                        20\n","79230          0              0  ...                   0.0                        18\n","\n","[74258 rows x 41 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"id":"79A_sfwnQVqX","executionInfo":{"status":"ok","timestamp":1621269508115,"user_tz":-120,"elapsed":504,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"3ae0871d-c689-4469-967e-297c49e88f5b"},"source":["train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>protocol_type</th>\n","      <th>service</th>\n","      <th>flag</th>\n","      <th>symbolic</th>\n","      <th>src_bytes</th>\n","      <th>dst_bytes</th>\n","      <th>DoS</th>\n","      <th>wrong_fragment</th>\n","      <th>urgent</th>\n","      <th>hot</th>\n","      <th>num_failed_logins</th>\n","      <th>logged_in</th>\n","      <th>num_compromised</th>\n","      <th>root_shell</th>\n","      <th>su_attempted</th>\n","      <th>num_root</th>\n","      <th>num_file_creations</th>\n","      <th>num_shells</th>\n","      <th>num_outbound_cmds</th>\n","      <th>is_host_login</th>\n","      <th>is_guest_login</th>\n","      <th>count</th>\n","      <th>srv_count</th>\n","      <th>serror_rate</th>\n","      <th>srv_serror_rate</th>\n","      <th>rerror_rate</th>\n","      <th>srv_rerror_rate</th>\n","      <th>same_srv_rate</th>\n","      <th>diff_srv_rate</th>\n","      <th>srv_diff_host_rate</th>\n","      <th>dst_host_count</th>\n","      <th>dst_host_srv_count</th>\n","      <th>dst_host_same_srv_rate</th>\n","      <th>dst_host_diff_srv_rate</th>\n","      <th>dst_host_same_src_port_rate</th>\n","      <th>dst_host_srv_diff_host_rate</th>\n","      <th>dst_host_serror_rate</th>\n","      <th>dst_host_srv_serror_rate</th>\n","      <th>dst_host_rerror_rate</th>\n","      <th>dst_host_srv_rerror_rate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>119981</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>54</td>\n","      <td>9</td>\n","      <td>1041</td>\n","      <td>330</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>82</td>\n","      <td>184</td>\n","      <td>0.72</td>\n","      <td>0.07</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>71403</th>\n","      <td>8855</td>\n","      <td>1</td>\n","      <td>44</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>2</td>\n","      <td>0.01</td>\n","      <td>0.74</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>61697</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>49</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>122</td>\n","      <td>18</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.15</td>\n","      <td>0.07</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>19</td>\n","      <td>0.07</td>\n","      <td>0.07</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>15939</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>214</td>\n","      <td>1533</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>13</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.15</td>\n","      <td>132</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>140372</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>44</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>159</td>\n","      <td>10</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.06</td>\n","      <td>0.47</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>10</td>\n","      <td>0.04</td>\n","      <td>0.65</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>70332</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>197</td>\n","      <td>5790</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>749</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9</td>\n","      <td>327</td>\n","      <td>6721</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.40</td>\n","      <td>255</td>\n","      <td>255</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>73724</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>41</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>101</td>\n","      <td>8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.08</td>\n","      <td>0.06</td>\n","      <td>0.00</td>\n","      <td>255</td>\n","      <td>15</td>\n","      <td>0.06</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>14423</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>9</td>\n","      <td>1874</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>11</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.92</td>\n","      <td>0.17</td>\n","      <td>0.00</td>\n","      <td>49</td>\n","      <td>102</td>\n","      <td>0.67</td>\n","      <td>0.14</td>\n","      <td>0.71</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>29253</th>\n","      <td>14479</td>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>9</td>\n","      <td>8412</td>\n","      <td>12031</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>64</td>\n","      <td>18</td>\n","      <td>0.27</td>\n","      <td>0.05</td>\n","      <td>0.02</td>\n","      <td>0.11</td>\n","      <td>0.25</td>\n","      <td>0.89</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>16</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>71758 rows √ó 41 columns</p>\n","</div>"],"text/plain":["        duration  protocol_type  ...  dst_host_rerror_rate  dst_host_srv_rerror_rate\n","119981         0              1  ...                   0.0                        21\n","71403       8855              1  ...                   1.0                        15\n","61697          0              1  ...                   0.0                        21\n","15939          0              1  ...                   0.0                        21\n","140372         0              2  ...                   0.0                         5\n","...          ...            ...  ...                   ...                       ...\n","70332          0              1  ...                   0.0                        21\n","749            0              1  ...                   0.0                        21\n","73724          0              1  ...                   0.0                        18\n","14423          0              1  ...                   0.0                        20\n","29253      14479              1  ...                   0.0                        16\n","\n","[71758 rows x 41 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"y5Q6p_b8SUIw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621269516231,"user_tz":-120,"elapsed":1716,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"41711bdd-5c2b-4103-95d3-80167824d82d"},"source":["#### Fase di Categorical Embeddings ###############\n","\n","for col in cat_names:\n","  train[col] = train[col].astype('category')\n","\n","embedded_cols = {n: len(col.cat.categories) for n,col in train[cat_names].items()}\n","print(embedded_cols)\n","\n","embedded_col_names = cat_names\n","\n","# Determiniamo una funzione per la dimensione dell'incorporamento, presa da una libreria \n","embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n","embedding_sizes"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'protocol_type': 3, 'service': 68, 'flag': 11}\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[(3, 2), (68, 34), (11, 6)]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"9mdA6mEJZkEq"},"source":["\"\"\" Pytorch Dataset e DataLoader\n","Estendiamo la Dataset classe (astratta) fornita da Pytorch per un accesso pi√π facile al nostro set di dati durante l'addestramento \n","e per utilizzare efficacemente  il DataLoader modulo per gestire i batch. Ci√≤ comporta la sovrascrittura dei metodi __len__e __getitem__\n","secondo il nostro particolare set di dati.\n","Poich√© abbiamo solo bisogno di incorporare colonne categoriali, dividiamo il nostro input in due parti: numerico e categoriale. \"\"\" \n","\n","class NSLKDD_Dataset(Dataset):\n","    def __init__(self, X, Y, embedded_col_names):\n","        X = X.copy()\n","        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n","        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X1[idx], self.X2[idx], self.y[idx]\n","        \n","#creating train and valid datasets\n","train_ds = NSLKDD_Dataset(train, y_train, embedded_col_names)\n","valid_ds = NSLKDD_Dataset(validation, y_val, embedded_col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AY1gQHsoPAm9","executionInfo":{"status":"ok","timestamp":1621269534161,"user_tz":-120,"elapsed":1486,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"febebc20-85ab-4166-9110-da089207f116"},"source":["train_ds.X2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.0000e+00, 1.0410e+03, 3.3000e+02, ..., 0.0000e+00, 0.0000e+00,\n","        2.1000e+01],\n","       [8.8550e+03, 1.0000e+00, 0.0000e+00, ..., 1.0000e+00, 1.0000e+00,\n","        1.5000e+01],\n","       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n","        2.1000e+01],\n","       ...,\n","       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n","        1.8000e+01],\n","       [0.0000e+00, 1.8740e+03, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n","        2.0000e+01],\n","       [1.4479e+04, 8.4120e+03, 1.2031e+04, ..., 0.0000e+00, 0.0000e+00,\n","        1.6000e+01]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR0qF0wEZ-hQ","executionInfo":{"status":"ok","timestamp":1621269537216,"user_tz":-120,"elapsed":1509,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"5157926f-0278-4c82-e998-0cbf9baae144"},"source":["\"\"\" Making device (GPU/CPU) compatible\n","\n","(borrowed from https://jovian.ml/aakashns/04-feedforward-nn)\n","\n","In order to make use of a GPU if available, we'll have to move our data and model to it. \"\"\" \n","\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"wZr4ZQflaCAw"},"source":["\"\"\" I nostri dati sono suddivisi in parti continue e categoriali. Per prima cosa convertiamo le parti categoriali in vettori \n","incorporanti in base alle dimensioni determinate in precedenza e le concateniamo con le parti continue per alimentare il resto della rete \"\"\" \n","\n","class NSLKDDModel(nn.Module):\n","    def __init__(self, embedding_sizes, n_cont):\n","        super().__init__()\n","        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n","        n_emb = sum(e.embedding_dim for e in self.embeddings)  #length of all embeddings combined\n","        self.n_emb, self.n_cont = n_emb, n_cont\n","        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n","        self.lin2 = nn.Linear(200, 70)\n","        self.lin3 = nn.Linear(70, 5)\n","        self.bn1 = nn.BatchNorm1d(self.n_cont)\n","        self.bn2 = nn.BatchNorm1d(200)\n","        self.bn3 = nn.BatchNorm1d(70)\n","        self.emb_drop = nn.Dropout(0.6)\n","        self.drops = nn.Dropout(0.3)\n","        \n","\n","    def forward(self, x_cat, x_cont):\n","        #x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n","        x = [e(x_cat[:,0]) for i,e in enumerate(self.embeddings)]\n","        x = torch.cat(x, 1)\n","        x = self.emb_drop(x)\n","        x2 = self.bn1(x_cont)\n","        x = torch.cat([x, x2], 1)\n","        x = F.relu(self.lin1(x))\n","        x = self.drops(x)\n","        x = self.bn2(x)\n","        x = F.relu(self.lin2(x))\n","        x = self.drops(x)\n","        x = self.bn3(x)\n","        x = self.lin3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQc6Y5qBapPL"},"source":["\"\"\" Fase di preparazione per l'addestramento \"\"\"\n","\n","# Optimizer\n","def get_optimizer(model, lr = 0.001, wd = 0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim\n","\n","# Training function\n","def train_model(model, optim, train_dl):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x1, x2, y in train_dl:\n","        batch = y.shape[0]\n","        output = model(x1, x2)\n","        loss = F.cross_entropy(output, y)   \n","        optim.zero_grad()\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","# Evaluation function\n","def val_loss(model, valid_dl):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x1, x2, y in valid_dl:\n","        current_batch_size = y.shape[0]\n","        out = model(x1, x2)\n","        loss = F.cross_entropy(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.max(out, 1)[1]\n","        correct += (pred == y).float().sum().item()\n","    #print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n","    print('valid loss ', sum_loss/total, ' and accuracy ', correct/total)\n","    return sum_loss/total, correct/total\n","\n","# Funzione per l'addestramento \n","def train_loop(model, epochs, lr=0.01, wd=0.0):\n","    optim = get_optimizer(model, lr = lr, wd = wd)\n","    for i in range(epochs): \n","        loss = train_model(model, optim, train_dl)\n","        print('ep ', i, \"training loss: \", loss)\n","        val_loss(model, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WY0Fao3aO-e","executionInfo":{"status":"ok","timestamp":1621271109615,"user_tz":-120,"elapsed":475,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"8ae58c75-c319-4f51-b27e-98da84bb7abd"},"source":["model = NSLKDDModel(embedding_sizes, len(cont_names))\n","to_device(model, device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NSLKDDModel(\n","  (embeddings): ModuleList(\n","    (0): Embedding(3, 2)\n","    (1): Embedding(68, 34)\n","    (2): Embedding(11, 6)\n","  )\n","  (lin1): Linear(in_features=80, out_features=200, bias=True)\n","  (lin2): Linear(in_features=200, out_features=70, bias=True)\n","  (lin3): Linear(in_features=70, out_features=5, bias=True)\n","  (bn1): BatchNorm1d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn3): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (emb_drop): Dropout(p=0.6, inplace=False)\n","  (drops): Dropout(p=0.3, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"qNNiIdczasqp"},"source":["\"\"\" Ora addestriamo il modello sul set di addestramento. Ho usato l'ottimizzatore Adam per ottimizzare la perdita di entropia incrociata. \n","L'addestramento √® piuttosto semplice: iterare attraverso ogni batch, eseguire un passaggio in avanti, calcolare i gradienti, \n","eseguire una discesa del gradiente e ripetere questo processo per tutte le epoche necessarie. \"\"\" \n","\n","batch_size = 512\n","train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b976OcWbQLBe","executionInfo":{"status":"ok","timestamp":1621271604867,"user_tz":-120,"elapsed":491689,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"d44c3e1b-c885-4eca-8363-87a6d0ddd037"},"source":["train_loop(model, epochs=500, lr=0.001, wd=0.002)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ep  0 training loss:  0.3509288188815034\n","valid loss  0.1141541720867157  and accuracy  0.9656\n","ep  1 training loss:  0.10777016501668046\n","valid loss  0.06726433721780777  and accuracy  0.9784\n","ep  2 training loss:  0.07812237313655332\n","valid loss  0.05598678565621376  and accuracy  0.9804\n","ep  3 training loss:  0.06272547082011608\n","valid loss  0.06337571187615394  and accuracy  0.9864\n","ep  4 training loss:  0.054448839710100914\n","valid loss  0.05552811758518219  and accuracy  0.9908\n","ep  5 training loss:  0.05017270808876202\n","valid loss  0.03523318614959717  and accuracy  0.9928\n","ep  6 training loss:  0.045880192583584396\n","valid loss  0.03574569497704506  and accuracy  0.9892\n","ep  7 training loss:  0.043398673253391855\n","valid loss  0.03127266548871994  and accuracy  0.99\n","ep  8 training loss:  0.042903887095145414\n","valid loss  0.03444221029877663  and accuracy  0.9884\n","ep  9 training loss:  0.040490738830835635\n","valid loss  0.03441407506465912  and accuracy  0.9924\n","ep  10 training loss:  0.03882005995024177\n","valid loss  0.02778596634119749  and accuracy  0.9928\n","ep  11 training loss:  0.03794303482984559\n","valid loss  0.028274715703725816  and accuracy  0.9932\n","ep  12 training loss:  0.03685814308540894\n","valid loss  0.033241611224412916  and accuracy  0.988\n","ep  13 training loss:  0.03646663291695814\n","valid loss  0.03373631134927273  and accuracy  0.992\n","ep  14 training loss:  0.03672433328103081\n","valid loss  0.04153847774863243  and accuracy  0.9884\n","ep  15 training loss:  0.03543424891209987\n","valid loss  0.02872246699333191  and accuracy  0.9916\n","ep  16 training loss:  0.035983168218850614\n","valid loss  0.036709365847706794  and accuracy  0.988\n","ep  17 training loss:  0.033165631327741654\n","valid loss  0.05490748977661133  and accuracy  0.994\n","ep  18 training loss:  0.03307641728428741\n","valid loss  0.029464986246824264  and accuracy  0.9924\n","ep  19 training loss:  0.03322750420614264\n","valid loss  0.037093925297260286  and accuracy  0.9928\n","ep  20 training loss:  0.03341056556884267\n","valid loss  0.02716004920899868  and accuracy  0.9928\n","ep  21 training loss:  0.033424064693375194\n","valid loss  0.025721394461393356  and accuracy  0.9924\n","ep  22 training loss:  0.03431526253150922\n","valid loss  0.028730976833403112  and accuracy  0.9916\n","ep  23 training loss:  0.034224667287176214\n","valid loss  0.023565359571576117  and accuracy  0.9932\n","ep  24 training loss:  0.03117991194037932\n","valid loss  0.027122472327947615  and accuracy  0.99\n","ep  25 training loss:  0.03207377550626729\n","valid loss  0.024044058841466903  and accuracy  0.9944\n","ep  26 training loss:  0.03236461453059418\n","valid loss  0.027699785733222963  and accuracy  0.9932\n","ep  27 training loss:  0.032741778818955604\n","valid loss  0.03222401576340198  and accuracy  0.9936\n","ep  28 training loss:  0.03229894321002241\n","valid loss  0.02919553677737713  and accuracy  0.9932\n","ep  29 training loss:  0.03311951720560736\n","valid loss  0.02790176251530647  and accuracy  0.9932\n","ep  30 training loss:  0.03187632775739628\n","valid loss  0.025451631098985673  and accuracy  0.9928\n","ep  31 training loss:  0.032003209250493075\n","valid loss  0.03625461484193802  and accuracy  0.9932\n","ep  32 training loss:  0.03132299348294989\n","valid loss  0.025796550658345224  and accuracy  0.9936\n","ep  33 training loss:  0.03155676289903217\n","valid loss  0.04083468005657196  and accuracy  0.9924\n","ep  34 training loss:  0.03306591064052472\n","valid loss  0.02200231614410877  and accuracy  0.9936\n","ep  35 training loss:  0.03164735538856704\n","valid loss  0.02472108807861805  and accuracy  0.9936\n","ep  36 training loss:  0.031038765435578774\n","valid loss  0.049838562843203545  and accuracy  0.9928\n","ep  37 training loss:  0.03039935716797521\n","valid loss  0.029388446280360223  and accuracy  0.9924\n","ep  38 training loss:  0.03124933805223344\n","valid loss  0.024575181171298026  and accuracy  0.9944\n","ep  39 training loss:  0.030676256714113528\n","valid loss  0.06692460188865662  and accuracy  0.992\n","ep  40 training loss:  0.031138822825900116\n","valid loss  0.02256893921792507  and accuracy  0.9944\n","ep  41 training loss:  0.029395952379088187\n","valid loss  0.023893923965096472  and accuracy  0.9936\n","ep  42 training loss:  0.03060430423770482\n","valid loss  0.023846613539755344  and accuracy  0.9924\n","ep  43 training loss:  0.03231667638849911\n","valid loss  0.04832405853271484  and accuracy  0.9912\n","ep  44 training loss:  0.030585453276186805\n","valid loss  0.035821790128946304  and accuracy  0.9924\n","ep  45 training loss:  0.03144190809395495\n","valid loss  0.035123484948277474  and accuracy  0.9924\n","ep  46 training loss:  0.02946168682235181\n","valid loss  0.022545662099123003  and accuracy  0.9936\n","ep  47 training loss:  0.029828936507820886\n","valid loss  0.0245838847219944  and accuracy  0.9936\n","ep  48 training loss:  0.03328921604145571\n","valid loss  0.024991231353580952  and accuracy  0.9924\n","ep  49 training loss:  0.03012423591165028\n","valid loss  0.05321363974511623  and accuracy  0.9924\n","ep  50 training loss:  0.030578616106444377\n","valid loss  0.021888460044562817  and accuracy  0.9944\n","ep  51 training loss:  0.02949803576333131\n","valid loss  0.03156268167495727  and accuracy  0.9916\n","ep  52 training loss:  0.03020764863218569\n","valid loss  0.027083278107643126  and accuracy  0.9932\n","ep  53 training loss:  0.029088769524723872\n","valid loss  0.021256505864858628  and accuracy  0.9944\n","ep  54 training loss:  0.029665320869756883\n","valid loss  0.04581564874053001  and accuracy  0.9924\n","ep  55 training loss:  0.0305153547520555\n","valid loss  0.02150199380517006  and accuracy  0.994\n","ep  56 training loss:  0.031918404443566556\n","valid loss  0.047004347637295725  and accuracy  0.9944\n","ep  57 training loss:  0.030428835327445974\n","valid loss  0.0642690601348877  and accuracy  0.9944\n","ep  58 training loss:  0.030207513410813694\n","valid loss  0.059526565635204316  and accuracy  0.9944\n","ep  59 training loss:  0.029902756852068866\n","valid loss  0.029865702490508555  and accuracy  0.9948\n","ep  60 training loss:  0.03002181517282601\n","valid loss  0.021607713055610656  and accuracy  0.9948\n","ep  61 training loss:  0.030755378748237677\n","valid loss  0.027782162088155746  and accuracy  0.99\n","ep  62 training loss:  0.031208102686054024\n","valid loss  0.028289799251407385  and accuracy  0.9936\n","ep  63 training loss:  0.029005347762839653\n","valid loss  0.03410228553712368  and accuracy  0.9932\n","ep  64 training loss:  0.02996642364700336\n","valid loss  0.03236346840262413  and accuracy  0.994\n","ep  65 training loss:  0.03046790880110522\n","valid loss  0.03874682022333145  and accuracy  0.9936\n","ep  66 training loss:  0.030226465645011424\n","valid loss  0.021879282020032406  and accuracy  0.9944\n","ep  67 training loss:  0.02989575975078275\n","valid loss  0.025856618452072145  and accuracy  0.9936\n","ep  68 training loss:  0.030171594019133444\n","valid loss  0.04145475780665874  and accuracy  0.9932\n","ep  69 training loss:  0.028933436494372442\n","valid loss  0.028165483617782594  and accuracy  0.9944\n","ep  70 training loss:  0.03087605538195603\n","valid loss  0.01992617831528187  and accuracy  0.9944\n","ep  71 training loss:  0.02905205928789918\n","valid loss  0.020742739194631578  and accuracy  0.9944\n","ep  72 training loss:  0.03135331381806851\n","valid loss  0.03017124031186104  and accuracy  0.9932\n","ep  73 training loss:  0.030730237181809353\n","valid loss  0.05102101462483406  and accuracy  0.9908\n","ep  74 training loss:  0.030587632532386916\n","valid loss  0.06942909596860408  and accuracy  0.9928\n","ep  75 training loss:  0.030684239605847714\n","valid loss  0.02032758705317974  and accuracy  0.9956\n","ep  76 training loss:  0.030012647065872974\n","valid loss  0.03212055299282074  and accuracy  0.9944\n","ep  77 training loss:  0.029755245879501808\n","valid loss  0.02200399815738201  and accuracy  0.9944\n","ep  78 training loss:  0.029566693424417666\n","valid loss  0.025736776119470596  and accuracy  0.9928\n","ep  79 training loss:  0.03072356680849031\n","valid loss  0.04049980581998825  and accuracy  0.99\n","ep  80 training loss:  0.03270977647223\n","valid loss  0.024155219888687133  and accuracy  0.9936\n","ep  81 training loss:  0.029905318054094267\n","valid loss  0.02576063796877861  and accuracy  0.9936\n","ep  82 training loss:  0.029440649034130278\n","valid loss  0.021469883072376253  and accuracy  0.994\n","ep  83 training loss:  0.028446966423118735\n","valid loss  0.022446510742604734  and accuracy  0.9936\n","ep  84 training loss:  0.030780623122435482\n","valid loss  0.025559641915559768  and accuracy  0.9936\n","ep  85 training loss:  0.03003196528157501\n","valid loss  0.025252300721406936  and accuracy  0.9916\n","ep  86 training loss:  0.03058569749498029\n","valid loss  0.02344554974883795  and accuracy  0.994\n","ep  87 training loss:  0.029429475948299255\n","valid loss  0.03124042946100235  and accuracy  0.9932\n","ep  88 training loss:  0.029564382945881724\n","valid loss  0.02217283434420824  and accuracy  0.9936\n","ep  89 training loss:  0.031154693566392923\n","valid loss  0.022842779007554055  and accuracy  0.994\n","ep  90 training loss:  0.0318493243206057\n","valid loss  0.03140882958173752  and accuracy  0.9932\n","ep  91 training loss:  0.031783454618228145\n","valid loss  0.022847602090239526  and accuracy  0.9936\n","ep  92 training loss:  0.030992506757457905\n","valid loss  0.023602067142724992  and accuracy  0.9932\n","ep  93 training loss:  0.030446233816820528\n","valid loss  0.03573128132969141  and accuracy  0.9944\n","ep  94 training loss:  0.031319602257480095\n","valid loss  0.02612313592135906  and accuracy  0.9928\n","ep  95 training loss:  0.02965759447522412\n","valid loss  0.05335277299880981  and accuracy  0.992\n","ep  96 training loss:  0.030072518184674338\n","valid loss  0.024969792744517327  and accuracy  0.994\n","ep  97 training loss:  0.03096689931916354\n","valid loss  0.03279351896047592  and accuracy  0.99\n","ep  98 training loss:  0.02963607052950633\n","valid loss  0.024150752165913583  and accuracy  0.994\n","ep  99 training loss:  0.03180450317707494\n","valid loss  0.026119161713123323  and accuracy  0.9924\n","ep  100 training loss:  0.030467967812469555\n","valid loss  0.02252488104104996  and accuracy  0.9936\n","ep  101 training loss:  0.030119699434742967\n","valid loss  0.022479215347766875  and accuracy  0.9944\n","ep  102 training loss:  0.028914858662544122\n","valid loss  0.031840508787333965  and accuracy  0.9912\n","ep  103 training loss:  0.03029127555094308\n","valid loss  0.03438463604450226  and accuracy  0.9928\n","ep  104 training loss:  0.03085049216278171\n","valid loss  0.026382121920585633  and accuracy  0.994\n","ep  105 training loss:  0.02959140660529113\n","valid loss  0.024342754966020584  and accuracy  0.9912\n","ep  106 training loss:  0.028482027612623438\n","valid loss  0.020518322157859804  and accuracy  0.9956\n","ep  107 training loss:  0.031121044582068887\n","valid loss  0.02089567673802376  and accuracy  0.9936\n","ep  108 training loss:  0.028527062723356054\n","valid loss  0.03734513659477234  and accuracy  0.9932\n","ep  109 training loss:  0.03171844505977621\n","valid loss  0.03877480449676514  and accuracy  0.9928\n","ep  110 training loss:  0.03046472424309465\n","valid loss  0.0418861862719059  and accuracy  0.9924\n","ep  111 training loss:  0.029369075036090587\n","valid loss  0.06540455462038516  and accuracy  0.9912\n","ep  112 training loss:  0.028962656461031973\n","valid loss  0.020569905459880827  and accuracy  0.9936\n","ep  113 training loss:  0.03053881618949614\n","valid loss  0.029123739379644395  and accuracy  0.9924\n","ep  114 training loss:  0.03176073253753019\n","valid loss  0.03526970369815827  and accuracy  0.9928\n","ep  115 training loss:  0.030493478820136154\n","valid loss  0.04108028905689717  and accuracy  0.994\n","ep  116 training loss:  0.03142629791516408\n","valid loss  0.024543626552820205  and accuracy  0.9924\n","ep  117 training loss:  0.029772951856241826\n","valid loss  0.028298402872681618  and accuracy  0.9936\n","ep  118 training loss:  0.03008814208021999\n","valid loss  0.023929506987333298  and accuracy  0.9924\n","ep  119 training loss:  0.02971480572356606\n","valid loss  0.02801155003607273  and accuracy  0.9924\n","ep  120 training loss:  0.03047092939032676\n","valid loss  0.03583393206596375  and accuracy  0.9944\n","ep  121 training loss:  0.029894494778462923\n","valid loss  0.029191370302438735  and accuracy  0.9916\n","ep  122 training loss:  0.030934133443800433\n","valid loss  0.046747596833109854  and accuracy  0.992\n","ep  123 training loss:  0.030928259450829\n","valid loss  0.025968822148442268  and accuracy  0.994\n","ep  124 training loss:  0.030945078630358874\n","valid loss  0.029663850304484367  and accuracy  0.994\n","ep  125 training loss:  0.03072571649722984\n","valid loss  0.05742372343540192  and accuracy  0.9952\n","ep  126 training loss:  0.03027546284021271\n","valid loss  0.03452914254069328  and accuracy  0.9912\n","ep  127 training loss:  0.029003300605657797\n","valid loss  0.026853572767972945  and accuracy  0.9932\n","ep  128 training loss:  0.030776385447471556\n","valid loss  0.024480903860926627  and accuracy  0.9924\n","ep  129 training loss:  0.029025503642496373\n","valid loss  0.02322553682923317  and accuracy  0.992\n","ep  130 training loss:  0.030473225196048028\n","valid loss  0.03219714716672897  and accuracy  0.9908\n","ep  131 training loss:  0.03040828581260543\n","valid loss  0.028885889226198197  and accuracy  0.994\n","ep  132 training loss:  0.03032966292324086\n","valid loss  0.07578420796394349  and accuracy  0.9912\n","ep  133 training loss:  0.029672329798857866\n","valid loss  0.024815696173906325  and accuracy  0.9932\n","ep  134 training loss:  0.030448566122448974\n","valid loss  0.019511863979697227  and accuracy  0.9948\n","ep  135 training loss:  0.030516278689721527\n","valid loss  0.024524858969449996  and accuracy  0.994\n","ep  136 training loss:  0.029140943732091775\n","valid loss  0.02746973732113838  and accuracy  0.9932\n","ep  137 training loss:  0.03252202750974639\n","valid loss  0.05421245559155941  and accuracy  0.9932\n","ep  138 training loss:  0.029482901416449627\n","valid loss  0.025445685729384424  and accuracy  0.9924\n","ep  139 training loss:  0.03160113028125183\n","valid loss  0.05661453766822815  and accuracy  0.992\n","ep  140 training loss:  0.029698877392145214\n","valid loss  0.025760077995061876  and accuracy  0.9928\n","ep  141 training loss:  0.029506600594048936\n","valid loss  0.03558268837928772  and accuracy  0.9924\n","ep  142 training loss:  0.03042101290184642\n","valid loss  0.02314673892259598  and accuracy  0.9944\n","ep  143 training loss:  0.030446384638384262\n","valid loss  0.030038755995035173  and accuracy  0.9932\n","ep  144 training loss:  0.03092788200745608\n","valid loss  0.03144752191603184  and accuracy  0.9936\n","ep  145 training loss:  0.031087412637996616\n","valid loss  0.023206191375851632  and accuracy  0.9916\n","ep  146 training loss:  0.030872469120293067\n","valid loss  0.04601646179258823  and accuracy  0.9948\n","ep  147 training loss:  0.03048913845088814\n","valid loss  0.06702354574799538  and accuracy  0.9908\n","ep  148 training loss:  0.03043973573228548\n","valid loss  0.03250101289153099  and accuracy  0.996\n","ep  149 training loss:  0.030837385354853225\n","valid loss  0.025326120314002037  and accuracy  0.9936\n","ep  150 training loss:  0.02872944899817483\n","valid loss  0.13795991501808166  and accuracy  0.9944\n","ep  151 training loss:  0.03143709420961807\n","valid loss  0.027762394163012506  and accuracy  0.992\n","ep  152 training loss:  0.03221621196701969\n","valid loss  0.04115177024900913  and accuracy  0.9924\n","ep  153 training loss:  0.029689224383199427\n","valid loss  0.028271452245116235  and accuracy  0.992\n","ep  154 training loss:  0.03102124580657284\n","valid loss  0.03335660972595215  and accuracy  0.9928\n","ep  155 training loss:  0.03165233473644504\n","valid loss  0.0477630508005619  and accuracy  0.9904\n","ep  156 training loss:  0.029310509363244006\n","valid loss  0.10272173761725426  and accuracy  0.9916\n","ep  157 training loss:  0.02934678233888952\n","valid loss  0.03746033013164997  and accuracy  0.9928\n","ep  158 training loss:  0.03044054860331654\n","valid loss  0.06848055410385132  and accuracy  0.9944\n","ep  159 training loss:  0.030000819121893542\n","valid loss  0.03222428911849856  and accuracy  0.9936\n","ep  160 training loss:  0.029634048515568043\n","valid loss  0.020422223982214927  and accuracy  0.9952\n","ep  161 training loss:  0.02982223667782709\n","valid loss  0.02205165573358536  and accuracy  0.9936\n","ep  162 training loss:  0.030339796581903646\n","valid loss  0.021213669419288636  and accuracy  0.9944\n","ep  163 training loss:  0.030424446411396158\n","valid loss  0.060474909475445746  and accuracy  0.9936\n","ep  164 training loss:  0.030789120911114052\n","valid loss  0.021312994080781935  and accuracy  0.994\n","ep  165 training loss:  0.02880701631245408\n","valid loss  0.0748594477891922  and accuracy  0.9932\n","ep  166 training loss:  0.029782025606115446\n","valid loss  0.021049817878007887  and accuracy  0.9932\n","ep  167 training loss:  0.03111204868410705\n","valid loss  0.02996713148355484  and accuracy  0.994\n","ep  168 training loss:  0.03318973089526759\n","valid loss  0.052807162353396414  and accuracy  0.9948\n","ep  169 training loss:  0.03096230225042873\n","valid loss  0.020367998239398003  and accuracy  0.9944\n","ep  170 training loss:  0.031905623371153095\n","valid loss  0.1195259113073349  and accuracy  0.9944\n","ep  171 training loss:  0.030715788788825613\n","valid loss  0.027670232111215592  and accuracy  0.9944\n","ep  172 training loss:  0.029900075583695226\n","valid loss  0.026343465799093246  and accuracy  0.9944\n","ep  173 training loss:  0.030373009657217058\n","valid loss  0.07419341232478618  and accuracy  0.9936\n","ep  174 training loss:  0.030985704460851644\n","valid loss  0.08370394968092441  and accuracy  0.9916\n","ep  175 training loss:  0.03054210642073036\n","valid loss  0.028476404872536658  and accuracy  0.9916\n","ep  176 training loss:  0.030349256920795493\n","valid loss  0.04498294610977173  and accuracy  0.9928\n","ep  177 training loss:  0.029706580330244692\n","valid loss  0.028242085164785387  and accuracy  0.9944\n","ep  178 training loss:  0.030930522397987426\n","valid loss  0.059186722162365916  and accuracy  0.994\n","ep  179 training loss:  0.03023456051954408\n","valid loss  0.0788943297252059  and accuracy  0.9912\n","ep  180 training loss:  0.030237419117508834\n","valid loss  0.028206706987321375  and accuracy  0.9924\n","ep  181 training loss:  0.029927620811936062\n","valid loss  0.033975642578303815  and accuracy  0.9944\n","ep  182 training loss:  0.030977352628712637\n","valid loss  0.05570070613026619  and accuracy  0.9952\n","ep  183 training loss:  0.029622677454163242\n","valid loss  0.04209429659843445  and accuracy  0.9916\n","ep  184 training loss:  0.02965674819722998\n","valid loss  0.024741968351602556  and accuracy  0.9928\n","ep  185 training loss:  0.03142199847507598\n","valid loss  0.05937962581813335  and accuracy  0.9928\n","ep  186 training loss:  0.03080647797930775\n","valid loss  0.028409861570596696  and accuracy  0.9916\n","ep  187 training loss:  0.030168629467359417\n","valid loss  0.026513492980599403  and accuracy  0.994\n","ep  188 training loss:  0.030912396949707734\n","valid loss  0.0237254756629467  and accuracy  0.994\n","ep  189 training loss:  0.03023030977652468\n","valid loss  0.04350810036659241  and accuracy  0.9932\n","ep  190 training loss:  0.02916602920298011\n","valid loss  0.030035738933086396  and accuracy  0.994\n","ep  191 training loss:  0.030577355138352177\n","valid loss  0.02281230393946171  and accuracy  0.9944\n","ep  192 training loss:  0.03006441396010846\n","valid loss  0.02353096812069416  and accuracy  0.994\n","ep  193 training loss:  0.031018840119609394\n","valid loss  0.02279786184132099  and accuracy  0.9944\n","ep  194 training loss:  0.029862193202302878\n","valid loss  0.02358438057899475  and accuracy  0.9944\n","ep  195 training loss:  0.030422874566172473\n","valid loss  0.027346126037836076  and accuracy  0.992\n","ep  196 training loss:  0.030587113974656164\n","valid loss  0.02851109350323677  and accuracy  0.9916\n","ep  197 training loss:  0.02905105157107187\n","valid loss  0.03689679067134857  and accuracy  0.9908\n","ep  198 training loss:  0.030055345326006695\n","valid loss  0.021854893410205842  and accuracy  0.994\n","ep  199 training loss:  0.030289919935983973\n","valid loss  0.08293046827316285  and accuracy  0.9916\n","ep  200 training loss:  0.030646160691770966\n","valid loss  0.03472552084624767  and accuracy  0.9908\n","ep  201 training loss:  0.029683776908348582\n","valid loss  0.019556464758515357  and accuracy  0.9952\n","ep  202 training loss:  0.030616968823232133\n","valid loss  0.036630516755580905  and accuracy  0.9928\n","ep  203 training loss:  0.03142942296337791\n","valid loss  0.04122417742908001  and accuracy  0.9904\n","ep  204 training loss:  0.030508671352731838\n","valid loss  0.02895550497472286  and accuracy  0.9928\n","ep  205 training loss:  0.03040716542503288\n","valid loss  0.02739936615228653  and accuracy  0.9896\n","ep  206 training loss:  0.029410971211029497\n","valid loss  0.028788553085923194  and accuracy  0.992\n","ep  207 training loss:  0.030332834992511354\n","valid loss  0.029493016976118086  and accuracy  0.9928\n","ep  208 training loss:  0.031202878000726002\n","valid loss  0.02885446847975254  and accuracy  0.9916\n","ep  209 training loss:  0.031759239229283795\n","valid loss  0.04149987151175737  and accuracy  0.992\n","ep  210 training loss:  0.031080920402579537\n","valid loss  0.023987194040417673  and accuracy  0.9936\n","ep  211 training loss:  0.03101928225941438\n","valid loss  0.023814926290512083  and accuracy  0.9952\n","ep  212 training loss:  0.02998142822534066\n","valid loss  0.03195666297078133  and accuracy  0.9928\n","ep  213 training loss:  0.029683873918784937\n","valid loss  0.05954853971004486  and accuracy  0.994\n","ep  214 training loss:  0.031300753019657714\n","valid loss  0.0498713871717453  and accuracy  0.9928\n","ep  215 training loss:  0.0295333481688396\n","valid loss  0.02733464378416538  and accuracy  0.9944\n","ep  216 training loss:  0.03116974476510975\n","valid loss  0.11395941878259182  and accuracy  0.994\n","ep  217 training loss:  0.030344361290923327\n","valid loss  0.022946251720190047  and accuracy  0.9944\n","ep  218 training loss:  0.030089284066097247\n","valid loss  0.07881073700934649  and accuracy  0.9944\n","ep  219 training loss:  0.029321508691859656\n","valid loss  0.01847008928656578  and accuracy  0.9952\n","ep  220 training loss:  0.028808002586331222\n","valid loss  0.05282744508087635  and accuracy  0.994\n","ep  221 training loss:  0.03135043524330072\n","valid loss  0.023378430637717246  and accuracy  0.9948\n","ep  222 training loss:  0.03147314868902907\n","valid loss  0.022826888114213944  and accuracy  0.9948\n","ep  223 training loss:  0.031067026867673875\n","valid loss  0.024215292620658874  and accuracy  0.9928\n","ep  224 training loss:  0.030877695014089934\n","valid loss  0.029426016655564307  and accuracy  0.9944\n","ep  225 training loss:  0.02986540238630556\n","valid loss  0.026134946474432947  and accuracy  0.9928\n","ep  226 training loss:  0.03077976211885544\n","valid loss  0.04924791216850281  and accuracy  0.9944\n","ep  227 training loss:  0.03124111313315225\n","valid loss  0.028497987481951714  and accuracy  0.994\n","ep  228 training loss:  0.031281214325828095\n","valid loss  0.033379600337147715  and accuracy  0.994\n","ep  229 training loss:  0.02948928225256276\n","valid loss  0.026725924859941005  and accuracy  0.9936\n","ep  230 training loss:  0.02966171608038406\n","valid loss  0.024828502345085143  and accuracy  0.9928\n","ep  231 training loss:  0.03123718714835489\n","valid loss  0.026988831725716592  and accuracy  0.9932\n","ep  232 training loss:  0.030118653994354604\n","valid loss  0.02944888803958893  and accuracy  0.9912\n","ep  233 training loss:  0.029977343836483882\n","valid loss  0.05276427068710327  and accuracy  0.9932\n","ep  234 training loss:  0.03067853834970952\n","valid loss  0.028052479626238348  and accuracy  0.9944\n","ep  235 training loss:  0.02877743973501145\n","valid loss  0.03859389931857586  and accuracy  0.994\n","ep  236 training loss:  0.02924382480384107\n","valid loss  0.025502129292488098  and accuracy  0.9936\n","ep  237 training loss:  0.031006399376096234\n","valid loss  0.024652530544996262  and accuracy  0.994\n","ep  238 training loss:  0.02968042371930338\n","valid loss  0.021029843682050704  and accuracy  0.9944\n","ep  239 training loss:  0.031433682181045974\n","valid loss  0.018807782900333404  and accuracy  0.9952\n","ep  240 training loss:  0.03059894968864694\n","valid loss  0.029567861878871917  and accuracy  0.994\n","ep  241 training loss:  0.030528889774882464\n","valid loss  0.024711795654892923  and accuracy  0.9928\n","ep  242 training loss:  0.029852381943458184\n","valid loss  0.02966860587000847  and accuracy  0.9924\n","ep  243 training loss:  0.02996848938557481\n","valid loss  0.0653526733726263  and accuracy  0.9944\n","ep  244 training loss:  0.030206108347996443\n","valid loss  0.04168766162395477  and accuracy  0.9944\n","ep  245 training loss:  0.030913316436887946\n","valid loss  0.021760207587480544  and accuracy  0.9936\n","ep  246 training loss:  0.031956365005931596\n","valid loss  0.03444288856834173  and accuracy  0.994\n","ep  247 training loss:  0.0303099030841287\n","valid loss  0.036874638682603836  and accuracy  0.992\n","ep  248 training loss:  0.03152910656226125\n","valid loss  0.04279029407054186  and accuracy  0.9924\n","ep  249 training loss:  0.030352313627469107\n","valid loss  0.03291274057030678  and accuracy  0.9948\n","ep  250 training loss:  0.029952573978795253\n","valid loss  0.03442569752931595  and accuracy  0.9936\n","ep  251 training loss:  0.030881901658431233\n","valid loss  0.02812926366329193  and accuracy  0.9944\n","ep  252 training loss:  0.030926804312505683\n","valid loss  0.03203620657622814  and accuracy  0.9936\n","ep  253 training loss:  0.031209460576016215\n","valid loss  0.020803341092914342  and accuracy  0.9936\n","ep  254 training loss:  0.031336615521348324\n","valid loss  0.027906031200289727  and accuracy  0.9936\n","ep  255 training loss:  0.031877213577936415\n","valid loss  0.03207479045689106  and accuracy  0.9936\n","ep  256 training loss:  0.02895928792581334\n","valid loss  0.03372772607803345  and accuracy  0.994\n","ep  257 training loss:  0.02946018582155071\n","valid loss  0.030403642082214355  and accuracy  0.99\n","ep  258 training loss:  0.030702774098768603\n","valid loss  0.027977352982759475  and accuracy  0.9948\n","ep  259 training loss:  0.028877086856544523\n","valid loss  0.022619727432727815  and accuracy  0.9944\n","ep  260 training loss:  0.030368813037383903\n","valid loss  0.02524466091096401  and accuracy  0.9936\n","ep  261 training loss:  0.03043621230477284\n","valid loss  0.044827891743183135  and accuracy  0.9936\n","ep  262 training loss:  0.0297301731206903\n","valid loss  0.028283072066307067  and accuracy  0.9936\n","ep  263 training loss:  0.03269124506833365\n","valid loss  0.03114425836354494  and accuracy  0.9944\n","ep  264 training loss:  0.03169526331060862\n","valid loss  0.02504489277303219  and accuracy  0.9936\n","ep  265 training loss:  0.030687113667053404\n","valid loss  0.02148888832628727  and accuracy  0.9936\n","ep  266 training loss:  0.028456435211636395\n","valid loss  0.021225844031572343  and accuracy  0.9948\n","ep  267 training loss:  0.03104755310394409\n","valid loss  0.022855645924806593  and accuracy  0.994\n","ep  268 training loss:  0.03103249190415858\n","valid loss  0.025641547080874443  and accuracy  0.9944\n","ep  269 training loss:  0.03143978998195404\n","valid loss  0.025341294422745703  and accuracy  0.9936\n","ep  270 training loss:  0.030424784402035394\n","valid loss  0.025951907125115393  and accuracy  0.992\n","ep  271 training loss:  0.029899389864425467\n","valid loss  0.02061528694033623  and accuracy  0.9932\n","ep  272 training loss:  0.028858823426436007\n","valid loss  0.02088117399215698  and accuracy  0.9944\n","ep  273 training loss:  0.028709486036360598\n","valid loss  0.027135577642917633  and accuracy  0.9944\n","ep  274 training loss:  0.03080235680597373\n","valid loss  0.025648714107275008  and accuracy  0.9936\n","ep  275 training loss:  0.030153024138978746\n","valid loss  0.022802535873651504  and accuracy  0.9944\n","ep  276 training loss:  0.029968074020404804\n","valid loss  0.052760958081483844  and accuracy  0.992\n","ep  277 training loss:  0.029203414971035322\n","valid loss  0.022871458511054515  and accuracy  0.9932\n","ep  278 training loss:  0.02962049079156189\n","valid loss  0.023854488405585288  and accuracy  0.992\n","ep  279 training loss:  0.030523034120028644\n","valid loss  0.025508455854654312  and accuracy  0.9944\n","ep  280 training loss:  0.03230458527571674\n","valid loss  0.03143263349831104  and accuracy  0.9936\n","ep  281 training loss:  0.03044887162428735\n","valid loss  0.02611934127807617  and accuracy  0.9944\n","ep  282 training loss:  0.0317517357329831\n","valid loss  0.02273780471086502  and accuracy  0.9936\n","ep  283 training loss:  0.032129905762197035\n","valid loss  0.025873301285505294  and accuracy  0.9924\n","ep  284 training loss:  0.03144172803331666\n","valid loss  0.022049800771474837  and accuracy  0.9952\n","ep  285 training loss:  0.02974594642779823\n","valid loss  0.05167794616222381  and accuracy  0.9912\n","ep  286 training loss:  0.03166875623934067\n","valid loss  0.024292772459983826  and accuracy  0.9936\n","ep  287 training loss:  0.030772947206058682\n","valid loss  0.09725316406786441  and accuracy  0.994\n","ep  288 training loss:  0.030663780284091675\n","valid loss  0.018109211486577986  and accuracy  0.9952\n","ep  289 training loss:  0.029964709279678675\n","valid loss  0.03455195714533329  and accuracy  0.994\n","ep  290 training loss:  0.030609602302758485\n","valid loss  0.023874312883615495  and accuracy  0.9912\n","ep  291 training loss:  0.030925923910918862\n","valid loss  0.03258244061172009  and accuracy  0.9944\n","ep  292 training loss:  0.02996693596680149\n","valid loss  0.07104592858552933  and accuracy  0.9944\n","ep  293 training loss:  0.031032353822887278\n","valid loss  0.024036512860655786  and accuracy  0.9928\n","ep  294 training loss:  0.031001726646766342\n","valid loss  0.0763360361456871  and accuracy  0.9948\n","ep  295 training loss:  0.028971565599880636\n","valid loss  0.02360346414744854  and accuracy  0.9928\n","ep  296 training loss:  0.030901274936974386\n","valid loss  0.04494979271888733  and accuracy  0.992\n","ep  297 training loss:  0.02958873687406232\n","valid loss  0.024400432240962982  and accuracy  0.9936\n","ep  298 training loss:  0.032410927712305776\n","valid loss  0.02200917568206787  and accuracy  0.994\n","ep  299 training loss:  0.031025984744582125\n","valid loss  0.022506098255515097  and accuracy  0.992\n","ep  300 training loss:  0.030554208707748314\n","valid loss  0.018740091694891454  and accuracy  0.996\n","ep  301 training loss:  0.02961620548974943\n","valid loss  0.02827174050807953  and accuracy  0.9948\n","ep  302 training loss:  0.031047936334448908\n","valid loss  0.022711917001008986  and accuracy  0.9944\n","ep  303 training loss:  0.030908214266098473\n","valid loss  0.024041347390413283  and accuracy  0.9936\n","ep  304 training loss:  0.03078645872131645\n","valid loss  0.03517487604618073  and accuracy  0.9948\n","ep  305 training loss:  0.03174730823382163\n","valid loss  0.03273617217540741  and accuracy  0.9928\n","ep  306 training loss:  0.03129403411124362\n","valid loss  0.03155960867702961  and accuracy  0.994\n","ep  307 training loss:  0.030992596482817516\n","valid loss  0.04918599809706211  and accuracy  0.9948\n","ep  308 training loss:  0.030872320578508562\n","valid loss  0.05271720303297043  and accuracy  0.9916\n","ep  309 training loss:  0.03016500823309185\n","valid loss  0.025654673635959625  and accuracy  0.994\n","ep  310 training loss:  0.03255992036400762\n","valid loss  0.03197087922692299  and accuracy  0.9944\n","ep  311 training loss:  0.029545095681736324\n","valid loss  0.037703999871015546  and accuracy  0.9944\n","ep  312 training loss:  0.03217264384400876\n","valid loss  0.025600686728954315  and accuracy  0.9924\n","ep  313 training loss:  0.029846506663574888\n","valid loss  0.021486984838545324  and accuracy  0.994\n","ep  314 training loss:  0.03131837549155338\n","valid loss  0.02432402885556221  and accuracy  0.9924\n","ep  315 training loss:  0.030104122645549725\n","valid loss  0.024660255828499796  and accuracy  0.9932\n","ep  316 training loss:  0.03058454402875281\n","valid loss  0.030084975570440292  and accuracy  0.9932\n","ep  317 training loss:  0.030835454305606646\n","valid loss  0.023574233531951903  and accuracy  0.9952\n","ep  318 training loss:  0.03091933352860218\n","valid loss  0.039012221810221674  and accuracy  0.994\n","ep  319 training loss:  0.03188910964175462\n","valid loss  0.02241624096632004  and accuracy  0.9948\n","ep  320 training loss:  0.03079549607249047\n","valid loss  0.030688353925943376  and accuracy  0.9944\n","ep  321 training loss:  0.030743065687118275\n","valid loss  0.024768429923057556  and accuracy  0.9936\n","ep  322 training loss:  0.029239443669587216\n","valid loss  0.031551781855523583  and accuracy  0.9948\n","ep  323 training loss:  0.029654395459766735\n","valid loss  0.03468046400547028  and accuracy  0.9912\n","ep  324 training loss:  0.031407826593008\n","valid loss  0.04474017851948738  and accuracy  0.9944\n","ep  325 training loss:  0.03041006012474553\n","valid loss  0.029158045411109924  and accuracy  0.9932\n","ep  326 training loss:  0.029454675132222937\n","valid loss  0.033110735249519345  and accuracy  0.9916\n","ep  327 training loss:  0.031238778265848113\n","valid loss  0.023968947514891623  and accuracy  0.992\n","ep  328 training loss:  0.031013764141777388\n","valid loss  0.0683205527305603  and accuracy  0.9912\n","ep  329 training loss:  0.030236630184263546\n","valid loss  0.023056499636173247  and accuracy  0.9936\n","ep  330 training loss:  0.030474893120593153\n","valid loss  0.03205918572247028  and accuracy  0.9912\n","ep  331 training loss:  0.030450830976431902\n","valid loss  0.03002144687771797  and accuracy  0.9932\n","ep  332 training loss:  0.03157267428608758\n","valid loss  0.028825461715459823  and accuracy  0.9912\n","ep  333 training loss:  0.030212460350566173\n","valid loss  0.027546433559060095  and accuracy  0.994\n","ep  334 training loss:  0.03069431296152305\n","valid loss  0.029174779051542282  and accuracy  0.9916\n","ep  335 training loss:  0.029554849260411725\n","valid loss  0.03646387010216713  and accuracy  0.9932\n","ep  336 training loss:  0.029055184822137807\n","valid loss  0.04412967743873596  and accuracy  0.994\n","ep  337 training loss:  0.030427629600158254\n","valid loss  0.024016950130462646  and accuracy  0.9936\n","ep  338 training loss:  0.03240243297394939\n","valid loss  0.026555371737480165  and accuracy  0.9944\n","ep  339 training loss:  0.03146531694703698\n","valid loss  0.030345849207043647  and accuracy  0.992\n","ep  340 training loss:  0.031214834300678587\n","valid loss  0.04133501514941454  and accuracy  0.9952\n","ep  341 training loss:  0.031192880025942685\n","valid loss  0.04278491639196873  and accuracy  0.9916\n","ep  342 training loss:  0.03091141108676088\n","valid loss  0.04020758135318756  and accuracy  0.994\n","ep  343 training loss:  0.03120957280740791\n","valid loss  0.02661226214170456  and accuracy  0.9932\n","ep  344 training loss:  0.029847430284925053\n","valid loss  0.028158948934078217  and accuracy  0.994\n","ep  345 training loss:  0.03149830259254184\n","valid loss  0.023224416905641556  and accuracy  0.9948\n","ep  346 training loss:  0.030127611740849016\n","valid loss  0.027787609735131263  and accuracy  0.9928\n","ep  347 training loss:  0.031435427263845496\n","valid loss  0.04119337149858475  and accuracy  0.9932\n","ep  348 training loss:  0.030930798324058876\n","valid loss  0.06538934873938561  and accuracy  0.9944\n","ep  349 training loss:  0.031620670151830454\n","valid loss  0.026214814925193786  and accuracy  0.9932\n","ep  350 training loss:  0.030362946503610894\n","valid loss  0.09559244751632213  and accuracy  0.9924\n","ep  351 training loss:  0.031167591649199706\n","valid loss  0.021557133516669272  and accuracy  0.9936\n","ep  352 training loss:  0.03278994136815562\n","valid loss  0.06382138256430626  and accuracy  0.9952\n","ep  353 training loss:  0.030588749183870924\n","valid loss  0.045401039385795594  and accuracy  0.9944\n","ep  354 training loss:  0.030647245053442903\n","valid loss  0.033451772874593735  and accuracy  0.9936\n","ep  355 training loss:  0.030637699327022507\n","valid loss  0.020063909363746642  and accuracy  0.996\n","ep  356 training loss:  0.030544441442376707\n","valid loss  0.06755969880819321  and accuracy  0.9924\n","ep  357 training loss:  0.030334150536952632\n","valid loss  0.0515271208897233  and accuracy  0.994\n","ep  358 training loss:  0.03148655685024562\n","valid loss  0.02119745097756386  and accuracy  0.9924\n","ep  359 training loss:  0.03184273806894176\n","valid loss  0.05172932074069977  and accuracy  0.9936\n","ep  360 training loss:  0.031659391685258546\n","valid loss  0.022633206503093244  and accuracy  0.9948\n","ep  361 training loss:  0.031454257679628006\n","valid loss  0.06408351395726204  and accuracy  0.9932\n","ep  362 training loss:  0.03341056412950944\n","valid loss  0.024372807531058787  and accuracy  0.994\n","ep  363 training loss:  0.03137533617988505\n","valid loss  0.028563091012835502  and accuracy  0.9932\n","ep  364 training loss:  0.03413193208020651\n","valid loss  0.09020053578615189  and accuracy  0.99\n","ep  365 training loss:  0.030956138452112304\n","valid loss  0.04201217731833458  and accuracy  0.9932\n","ep  366 training loss:  0.030487261253002777\n","valid loss  0.021557334965467453  and accuracy  0.9944\n","ep  367 training loss:  0.02942209617311107\n","valid loss  0.03592111069560051  and accuracy  0.9936\n","ep  368 training loss:  0.030151082914178395\n","valid loss  0.07717485622167587  and accuracy  0.9932\n","ep  369 training loss:  0.030289155905721326\n","valid loss  0.03725880057811737  and accuracy  0.9952\n","ep  370 training loss:  0.030964373871810745\n","valid loss  0.02926784289777279  and accuracy  0.9932\n","ep  371 training loss:  0.030877751788582486\n","valid loss  0.02659661229029298  and accuracy  0.9944\n","ep  372 training loss:  0.030878906304809394\n","valid loss  0.038700939524173736  and accuracy  0.9948\n","ep  373 training loss:  0.030404997202665224\n","valid loss  0.03381098178029061  and accuracy  0.9928\n","ep  374 training loss:  0.03159343454397164\n","valid loss  0.11274544086456299  and accuracy  0.994\n","ep  375 training loss:  0.03206806607759764\n","valid loss  0.056531720039248466  and accuracy  0.9928\n","ep  376 training loss:  0.03135976277910805\n","valid loss  0.03661063566207886  and accuracy  0.994\n","ep  377 training loss:  0.031143170119547956\n","valid loss  0.026652260434627532  and accuracy  0.992\n","ep  378 training loss:  0.032600016678940205\n","valid loss  0.0242436980932951  and accuracy  0.9936\n","ep  379 training loss:  0.03075448533475404\n","valid loss  0.023345072555541992  and accuracy  0.9936\n","ep  380 training loss:  0.03140607533376873\n","valid loss  0.02434559210538864  and accuracy  0.994\n","ep  381 training loss:  0.030252088687780485\n","valid loss  0.025293309476971625  and accuracy  0.9936\n","ep  382 training loss:  0.030379007101007462\n","valid loss  0.043377340626716616  and accuracy  0.9928\n","ep  383 training loss:  0.03131730754565208\n","valid loss  0.02370213535577059  and accuracy  0.9932\n","ep  384 training loss:  0.031859193231559835\n","valid loss  0.027705386504530905  and accuracy  0.9932\n","ep  385 training loss:  0.030826147796679654\n","valid loss  0.024350026458501817  and accuracy  0.994\n","ep  386 training loss:  0.030892654554379794\n","valid loss  0.02209733341932297  and accuracy  0.9952\n","ep  387 training loss:  0.029187995166866893\n","valid loss  0.024756821972131728  and accuracy  0.994\n","ep  388 training loss:  0.03085534206299968\n","valid loss  0.020188139081001283  and accuracy  0.9932\n","ep  389 training loss:  0.030320388775404727\n","valid loss  0.027936466431617738  and accuracy  0.9928\n","ep  390 training loss:  0.030611172939980554\n","valid loss  0.02411685172021389  and accuracy  0.9932\n","ep  391 training loss:  0.03168037438696589\n","valid loss  0.021410634285211564  and accuracy  0.9952\n","ep  392 training loss:  0.03040448430321425\n","valid loss  0.022874946808815  and accuracy  0.9944\n","ep  393 training loss:  0.029832009926827482\n","valid loss  0.02008058493733406  and accuracy  0.994\n","ep  394 training loss:  0.031798917937450356\n","valid loss  0.031682230733335016  and accuracy  0.9944\n","ep  395 training loss:  0.029758260875830056\n","valid loss  0.041044815915822985  and accuracy  0.9908\n","ep  396 training loss:  0.03127580777891716\n","valid loss  0.056395852410793305  and accuracy  0.9944\n","ep  397 training loss:  0.03134562762646184\n","valid loss  0.022596834868192674  and accuracy  0.994\n","ep  398 training loss:  0.03255097902033114\n","valid loss  0.03941426613628864  and accuracy  0.9928\n","ep  399 training loss:  0.0296687913331294\n","valid loss  0.027577146029472352  and accuracy  0.9932\n","ep  400 training loss:  0.030512029529968676\n","valid loss  0.0313249639749527  and accuracy  0.9916\n","ep  401 training loss:  0.03115489741096798\n","valid loss  0.02779020675122738  and accuracy  0.9928\n","ep  402 training loss:  0.030217963307053487\n","valid loss  0.021842781427502633  and accuracy  0.9948\n","ep  403 training loss:  0.030912390327996184\n","valid loss  0.04102215488255024  and accuracy  0.9928\n","ep  404 training loss:  0.030175935242156553\n","valid loss  0.024702669118344785  and accuracy  0.9928\n","ep  405 training loss:  0.03133928092294858\n","valid loss  0.021053770789504052  and accuracy  0.994\n","ep  406 training loss:  0.030560587857206906\n","valid loss  0.03377682860195637  and accuracy  0.9936\n","ep  407 training loss:  0.03181566108083469\n","valid loss  0.025501069805026054  and accuracy  0.992\n","ep  408 training loss:  0.03246624476876653\n","valid loss  0.03775869590044022  and accuracy  0.9928\n","ep  409 training loss:  0.03214872485456726\n","valid loss  0.02001833924651146  and accuracy  0.9948\n","ep  410 training loss:  0.03092563763826088\n","valid loss  0.019965919390320778  and accuracy  0.9944\n","ep  411 training loss:  0.031674776095145754\n","valid loss  0.030053419607877732  and accuracy  0.9936\n","ep  412 training loss:  0.03161984051154201\n","valid loss  0.04179668514728546  and accuracy  0.9932\n","ep  413 training loss:  0.03200553398650067\n","valid loss  0.04852415766119957  and accuracy  0.9928\n","ep  414 training loss:  0.031606875782597174\n","valid loss  0.02078537859618664  and accuracy  0.994\n","ep  415 training loss:  0.03252907978725519\n","valid loss  0.025611866873502732  and accuracy  0.9936\n","ep  416 training loss:  0.03393188447137649\n","valid loss  0.02162098958492279  and accuracy  0.9948\n","ep  417 training loss:  0.03134835102670172\n","valid loss  0.028206423470377922  and accuracy  0.9928\n","ep  418 training loss:  0.031367622898988805\n","valid loss  0.025181507709622384  and accuracy  0.9932\n","ep  419 training loss:  0.03192475922523047\n","valid loss  0.031424359652400015  and accuracy  0.9932\n","ep  420 training loss:  0.029871878516362575\n","valid loss  0.03329463443160057  and accuracy  0.9924\n","ep  421 training loss:  0.031103980701354745\n","valid loss  0.06420834213197231  and accuracy  0.9948\n","ep  422 training loss:  0.03151763590455218\n","valid loss  0.028803579235076905  and accuracy  0.9936\n","ep  423 training loss:  0.032239153207680754\n","valid loss  0.02573561408519745  and accuracy  0.9932\n","ep  424 training loss:  0.031693515802980375\n","valid loss  0.035703971523046496  and accuracy  0.9936\n","ep  425 training loss:  0.030536788615967793\n","valid loss  0.041254742403328416  and accuracy  0.9916\n","ep  426 training loss:  0.0321184432665067\n","valid loss  0.029367199051380157  and accuracy  0.9944\n","ep  427 training loss:  0.031013480330080705\n","valid loss  0.025972215622663497  and accuracy  0.992\n","ep  428 training loss:  0.030737020647887695\n","valid loss  0.023109525698423384  and accuracy  0.994\n","ep  429 training loss:  0.031716017868162055\n","valid loss  0.0650894396275282  and accuracy  0.994\n","ep  430 training loss:  0.03211689363622975\n","valid loss  0.025623350924253465  and accuracy  0.994\n","ep  431 training loss:  0.03311756443623838\n","valid loss  0.04371483950316906  and accuracy  0.994\n","ep  432 training loss:  0.03178658434550793\n","valid loss  0.034177368196845054  and accuracy  0.9928\n","ep  433 training loss:  0.030024971272635977\n","valid loss  0.05654516666829586  and accuracy  0.994\n","ep  434 training loss:  0.03055353737449084\n","valid loss  0.028224727514386178  and accuracy  0.9932\n","ep  435 training loss:  0.030509699582744177\n","valid loss  0.02169781676530838  and accuracy  0.994\n","ep  436 training loss:  0.030570484124551413\n","valid loss  0.027556400755047797  and accuracy  0.9908\n","ep  437 training loss:  0.031156660479488583\n","valid loss  0.026774453768134115  and accuracy  0.9912\n","ep  438 training loss:  0.031218020762035375\n","valid loss  0.021499561822414397  and accuracy  0.9936\n","ep  439 training loss:  0.031124603209831802\n","valid loss  0.025112086623907088  and accuracy  0.9932\n","ep  440 training loss:  0.031205413929098272\n","valid loss  0.03418142437636852  and accuracy  0.9932\n","ep  441 training loss:  0.030551857657718326\n","valid loss  0.024012087073922158  and accuracy  0.9948\n","ep  442 training loss:  0.030165913323201964\n","valid loss  0.03958563141226769  and accuracy  0.9936\n","ep  443 training loss:  0.03356530611139067\n","valid loss  0.031600032269954684  and accuracy  0.9944\n","ep  444 training loss:  0.03410759519292661\n","valid loss  0.02252272830307484  and accuracy  0.9936\n","ep  445 training loss:  0.03193728233206305\n","valid loss  0.024760871094465255  and accuracy  0.9924\n","ep  446 training loss:  0.031115584383408505\n","valid loss  0.033194951766729354  and accuracy  0.9948\n","ep  447 training loss:  0.030759973918051985\n","valid loss  0.08178764328956604  and accuracy  0.9916\n","ep  448 training loss:  0.03182089867141388\n","valid loss  0.04920310794115067  and accuracy  0.9928\n","ep  449 training loss:  0.03157079562516996\n","valid loss  0.035316681551933286  and accuracy  0.9912\n","ep  450 training loss:  0.03176083803847513\n","valid loss  0.024170584839582444  and accuracy  0.9908\n","ep  451 training loss:  0.030007021105292117\n","valid loss  0.04073576700091362  and accuracy  0.9936\n","ep  452 training loss:  0.03180485993745006\n","valid loss  0.029925394982099535  and accuracy  0.9932\n","ep  453 training loss:  0.03140894206239579\n","valid loss  0.02490243522375822  and accuracy  0.9928\n","ep  454 training loss:  0.031930539998624025\n","valid loss  0.03577276009619236  and accuracy  0.994\n","ep  455 training loss:  0.031730231502989655\n","valid loss  0.020806199795007706  and accuracy  0.9924\n","ep  456 training loss:  0.02956931142250615\n","valid loss  0.026072029039263724  and accuracy  0.9892\n","ep  457 training loss:  0.0315132483118583\n","valid loss  0.024213754945993423  and accuracy  0.992\n","ep  458 training loss:  0.03072706759520048\n","valid loss  0.05955549693107605  and accuracy  0.994\n","ep  459 training loss:  0.031545057291004944\n","valid loss  0.034393729254603385  and accuracy  0.9876\n","ep  460 training loss:  0.03125909959103157\n","valid loss  0.03789709928035736  and accuracy  0.9932\n","ep  461 training loss:  0.03039355393049659\n","valid loss  0.02649423668384552  and accuracy  0.9928\n","ep  462 training loss:  0.032059484649667735\n","valid loss  0.026187917348742484  and accuracy  0.9916\n","ep  463 training loss:  0.03066221205698053\n","valid loss  0.024136126701533794  and accuracy  0.9932\n","ep  464 training loss:  0.030428873091967675\n","valid loss  0.0556991976827383  and accuracy  0.9924\n","ep  465 training loss:  0.030744215758907418\n","valid loss  0.031186821141839027  and accuracy  0.9936\n","ep  466 training loss:  0.03129957941958543\n","valid loss  0.024584387427568435  and accuracy  0.9936\n","ep  467 training loss:  0.03128284841645121\n","valid loss  0.02915767152309418  and accuracy  0.992\n","ep  468 training loss:  0.03165291546535028\n","valid loss  0.022946551966667176  and accuracy  0.9928\n","ep  469 training loss:  0.03275514748621826\n","valid loss  0.042235251887887716  and accuracy  0.9944\n","ep  470 training loss:  0.031468732229073225\n","valid loss  0.034064998266100883  and accuracy  0.9924\n","ep  471 training loss:  0.030193392140529948\n","valid loss  0.029571809512376784  and accuracy  0.9936\n","ep  472 training loss:  0.03288574586307685\n","valid loss  0.0290266999065876  and accuracy  0.9928\n","ep  473 training loss:  0.029880389198858125\n","valid loss  0.02430893121957779  and accuracy  0.9928\n","ep  474 training loss:  0.03084921395978382\n","valid loss  0.03853860833644867  and accuracy  0.992\n","ep  475 training loss:  0.03075577352677642\n","valid loss  0.028576823967695238  and accuracy  0.994\n","ep  476 training loss:  0.03203511329052583\n","valid loss  0.02403378560692072  and accuracy  0.9928\n","ep  477 training loss:  0.03131701911427979\n","valid loss  0.021526289635896683  and accuracy  0.9948\n","ep  478 training loss:  0.032301307879441285\n","valid loss  0.020832863262295723  and accuracy  0.9944\n","ep  479 training loss:  0.030611630756239877\n","valid loss  0.024224241039156914  and accuracy  0.9944\n","ep  480 training loss:  0.031657661319491985\n","valid loss  0.02294850069284439  and accuracy  0.9944\n","ep  481 training loss:  0.030728630926060804\n","valid loss  0.06458545892983675  and accuracy  0.9936\n","ep  482 training loss:  0.03059733976933459\n","valid loss  0.02563498826622963  and accuracy  0.9932\n","ep  483 training loss:  0.030248272275748545\n","valid loss  0.04335942004919052  and accuracy  0.9948\n","ep  484 training loss:  0.030362473279931846\n","valid loss  0.030949390310049056  and accuracy  0.994\n","ep  485 training loss:  0.031177169901643757\n","valid loss  0.02673243979215622  and accuracy  0.994\n","ep  486 training loss:  0.03309171888215701\n","valid loss  0.03335298502147198  and accuracy  0.994\n","ep  487 training loss:  0.03380141564801889\n","valid loss  0.04164525775909424  and accuracy  0.9924\n","ep  488 training loss:  0.031271693255743915\n","valid loss  0.06791441072821618  and accuracy  0.9932\n","ep  489 training loss:  0.03171883614356344\n","valid loss  0.021933959034085272  and accuracy  0.9944\n","ep  490 training loss:  0.03061330888016759\n","valid loss  0.03075360489487648  and accuracy  0.9944\n","ep  491 training loss:  0.031134050707535128\n","valid loss  0.04756104620099068  and accuracy  0.9924\n","ep  492 training loss:  0.02986216240428377\n","valid loss  0.024495001673698426  and accuracy  0.9928\n","ep  493 training loss:  0.033182235232651534\n","valid loss  0.0202593095600605  and accuracy  0.9948\n","ep  494 training loss:  0.03151589237638828\n","valid loss  0.022152808025479316  and accuracy  0.994\n","ep  495 training loss:  0.03134119554508118\n","valid loss  0.020672470358014106  and accuracy  0.9944\n","ep  496 training loss:  0.0309266479452131\n","valid loss  0.03128221223950386  and accuracy  0.9896\n","ep  497 training loss:  0.03109223102569437\n","valid loss  0.024438957306742667  and accuracy  0.9932\n","ep  498 training loss:  0.03125321095052788\n","valid loss  0.02998338512778282  and accuracy  0.9924\n","ep  499 training loss:  0.032052193298728646\n","valid loss  0.02491478900909424  and accuracy  0.9936\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1WL6v0uDgDJA"},"source":["\"\"\" Effettuiamo le predizioni sul dataset di test \"\"\"\n","\n","test_ds = NSLKDD_Dataset(test, np.zeros(len(test)), embedded_col_names)\n","test_dl = DataLoader(test_ds, batch_size=batch_size)\n","test_dl = DeviceDataLoader(test_dl, device)\n","\n","# Utilizziamo la funzione softmax poich√© siamo interessati alla probabilit√† per ogni classe\n","preds = []\n","with torch.no_grad():\n","    for x1,x2,y in test_dl:\n","        out = model(x1, x2)\n","        prob = F.softmax(out, dim=1)\n","        preds.append(prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syhoruAUlMAX"},"source":["y_pred = []\n","for i in range(0, len(preds)):\n","  pred = preds[i].cpu()\n","  temp = np.argmax(pred, 1)\n","  temp = np.array(temp)\n","  y_pred = np.append(y_pred, temp)\n","\n","y_pred = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3hmjVkNmFkX","executionInfo":{"status":"ok","timestamp":1621271632263,"user_tz":-120,"elapsed":609,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"7ec5d985-e8e6-44b9-87d8-3274ce2b02f6"},"source":["y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 2, 0, 0])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"WOciCjPisC_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621271634321,"user_tz":-120,"elapsed":599,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"492745c2-aad6-4e40-ae39-8136d8c93d61"},"source":["print('Test:', Counter(y_test))\n","print('Pred:', Counter(y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: Counter({2: 38484, 0: 26586, 1: 7026, 3: 2093, 4: 69})\n","Pred: Counter({2: 38383, 0: 26585, 1: 7119, 3: 2101, 4: 70})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oqp05uc1-d4t"},"source":["# Matrice di confusione, accuracy, classification_report\n","from sklearn.metrics import *\n","\n","# y_test √® la variabile che contiene i valori effettivi\n","# y_pred contiene i valori predetti dal modello\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","acc = accuracy_score(y_test, y_pred)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","# non presente nella libreria, calcolo mediante formula\n","f2 = (1+2**2)*((precision*recall)/((2**2*precision)+recall))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"8rOxIo2L-d4z","executionInfo":{"status":"ok","timestamp":1621271642087,"user_tz":-120,"elapsed":690,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"e423ac28-143d-467c-e086-aea0ba1e358f"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","target_dict = {'Dos' : 0,\n","               'Probe' : 1,\n","               'normal' : 2,\n","               'r2l' : 3,\n","               'u2r' : 4}\n","\n","disp = ConfusionMatrixDisplay(cm, target_dict)\n","disp.plot()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ffa7a2e89d0>"]},"metadata":{"tags":[]},"execution_count":73},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELCQniZh-d40","executionInfo":{"status":"ok","timestamp":1621271644241,"user_tz":-120,"elapsed":627,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"b3abe921-c0d3-466d-f880-3be21658b201"},"source":["mcm = multilabel_confusion_matrix(y_test, y_pred)\n","print(mcm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[47626    46]\n","  [   47 26539]]\n","\n"," [[67074   158]\n","  [   65  6961]]\n","\n"," [[35613   161]\n","  [  262 38222]]\n","\n"," [[72029   136]\n","  [  128  1965]]\n","\n"," [[74158    31]\n","  [   30    39]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89Nap2dd-d40","executionInfo":{"status":"ok","timestamp":1621271646314,"user_tz":-120,"elapsed":471,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"ead11db0-0f37-4e58-f1f6-bdbe07003382"},"source":["print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[26539    16    25     6     0]\n"," [    7  6961    44    12     2]\n"," [   34   107 38222   101    20]\n"," [    5    30    84  1965     9]\n"," [    0     5     8    17    39]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZptITmkjOPCq","executionInfo":{"status":"ok","timestamp":1621271649329,"user_tz":-120,"elapsed":766,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"c957ced4-a1c2-4238-d1a6-c7d684be3bd0"},"source":["FP = cm.sum (axis = 0) - np.diag (cm) \n","FN = cm.sum (axis = 1) - np.diag (cm) \n","TP = np.diag (cm) \n","TN = cm.sum () - (FP + FN + TP)\n","\n","print('True positive: ', TP)\n","print('True negative: ', TN)\n","print('False positive: ', FP)\n","print('False negative: ', FN)\n","\n","FP = FP.astype(float)\n","FN = FN.astype(float)\n","TP = TP.astype(float)\n","TN = TN.astype(float)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","# False negative rate\n","FNR = FN/(TP+FN)\n","\n","print('True positive rate: ', TPR)\n","print('True negative rate: ', TNR)\n","print('False positive rate: ', FPR)\n","print('False negative rate: ', FNR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True positive:  [26539  6961 38222  1965    39]\n","True negative:  [47626 67074 35613 72029 74158]\n","False positive:  [ 46 158 161 136  31]\n","False negative:  [ 47  65 262 128  30]\n","True positive rate:  [0.99823215 0.99074865 0.99319198 0.93884376 0.56521739]\n","True negative rate:  [0.99903507 0.99764993 0.99549952 0.99811543 0.99958215]\n","False positive rate:  [0.00096493 0.00235007 0.00450048 0.00188457 0.00041785]\n","False negative rate:  [0.00176785 0.00925135 0.00680802 0.06115624 0.43478261]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wE1uRjas-d41","executionInfo":{"status":"ok","timestamp":1621271654044,"user_tz":-120,"elapsed":497,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"11bb1b24-3025-4786-f23f-48250a0bc43e"},"source":["print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     26586\n","           1       0.98      0.99      0.98      7026\n","           2       1.00      0.99      0.99     38484\n","           3       0.94      0.94      0.94      2093\n","           4       0.56      0.57      0.56        69\n","\n","    accuracy                           0.99     74258\n","   macro avg       0.89      0.90      0.90     74258\n","weighted avg       0.99      0.99      0.99     74258\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJICMuXI-d41","executionInfo":{"status":"ok","timestamp":1621271656578,"user_tz":-120,"elapsed":369,"user":{"displayName":"MARCO PELLEGRINO","photoUrl":"","userId":"06611700164644863319"}},"outputId":"d8c2281e-9459-47f7-bac7-ffc9caf60db8"},"source":["print('Accuracy: ', acc)\n","print('Precision_weighted: ', precision)\n","print('Recall_weighted: ', recall)\n","print('mcc: ', mcc)\n","print('f2: ', f2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy:  0.9928357887365671\n","Precision_weighted:  0.992870793427401\n","Recall_weighted:  0.9928357887365671\n","mcc:  0.9879440111655603\n","f2:  0.9928427894772722\n"],"name":"stdout"}]}]}