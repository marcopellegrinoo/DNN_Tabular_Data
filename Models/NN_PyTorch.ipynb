{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN_PyTorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMqvIF8Dc613UPh1goi03jr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4QYedUdZH5IK"},"source":["### *Rete Neurale che utilizza l'incorporamento delle variabili categoriali.*"]},{"cell_type":"code","metadata":{"id":"xaOcbfhy9L4o"},"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as torch_optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QdVMyx4Kpp8"},"source":["### ***PRE-ELABORAZIONE DATI***"]},{"cell_type":"code","metadata":{"id":"IuKjUqRo-I1z"},"source":["# Caricamento dataset dal drive\n","\n","path = # Inserire percorso del file\n","dataset = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WK6XJ2ozUx4Q"},"source":["dep_var = # Inserire nome variabile target\n","cat_names = # array che contiene il nome delle variabili categoriali (se non presenti lasciare array vuoto)\n","cont_names = [col for col in dataset.columns if col not in cat_names and col != dep_var]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pqCKXOoU-mA"},"source":["# LabelEncoding della variabile target \n","target_index = dataset.columns.get_loc(dep_var)\n","dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[dep_var])\n","\n","#LabelEncoding delle variabili categoriali\n","for col in cat_names:\n","  target_index = dataset.columns.get_loc(col)\n","  dataset.iloc[:, target_index] = LabelEncoder().fit_transform(dataset[col])\n","\n","# Fill NaN\n","\"\"\" Eliminiamo dalle colonne i valori nan \"\"\" \n","for col in dataset.columns:\n","  dataset[col] = dataset[col].fillna(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5ZN9TkwXom7"},"source":["from sklearn.model_selection import train_test_split\n","\n","# train 50% e test 50%\n","train, test = train_test_split(dataset, test_size=0.50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yDgHGN2XrGJ"},"source":["y_train = train[dep_var]\n","train = train.drop(dep_var, axis=1)\n","y_test = test[dep_var]\n","test = test.drop(dep_var, axis=1)\n","\n","# validation di un numero arbitrario di righe da train\n","valid_row = # Inserire il numero di righe da attribuire a valid \n","train, validation, y_train, y_val = train_test_split(train, y_train, test_size=valid_row/len(train), random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qe_3ehuBYBno"},"source":["y_train = y_train.values\n","y_test = y_test.values\n","y_val = y_val.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXPzv-kYIoHT"},"source":["#### Fase di Categorical Embeddings ###############\n","\n","for col in cat_names:\n","  train[col] = train[col].astype('category')\n","\n","embedded_cols = {n: len(col.cat.categories) for n,col in train[cat_names].items()}\n","print(embedded_cols)\n","\n","embedded_col_names = cat_names\n","\n","# Determiniamo una funzione per la dimensione dell'incorporamento, presa da una libreria \n","embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n","embedding_sizes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tc8GPa7lBOWj"},"source":["### ***SMOTE***"]},{"cell_type":"markdown","metadata":{"id":"6i7v46RRBTQa"},"source":["*Eventualmente per dataset molto squilibrati si può utilizzare una tecnica di sovra-campionamento dei dati; questa viene utilizzata per creare dati fittizi 'simili' a quelli delle classi di minoranza, ovvero quelle classi che presentano pochi esempi nel dataset.*\n"]},{"cell_type":"code","metadata":{"id":"fNTwI2TaaXlY"},"source":["\"\"\"Visto che nel dataset la variabile target è molto squilibrata lo amplio con una generazione\n"," randomica di dati mediante la tecnica chiamata Synthetic Minority Over-sampling Technique (SMOTE)\"\"\"\n","\n","# Inserire tupla con la strategia di sovra-campionamento. In pratica si decide a che numero impostare le righe per ogni classe. ex: {0: 100, 1:5000, 2: 10000}\n","# Se si omette la strategia l'algoritmo di SMOTE imposta tutte le classi al numero di esempi della classe maggioritaria (spesso questa soluzione non è ottimale)\n","# Molto spesso si raggiungono risultati migliori se si sovra-campiona le classi minoritarie moderatamente.\n","sampling_strategy = \n","\n","from imblearn.over_sampling import SMOTE\n","sm = SMOTE( sampling_strategy = sampling_strategy ,random_state=1)\n","sm = SMOTE( random_state=1)\n","x_sm, y_train = sm.fit_resample(train, y_train)\n","train = pd.DataFrame(x_sm,columns=train.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GQ_rp_-Fe7R"},"source":["### ***GPU/CPU***"]},{"cell_type":"code","metadata":{"id":"0oGGkh8o6icS"},"source":["\"\"\" Making device (GPU/CPU) compatible\n","\n","(borrowed from https://jovian.ml/aakashns/04-feedforward-nn)\n","\n","In order to make use of a GPU if available, we'll have to move our data and model to it. \"\"\" \n","\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65IhHyNYYHgJ"},"source":["### ***MODEL***"]},{"cell_type":"code","metadata":{"id":"GaCCliFBYLdY"},"source":["\"\"\" Pytorch Dataset e DataLoader\n","Estendiamo la Dataset classe (astratta) fornita da Pytorch per un accesso più facile al nostro set di dati durante l'addestramento \n","e per utilizzare efficacemente  il DataLoader modulo per gestire i batch. Ciò comporta la sovrascrittura dei metodi __len__e __getitem__\n","secondo il nostro particolare set di dati.\n","Poiché abbiamo solo bisogno di incorporare colonne categoriali, dividiamo il nostro input in due parti: numerico e categoriale. \"\"\" \n","\n","class Name_Dataset(Dataset):\n","    def __init__(self, X, Y, embedded_col_names):\n","        X = X.copy()\n","        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n","        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X1[idx], self.X2[idx], self.y[idx]\n","        \n","#creating train and valid datasets\n","train_ds = Name_Dataset(train, y_train, embedded_col_names)\n","valid_ds = Name_Dataset(validation, y_val, embedded_col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrjDlti4JfAy"},"source":["\"\"\" I nostri dati sono suddivisi in parti continue e categoriali. Per prima cosa convertiamo le parti categoriali in vettori \n","incorporanti in base alle dimensioni determinate in precedenza e le concateniamo con le parti continue per alimentare il resto della rete \"\"\" \n","\n","n_output = \n","\n","class _Model(nn.Module):\n","    def __init__(self, embedding_sizes, n_cont):\n","        super().__init__()\n","        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n","        n_emb = sum(e.embedding_dim for e in self.embeddings)  #length of all embeddings combined\n","        self.n_emb, self.n_cont = n_emb, n_cont\n","        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n","        self.lin2 = nn.Linear(200, 70)\n","        self.lin3 = nn.Linear(70, n_output)\n","        self.bn1 = nn.BatchNorm1d(self.n_cont, momentum=1.0)\n","        self.bn2 = nn.BatchNorm1d(200, momentum=1.0)\n","        self.bn3 = nn.BatchNorm1d(70, momentum=1.0)\n","        self.emb_drop = nn.Dropout(0.01)\n","\n","      \n","    def forward(self, x_cat, x_cont):\n","        x = [e(x_cat[:,0]) for i,e in enumerate(self.embeddings)]\n","        x = torch.cat(x, 1)\n","        x = self.emb_drop(x)\n","        x2 = self.bn1(x_cont)\n","        x = torch.cat([x, x2], 1)\n","        x = F.relu(self.lin1(x))\n","        x = self.bn2(x)\n","        x = F.relu(self.lin2(x))\n","        x = self.bn3(x)\n","        x = self.lin3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXLa9bd3Yc4q"},"source":["\"\"\" Fase di preparazione per l'addestramento \"\"\"\n","\n","# Optimizer\n","def get_optimizer(model, lr = 0.001, wd = 0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim\n","\n","# Training function\n","def train_model(model, optim, train_dl):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x1, x2, y in train_dl:\n","        batch = y.shape[0]\n","        output = model(x1, x2)\n","        loss = F.cross_entropy(output, y)   \n","        optim.zero_grad()\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","# Evaluation function\n","def val_loss(model, valid_dl):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x1, x2, y in valid_dl:\n","        current_batch_size = y.shape[0]\n","        out = model(x1, x2)\n","        loss = F.cross_entropy(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.max(out, 1)[1]\n","        correct += (pred == y).float().sum().item()\n","    #print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n","    print('valid loss ', sum_loss/total, ' and accuracy ', correct/total)\n","    return sum_loss/total, correct/total\n","\n","# Funzione per l'addestramento \n","def train_loop(model, epochs, lr=0.01, wd=0.0):\n","    optim = get_optimizer(model, lr = lr, wd = wd)\n","    for i in range(epochs): \n","        loss = train_model(model, optim, train_dl)\n","        print('ep ', i, \"training loss: \", loss)\n","        val_loss(model, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpqXH6hEYfdB"},"source":["model = _Model(embedding_sizes, len(cont_names))\n","to_device(model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6Nq8kEQYjfk"},"source":["### ***TRAINING***"]},{"cell_type":"code","metadata":{"id":"iOcSuvjpYlpV"},"source":["\"\"\" Ora addestriamo il modello sul set di addestramento. Ho usato l'ottimizzatore Adam per ottimizzare la perdita di entropia incrociata. \n","L'addestramento è piuttosto semplice: iterare attraverso ogni batch, eseguire un passaggio in avanti, calcolare i gradienti, \n","eseguire una discesa del gradiente e ripetere questo processo per tutte le epoche necessarie. \"\"\" \n","\n","batch_size = # Inserire numero batch\n","train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=False)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=False)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kl-71lYYoNS"},"source":["# Epoche e lr eventualmente da modificare\n","train_loop(model, epochs=100, lr=0.00005)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDgsCJMYYufq"},"source":["### ***PREDICTION***"]},{"cell_type":"code","metadata":{"id":"xl04bkuYYzlh"},"source":["\"\"\" Effettuiamo le predizioni sul dataset di test \"\"\"\n","\n","test_ds = Name_Dataset(test, np.zeros(len(test)), embedded_col_names)\n","test_dl = DataLoader(test_ds, batch_size=batch_size)\n","test_dl = DeviceDataLoader(test_dl, device)\n","\n","# Utilizziamo la funzione softmax poiché siamo interessati alla probabilità per ogni classe\n","preds = []\n","with torch.no_grad():\n","    for x1,x2,y in test_dl:\n","        out = model(x1, x2)\n","        prob = F.softmax(out, dim=1)\n","        preds.append(prob)\n","        \n","y_pred = []\n","for i in range(0, len(preds)):\n","  pred = preds[i].cpu()\n","  temp = np.argmax(pred, 1)\n","  temp = np.array(temp)\n","  y_pred = np.append(y_pred, temp)\n","\n","y_pred = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mmut1WmZCUq"},"source":["### ***EVALUATION***"]},{"cell_type":"code","metadata":{"id":"2dFGKSLY74tv"},"source":["# Matrice di confusione, accuracy, classification_report\n","from sklearn.metrics import *\n","\n","# y_test è la variabile che contiene i valori effettivi\n","# y_pred contiene i valori predetti dal modello\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","acc = accuracy_score(y_test, y_pred)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","# non presente nella libreria, calcolo mediante formula\n","f2 = (1+2**2)*((precision*recall)/((2**2*precision)+recall))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhOAhOzT79qn"},"source":["# Multilabel confusione Matrix\n","mcm = multilabel_confusion_matrix(y_test, y_pred)\n","print(mcm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzWM556f7-fI"},"source":["# Confusione matrix\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3tVhPKW8AfA"},"source":["# True/False Positive and True/False Negative\n","FP = cm.sum (axis = 0) - np.diag (cm) \n","FN = cm.sum (axis = 1) - np.diag (cm) \n","TP = np.diag (cm) \n","TN = cm.sum () - (FP + FN + TP)\n","\n","print('True positive: ', TP)\n","print('True negative: ', TN)\n","print('False positive: ', FP)\n","print('False negative: ', FN)\n","\n","FP = FP.astype(float)\n","FN = FN.astype(float)\n","TP = TP.astype(float)\n","TN = TN.astype(float)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","# False negative rate\n","FNR = FN/(TP+FN)\n","\n","print('True positive rate: ', TPR)\n","print('True negative rate: ', TNR)\n","print('False positive rate: ', FPR)\n","print('False negative rate: ', FNR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gw9RIRmX8Bs4"},"source":["# Stampa report con gli indice della performance\n","print(report)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHChwUi28C3A"},"source":["# Stampa di tutte le varie metriche\n","print('Accuracy: ', acc)\n","print('Precision_weighted: ', precision)\n","print('Recall_weighted: ', recall)\n","print('mcc: ', mcc)\n","print('f2: ', f2)"],"execution_count":null,"outputs":[]}]}